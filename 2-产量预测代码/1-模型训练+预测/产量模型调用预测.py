"""
Copyright (c) 2025 å¼ å¾·æµ·
MIT Licensed - è¯¦è§é¡¹ç›®æ ¹ç›®å½• LICENSE æ–‡ä»¶

é¡¹ç›®: Meteorology-Assisted-Crop-Yield-Prediction
ä»“åº“: https://github.com/ZDH4869/Meteorology-Assisted-Crop-Yield-Prediction.git
è”ç³»: zhangdehai1412@163.com
"""
# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹ æ··åˆæ¨¡å‹ï¼šåŸºäºåœ°ç†åæ ‡ã€è®­ç»ƒæ°”è±¡ã€åœŸå£¤æ•°æ®åŠå†œä½œç‰©é€‚å®œæ€§ç­‰çº§ã€
åŒºäº§ï¼ˆyield_per_cellï¼‰å’Œäº©äº§ï¼ˆyield_per_muï¼‰çš„å¤šè¾“å‡ºé¢„æµ‹
"""

# ================= è®¾ç½®CUDAç¯å¢ƒå˜é‡ =================
import os

# ================= å…¨å±€å˜é‡ =================
# å…¨å±€RÂ²æ—©åœç›¸å…³å˜é‡
GLOBAL_R2_ACHIEVED = False
GLOBAL_BEST_PARAMS = None
GLOBAL_BEST_R2 = 0.0
# è®¾ç½®CUDA 11.8ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿TensorFlowèƒ½æ‰¾åˆ°GPU
os.environ['CUDA_PATH'] = r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8'
cuda_bin_path = r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin'
cuda_lib_path = r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\libnvvp'
current_path = os.environ.get('PATH', '')
if cuda_bin_path not in current_path:
    os.environ['PATH'] = cuda_bin_path + ';' + cuda_lib_path + ';' + current_path
    print(f"âœ… å·²è®¾ç½®CUDA 11.8ç¯å¢ƒå˜é‡")
    print(f"CUDA_PATH: {os.environ['CUDA_PATH']}")
else:
    print("âœ… CUDA 11.8ç¯å¢ƒå˜é‡å·²å­˜åœ¨")

# ================= ç³»ç»Ÿå’ŒåŸºç¡€åº“ =================
import json
import time
import psutil
import warnings
from typing import Union
import platform
import random
# import re  # æœªä½¿ç”¨
# import itertools  # æœªä½¿ç”¨

# ================= æ•°æ®å¤„ç†åº“ =================
import numpy as np
import pandas as pd
import joblib
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder
# from sklearn.feature_selection import mutual_info_regression, mutual_info_classif  # æœªä½¿ç”¨
from scipy.spatial import cKDTree
# from itertools import combinations  # æœªä½¿ç”¨
# import scipy  # æœªä½¿ç”¨
import xgboost as xgb
from datetime import datetime

# ================= æ·±åº¦å­¦ä¹ åº“ =================
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate, Layer, Add


# ================= ç©ºé—´åˆå¹¶å‡½æ•° =================
def spatial_merge(left, right, on=None, tolerance=50000):
    """ç©ºé—´è¿‘é‚»åˆå¹¶å‡½æ•°"""
    if on is None:
        on = ['x', 'y']
    left_coords = left[on].values
    right_coords = right[on].values
    tree = cKDTree(right_coords)
    dist, idx = tree.query(left_coords, distance_upper_bound=tolerance)
    mask = idx < right.shape[0]

    if not mask.any():
        return pd.DataFrame()

    # é¿å…å¤šæ¬¡reset_indexï¼Œç›´æ¥ä½¿ç”¨ç´¢å¼•
    left_valid = left[mask].copy()
    right_valid = right.iloc[idx[mask]].copy()

    # ç»™ right_valid æ‰€æœ‰åˆ—åŠ å‰ç¼€
    right_valid.columns = [f'right_{col}' for col in right_valid.columns]

    # é‡ç½®ç´¢å¼•ä½†é¿å…æ·±åº¦å¤åˆ¶
    left_valid.reset_index(drop=True, inplace=True)
    right_valid.reset_index(drop=True, inplace=True)

    # ç›´æ¥åˆå¹¶ï¼Œé¿å…é¢å¤–çš„reset_index
    merged = pd.concat([left_valid, right_valid], axis=1)
    return merged


def spatial_temporal_merge(left, right, xy_cols=None, time_col='yyyy', tolerance=50000):
    """ç©ºé—´æ—¶é—´åˆå¹¶å‡½æ•°"""
    if xy_cols is None:
        xy_cols = ['x', 'y']
    merged_list = []
    years = left[time_col].unique()
    print(f"å¤„ç†å¹´ä»½: {years}")

    # è·å–å³è¡¨å¯ç”¨çš„å¹´ä»½
    right_years = right[time_col].unique()
    print(f"æ°”è±¡æ•°æ®å¯ç”¨å¹´ä»½: {right_years}")

    # åªå¤„ç†å¹´ä»½å®Œå…¨åŒ¹é…çš„æ•°æ®
    common_years = set(years) & set(right_years)
    print(f"å¹´ä»½å®Œå…¨åŒ¹é…çš„å¹´ä»½: {sorted(common_years)}")

    if not common_years:
        print("âš ï¸ æ²¡æœ‰å¹´ä»½å®Œå…¨åŒ¹é…çš„æ•°æ®ï¼Œæ— æ³•è¿›è¡Œåˆå¹¶")
        return pd.DataFrame()

    for i, year in enumerate(sorted(common_years)):
        print(f"å¤„ç†å¹´ä»½ {year} ({i + 1}/{len(common_years)})...")

        # ä½¿ç”¨è§†å›¾è€Œä¸æ˜¯å‰¯æœ¬
        left_year = left[left[time_col] == year]
        right_year = right[right[time_col] == year]
        print(f"  ä½¿ç”¨ç›¸åŒå¹´ä»½ {year} çš„æ°”è±¡æ•°æ®")

        if len(right_year) == 0:
            print(f"  å¹´ä»½ {year} åœ¨å³è¡¨ä¸­æ— æ•°æ®ï¼Œè·³è¿‡")
            continue

        print(f"  å·¦è¡¨: {len(left_year)} è¡Œ, å³è¡¨: {len(right_year)} è¡Œ")

        try:
            merged = spatial_merge(left_year, right_year, on=xy_cols, tolerance=tolerance)
            if not merged.empty:
                merged_list.append(merged)
                print(f"  åˆå¹¶æˆåŠŸ: {len(merged)} è¡Œ")
            else:
                print(f"  å¹´ä»½ {year} åˆå¹¶ç»“æœä¸ºç©º")
        except MemoryError as e:
            print(f"  å¹´ä»½ {year} å¤„ç†æ—¶å†…å­˜ä¸è¶³: {e}")
            # å°è¯•åˆ†å—å¤„ç†
            chunk_size = len(left_year) // 4
            if chunk_size > 0:
                for j in range(0, len(left_year), chunk_size):
                    left_chunk = left_year.iloc[j:j + chunk_size]
                    try:
                        merged_chunk = spatial_merge(left_chunk, right_year, on=xy_cols, tolerance=tolerance)
                        if not merged_chunk.empty:
                            merged_list.append(merged_chunk)
                    except MemoryError:
                        print(f"    åˆ†å— {j // chunk_size + 1} ä»ç„¶å†…å­˜ä¸è¶³ï¼Œè·³è¿‡")
                        continue

    if merged_list:
        return pd.concat(merged_list, ignore_index=True)
    else:
        return pd.DataFrame()


from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from tensorflow.keras import mixed_precision
# from tensorflow.keras.optimizers import Adam  # æœªä½¿ç”¨

# ================= XGBoostå’Œä¼˜åŒ–åº“ =================
import xgboost as xgb
import optuna

# ================= å¯è§†åŒ–åº“ =================
import matplotlib.pyplot as plt
import geopandas as gpd
from tqdm.auto import tqdm
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ================= GPUç›‘æ§åº“ =================
try:
    import GPUtil
except ImportError:
    GPUtil = None

# ç¦ç”¨è­¦å‘Š
warnings.filterwarnings('ignore')

os.environ['CUDA_VISIBLE_DEVICES'] = '0'


def json_fallback(obj):
    """JSONåºåˆ—åŒ–æ—¶çš„å›é€€å‡½æ•°ï¼Œå¤„ç†numpyç±»å‹"""
    if isinstance(obj, (np.floating,)):
        return float(obj)
    if hasattr(obj, 'item'):
        return obj.item()
    return str(obj)


class Config:
    """é…ç½®ç±»ï¼šåŒ…å«æ‰€æœ‰å¯è°ƒæ•´çš„å‚æ•°å’Œè·¯å¾„è®¾ç½®"""

    @classmethod
    def validate_params(cls):
        """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
        try:
            # éªŒè¯éšæœºç§å­
            assert isinstance(cls.random_seed, int) and cls.random_seed >= 0

            # éªŒè¯æµ‹è¯•é›†æ¯”ä¾‹
            assert 0.0 < cls.test_size < 1.0

            # éªŒè¯XGBoostå‚æ•°
            assert 1 <= cls.xgb_params['max_depth'] <= 20
            assert 0.0 < cls.xgb_params['learning_rate'] <= 1.0
            assert cls.xgb_params['n_estimators'] > 0

            # éªŒè¯æ·±åº¦å­¦ä¹ å‚æ•°
            assert cls.dl_params['epochs'] > 0
            assert cls.dl_params['batch_size'] > 0
            assert 0.0 < cls.dl_params['learning_rate'] <= 1.0
            assert 0.0 <= cls.dl_params['dropout_rate'] < 1.0
            assert cls.dl_params['early_stop_patience'] > 0

            # éªŒè¯Optunaå‚æ•°
            assert cls.optuna_params['n_trials'] > 0
            assert cls.optuna_params['timeout'] > 0

            # éªŒè¯GPUå‚æ•°
            assert 0.0 < cls.gpu_memory_limit <= 1.0

            # éªŒè¯æ•°æ®è¯»å–å‚æ•°
            assert cls.max_rows_per_weather_file > 0, "æ¯ä¸ªæ°”è±¡æ–‡ä»¶æœ€å¤§è¡Œæ•°å¿…é¡»å¤§äº0"

            print("å‚æ•°éªŒè¯é€šè¿‡")
            return True

        except AssertionError as e:
            print(f"å‚æ•°éªŒè¯å¤±è´¥: {str(e)}")
            return False

        except Exception as e:
            print(f"å‚æ•°éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return False

    @classmethod
    def validate_paths(cls):
        """éªŒè¯æ–‡ä»¶è·¯å¾„æœ‰æ•ˆæ€§"""
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            input_files = [
                cls.soil_data_csv,
                cls.test_soil_csv
            ]

            for file_path in input_files:
                if not os.path.exists(file_path):
                    print(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return False

            # éªŒè¯è¾“å‡ºç›®å½•æƒé™
            output_dirs = [
                cls.output_dir,
                cls.model_dir,
                cls.final_model_dir,
                cls.label_encoder_dir,
                cls.scaler_dir,
                cls.xgboost_dir,
                cls.logs_dir,
                cls.result_dir,
                cls.result_analysis_dir,
                cls.training_analysis_dir,
                cls.feature_importance_dir  # æ–°å¢
            ]

            for dir_path in output_dirs:
                try:
                    os.makedirs(dir_path, exist_ok=True)
                except Exception as e:
                    print(f"æ— æ³•åˆ›å»ºç›®å½• {dir_path}: {str(e)}")
                    return False

            print("è·¯å¾„éªŒè¯é€šè¿‡")
            return True

        except Exception as e:
            print(f"è·¯å¾„éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return False

    # ============= è¾“å…¥è¾“å‡ºè·¯å¾„é…ç½® =============
    # åŸºç¡€è·¯å¾„
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    # è¾“å…¥æ•°æ®è·¯å¾„
    input_data_dir = os.path.join(base_dir, "train+text_csvæ•°æ®")

    # é¢„æµ‹æ°”è±¡æ•°æ®é…ç½® - æ–‡ä»¶å¤¹æ¨¡å¼
    weather_data_folder = r"2-äº§é‡é¢„æµ‹ä»£ç /train+text_csvæ•°æ®/é¢„æµ‹æ°”è±¡"  # è®­ç»ƒæ°”è±¡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
    use_weather_folder = True  # ä½¿ç”¨æ–‡ä»¶å¤¹æ¨¡å¼è¯»å–è®­ç»ƒæ°”è±¡æ•°æ®

    soil_data_csv = os.path.join(input_data_dir, "soil_test.csv") # å½“ç”¨äºé¢„æµ‹æ—¶ï¼Œä¿æŒåœŸå£¤ç›¸åŒï¼ˆsoil_data_csv=test_soil_csvï¼‰
    test_soil_csv = os.path.join(input_data_dir, "soil_test.csv") # å½“ç”¨äºé¢„æµ‹æ—¶ï¼Œä¿æŒåœŸå£¤ç›¸åŒ

    # è¾“å‡ºç›®å½•é…ç½®
    _default_model_name = "model_test_06"  # é»˜è®¤æ¨¡å‹åç§°
    _current_model_name = _default_model_name  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°
    
    @classmethod
    def set_model_name(cls, model_name):
        """è®¾ç½®å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°"""
        cls._current_model_name = model_name
        cls._update_paths()
    
    @classmethod
    def get_model_name(cls):
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°"""
        return cls._current_model_name
    
    @classmethod
    def _update_paths(cls):
        """æ ¹æ®å½“å‰æ¨¡å‹åç§°æ›´æ–°æ‰€æœ‰è·¯å¾„"""
        cls.output_dir = os.path.join(cls.base_dir, cls._current_model_name)
        cls.model_dir = os.path.join(cls.output_dir, "modelAA")
        cls.final_model_dir = os.path.join(cls.output_dir, "final_modelAA")
        cls.label_encoder_dir = os.path.join(cls.output_dir, "LabelEnconderAA")
        cls.scaler_dir = os.path.join(cls.output_dir, "ScalerAA")
        cls.xgboost_dir = os.path.join(cls.output_dir, "XGBoostAA")
        cls.logs_dir = os.path.join(cls.output_dir, "final_logsAA")
        cls.result_dir = os.path.join(cls.output_dir, "result_predited_csvAA") # é¢„æµ‹ç»“æœè¾“å‡ºè·¯å¾„
        cls.feature_importance_dir = os.path.join(cls.output_dir, "feature_importanceAA")
        cls.result_analysis_dir = os.path.join(cls.output_dir, "analysis_result_mapAA")  # åº”è¯¥è¾“å‡º2
        cls.training_analysis_dir = os.path.join(cls.output_dir, "analysis_training_mapAA")  # åº”è¯¥è¾“å‡º4
        
        # æ›´æ–°å…·ä½“æ–‡ä»¶è·¯å¾„
        cls.label_encoder_file = os.path.join(cls.label_encoder_dir, "label_encoder_AA.pkl")
        cls.scaler_file = os.path.join(cls.scaler_dir, "scaler_AA.pkl")
        cls.xgboost_model_file = os.path.join(cls.xgboost_dir, "xgb_model_AA.json")
        cls.final_model_file = os.path.join(cls.final_model_dir, "final_model_r2_r2_0.9196.h5") # å…·ä½“è°ƒç”¨æ¨¡å‹è·¯å¾„
        cls.result_file = os.path.join(cls.result_dir, "result_predited_AA_test.csv")
        cls.feature_importance_file = os.path.join(cls.feature_importance_dir, "feature_importance_AA.csv")
        cls.key_features_list_file = os.path.join(cls.feature_importance_dir, "key_features_AA.json")
        cls.feature_importance_plot = os.path.join(cls.feature_importance_dir, "feature_importance_plot_AA.png")
        cls.suitability_map_file = os.path.join(cls.result_analysis_dir, "result_suitability_map_AA.png")
        cls.feature_importance_map_file = os.path.join(cls.result_analysis_dir, "feature_importance.png")
        cls.evaluation_file = os.path.join(cls.training_analysis_dir, "evaluation_report.json")
        cls.confusion_matrix_file = os.path.join(cls.training_analysis_dir, "confusion_matrix.png")
        cls.accuracy_loss_map_file = os.path.join(cls.training_analysis_dir, "Accuracy+Loss_map_AA.png")
        cls.regression_scatter_file = os.path.join(cls.training_analysis_dir, "regression_scatter.png")
    
    @classmethod
    def auto_detect_available_models(cls):
        """è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„æ¨¡å‹ç›®å½•"""
        available_models = []
        for item in os.listdir(cls.base_dir):
            item_path = os.path.join(cls.base_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                model_files = [
                    os.path.join(item_path, "final_modelAA", "best_hybrid_multioutput_AA.h5"),
                    os.path.join(item_path, "ScalerAA", "scaler_AA.pkl"),
                    os.path.join(item_path, "ScalerAA", "pca_AA.pkl"),
                    os.path.join(item_path, "XGBoostAA", "xgb_model_AA.json"),
                    os.path.join(item_path, "feature_order.json")
                ]
                if all(os.path.exists(f) for f in model_files):
                    available_models.append(item)
        return available_models
    
    @classmethod
    def list_available_models(cls):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        models = cls.auto_detect_available_models()
        print(f"ğŸ” æ£€æµ‹åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"   {i}. {model}")
        return models
    
    # åˆå§‹åŒ–è·¯å¾„ - åœ¨ç±»å®šä¹‰å®Œæˆåè°ƒç”¨
    # _update_paths() å°†åœ¨ç±»å®šä¹‰å®Œæˆåè°ƒç”¨

    # å…·ä½“æ–‡ä»¶è·¯å¾„å°†åœ¨_update_paths()ä¸­åŠ¨æ€è®¾ç½®
    # è¿™äº›å˜é‡å°†åœ¨ç±»åˆå§‹åŒ–æ—¶é€šè¿‡_update_paths()è®¾ç½®
    label_encoder_file = None
    scaler_file = None
    xgboost_model_file = None
    final_model_file = None
    result_file = None
    feature_importance_file = None
    key_features_list_file = None
    feature_importance_plot = None
    suitability_map_file = None
    feature_importance_map_file = None
    evaluation_file = None
    confusion_matrix_file = None
    accuracy_loss_map_file = None
    regression_scatter_file = None

    # ============= ç‰¹å¾å’Œç›®æ ‡å˜é‡é…ç½® =============
    # ä¸å‚ä¸è®­ç»ƒçš„ç‰¹å¾åˆ—ï¼ˆåŸºäºV3ä¼˜åŒ–ç»“æœï¼‰
    exclude_columns = ['x', 'y', 'YYYY', 'SUIT', 'per_mu', 'per_qu']  # æ’é™¤åæ ‡ã€å¹´ä»½ã€ç›®æ ‡å˜é‡

    # åˆ†ç±»ç‰¹å¾åˆ—ï¼ˆéœ€è¦è¿›è¡Œç¼–ç çš„åˆ—ï¼‰
    categorical_columns = ['tz']  # ä¿ç•™'tz'åˆ†ç±»ç‰¹å¾

    # èŒƒå›´å€¼ç‰¹å¾åˆ—
    range_columns = []  # ä¸åŒ…å«'tz'

    # ç›®æ ‡å˜é‡é…ç½®ï¼ˆåŸºäºV3ä¼˜åŒ–ç»“æœï¼‰
    target_columns = {
        'classification': 'SUIT',  # åˆ†ç±»ç›®æ ‡
        'regression': ['per_mu', 'per_qu']  # å›å½’ç›®æ ‡
    }

    # æ–°å¢ï¼šåŸºäºäº§é‡åˆ›å»ºé€‚å®œåº¦åˆ†ç±»çš„é…ç½®
    create_suitability_from_yield = True  # æ˜¯å¦åŸºäºäº§é‡åˆ›å»ºé€‚å®œåº¦åˆ†ç±»
    suitability_quantiles = [0.33, 0.67]  # é€‚å®œåº¦åˆ†ç±»çš„åˆ†ä½æ•°é˜ˆå€¼
    # æ–°å¢æŸå¤±å‡½æ•°ç±»å‹å‚æ•°ï¼ˆåŸºäºV3ä¼˜åŒ–ç»“æœï¼‰
    loss_type = 'huber'  # ä½¿ç”¨HuberæŸå¤±ï¼Œå¯¹å¼‚å¸¸å€¼æ›´é²æ£’
    # ============= æ¨¡å‹å‚æ•°é…ç½® =============
    # é€šç”¨å‚æ•°
    random_seed = 42
    test_size = 0.2

    # XGBoostå‚æ•°ï¼ˆç¨³å®šæ€§ä¼˜åŒ–é…ç½®ï¼‰
    xgb_params = {
        'max_depth': 6,  # å¢åŠ æ·±åº¦ï¼Œæé«˜å­¦ä¹ èƒ½åŠ›
        'learning_rate': 0.1,  # é™ä½å­¦ä¹ ç‡ï¼Œæé«˜ç¨³å®šæ€§
        'n_estimators': 300,  # å¢åŠ æ ‘æ•°é‡ï¼Œæé«˜æ€§èƒ½
        'subsample': 0.8,  # é™ä½å­é‡‡æ ·ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        'colsample_bytree': 0.8,  # é™ä½ç‰¹å¾é‡‡æ ·ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        'random_state': random_seed,
        'tree_method': 'hist',
        'predictor': 'cpu_predictor',
    }

    # XGBoost GPUå‚æ•°éªŒè¯
    @staticmethod
    def validate_xgboost_gpu():
        """éªŒè¯XGBoost GPUå‚æ•°æ˜¯å¦å¯ç”¨"""
        try:
            import xgboost as xgb
            # æ£€æŸ¥XGBoostç‰ˆæœ¬æ˜¯å¦æ”¯æŒGPU
            if hasattr(xgb, '__version__'):
                version = xgb.__version__
                print(f"XGBoostç‰ˆæœ¬: {version}")
                # æ£€æŸ¥æ˜¯å¦æ”¯æŒGPU
                try:
                    # å°è¯•åˆ›å»ºGPU DMatrix
                    test_data = np.random.random((10, 5))
                    dtest = xgb.DMatrix(test_data)
                    print("âœ… XGBoost GPUæ”¯æŒæ­£å¸¸")
                    return True
                except Exception as e:
                    print(f"âš ï¸ XGBoost GPUæ”¯æŒå¯èƒ½æœ‰é—®é¢˜: {str(e)}")
                    # å›é€€åˆ°CPUå‚æ•°
                    Config.xgb_params.update({
                        'tree_method': 'hist',
                        'predictor': 'cpu_predictor'
                    })
                    print("å·²å›é€€åˆ°CPUå‚æ•°")
                    return False
            else:
                print("âš ï¸ æ— æ³•æ£€æµ‹XGBoostç‰ˆæœ¬")
                return False
        except ImportError:
            print("âŒ XGBoostæœªå®‰è£…")
            return False

    # æ·±åº¦å­¦ä¹ å‚æ•°ï¼ˆç¨³å®šæ€§ä¼˜åŒ–é…ç½®ï¼‰
    dl_params = {
        'epochs': 50,  # å¢åŠ è®­ç»ƒè½®æ¬¡ï¼Œç¡®ä¿å……åˆ†å­¦ä¹ 
        'batch_size': 32,  # å‡å°batch sizeï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        'learning_rate': 0.0001,  # é™ä½å­¦ä¹ ç‡ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
        'dropout_rate': 0.3,  # é™ä½dropoutï¼Œæé«˜æ¨¡å‹å­¦ä¹ èƒ½åŠ›
        'early_stop_patience': 15,  # å¢åŠ æ—©åœè€å¿ƒï¼Œé¿å…è¿‡æ—©åœæ­¢
        'l2_reg': 1e-4,  # é™ä½L2æ­£åˆ™åŒ–å¼ºåº¦
        'l1_reg': 1e-5,  # é™ä½L1æ­£åˆ™åŒ–å¼ºåº¦
        'min_delta': 0.0001,  # é™ä½æœ€å°æ”¹å–„é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
        'reduce_lr_patience': 10,  # å¢åŠ å­¦ä¹ ç‡è¡°å‡è€å¿ƒ
        'reduce_lr_factor': 0.7,  # æ¸©å’Œçš„å­¦ä¹ ç‡è¡°å‡
        'min_lr': 1e-7  # é™ä½æœ€å°å­¦ä¹ ç‡
    }

    # ============= ç¡¬ä»¶é…ç½® =============
    use_gpu = True
    gpu_memory_limit = 0.9  # GPUå†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆå æ¯”ï¼‰
    use_amp = True  # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    multi_gpu_enabled = True  # æ˜¯å¦å¯ç”¨å¤šGPUè®­ç»ƒ
    gpu_memory_growth = True  # æ˜¯å¦å¯ç”¨GPUå†…å­˜å¢é•¿
    gpu_visible_devices = None  # æŒ‡å®šå¯è§çš„GPUè®¾å¤‡ï¼Œå¦‚"0,1"è¡¨ç¤ºä½¿ç”¨GPU 0å’Œ1

    # ============= Optunaè¶…å‚æ•°é…ç½®ï¼ˆç¨³å®šæ€§ä¼˜åŒ–ï¼‰ =============
    optuna_params = {
        'n_trials': 3,  # å¢åŠ è¯•éªŒæ¬¡æ•°ï¼Œæ‰¾åˆ°æ›´å¥½çš„å‚æ•°
        'timeout': 600,  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå…è®¸å……åˆ†è®­ç»ƒ
        'param_ranges': {
            'lr': (1e-5, 0.001),  # æ‰©å¤§å­¦ä¹ ç‡èŒƒå›´ï¼ŒåŒ…å«æ›´ä¿å®ˆçš„å€¼
            'neurons1': (64, 256),  # å¢åŠ ç¥ç»å…ƒæ•°é‡ï¼Œæé«˜æ¨¡å‹å®¹é‡
            'neurons2': (32, 128),  # å¢åŠ ç¥ç»å…ƒæ•°é‡
            'dropout_rate': (0.1, 0.5),  # é™ä½dropoutèŒƒå›´ï¼Œæé«˜å­¦ä¹ èƒ½åŠ›
            'batch_size': [16, 32, 64, 128],  # å¢åŠ batch sizeé€‰é¡¹
            'attention_units': (16, 64),  # å¢åŠ attention units
            'l1_lambda': (1e-6, 1e-4),  # é™ä½L1æ­£åˆ™åŒ–èŒƒå›´
            'l2_lambda': (1e-6, 1e-4),  # é™ä½L2æ­£åˆ™åŒ–èŒƒå›´
            'optimizer_type': ['adam'],  # TensorFlow 2.10.1åªæ”¯æŒAdam
            'activation': ['relu', 'gelu'],  # æ·»åŠ GELUæ¿€æ´»å‡½æ•°
            'loss_type': ['mse', 'huber']  # æ·»åŠ HuberæŸå¤±
        }
    }

    # ============= ç‰¹å¾é‡è¦æ€§é…ç½®ï¼ˆé˜²è¿‡æ‹Ÿåˆä¼˜åŒ–ï¼‰ =============
    feature_importance = {
        'threshold': 0.05,  # è¿›ä¸€æ­¥æé«˜é˜ˆå€¼ï¼Œå‡å°‘ç‰¹å¾æ•°é‡
        'sample_size': 50000,  # å‡å°‘é‡‡æ ·æ•°é‡ï¼Œé€‚åˆå°æ•°æ®é›†
        'save_plots': True,
        'min_features': 10,  # è¿›ä¸€æ­¥å‡å°‘æœ€å°ç‰¹å¾æ•°
        'max_features': 30  # è¿›ä¸€æ­¥å‡å°‘æœ€å¤§ç‰¹å¾æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    }

    # é›†æˆå­¦ä¹ é…ç½®ï¼ˆé€Ÿåº¦ä¼˜åŒ–ï¼‰
    ensemble_params = {
        'n_splits': 2,  # ä¿æŒè¾ƒå°‘çš„äº¤å‰éªŒè¯æŠ˜æ•°ï¼ŒåŠ å¿«è®­ç»ƒ
        'n_models': 1,  # ä¿æŒå•æ¨¡å‹ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´
        'voting': 'soft',
        'weights': None,
        'bootstrap': False,  # ç¦ç”¨bootstrapé‡‡æ ·ï¼ŒåŠ å¿«è®­ç»ƒ
        'bootstrap_ratio': 0.8  # é™ä½bootstrapé‡‡æ ·æ¯”ä¾‹
    }

    # æ•°æ®å¢å¼ºé…ç½®ï¼ˆé€Ÿåº¦ä¼˜åŒ–ï¼‰``
    augmentation_params = {
        'augmentation_factor': 1.5,  # å‡å°‘æ•°æ®å¢å¼ºæ¯”ä¾‹ï¼ŒåŠ å¿«è®­ç»ƒ
        'noise_factor': 0.02,  # é™ä½å™ªå£°å¼ºåº¦
        'feature_mixing': False,  # ç¦ç”¨ç‰¹å¾æ··åˆï¼ŒåŠ å¿«è®­ç»ƒ
        'random_rotation': False,
        'gaussian_noise': True,  # æ·»åŠ é«˜æ–¯å™ªå£°
        'feature_dropout': 0.2,  # å¢åŠ ç‰¹å¾éšæœºä¸¢å¼ƒæ¯”ä¾‹
        'mixup_alpha': 0.4,  # å¢åŠ Mixupæ•°æ®å¢å¼º
        'cutmix_alpha': 1.0  # CutMixæ•°æ®å¢å¼º
    }

    @staticmethod
    def validate_input_files():
        """éªŒè¯è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        input_files = [
            Config.soil_data_csv,
            Config.test_soil_csv
        ]

        # éªŒè¯è®­ç»ƒæ°”è±¡æ•°æ®æ–‡ä»¶å¤¹
        if not os.path.exists(Config.weather_data_folder):
            raise FileNotFoundError(f"è®­ç»ƒæ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {Config.weather_data_folder}")
        # æ£€æŸ¥æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰CSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(Config.weather_data_folder) if f.endswith('.csv')]
        if not csv_files:
            raise FileNotFoundError(f"è®­ç»ƒæ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰CSVæ–‡ä»¶: {Config.weather_data_folder}")
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªè®­ç»ƒæ°”è±¡æ•°æ®CSVæ–‡ä»¶")

        # éªŒè¯æµ‹è¯•æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹
        if not os.path.exists(Config.test_weather_folder):
            raise FileNotFoundError(f"æµ‹è¯•æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {Config.test_weather_folder}")
        # æ£€æŸ¥æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰CSVæ–‡ä»¶
        test_csv_files = [f for f in os.listdir(Config.test_weather_folder) if f.endswith('.csv')]
        if not test_csv_files:
            raise FileNotFoundError(f"æµ‹è¯•æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰CSVæ–‡ä»¶: {Config.test_weather_folder}")
        print(f"æ‰¾åˆ° {len(test_csv_files)} ä¸ªæµ‹è¯•æ°”è±¡æ•°æ®CSVæ–‡ä»¶")

        missing_files = []
        for file_path in input_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)

        if missing_files:
            raise FileNotFoundError(f"Missing input files: {', '.join(missing_files)}")

        print("All input files found successfully.")

    @staticmethod
    def create_output_dirs():
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•"""
        dirs_to_create = [
            Config.output_dir,
            Config.model_dir,
            Config.final_model_dir,
            Config.label_encoder_dir,
            Config.scaler_dir,
            Config.xgboost_dir,
            Config.logs_dir,
            Config.result_dir,
            Config.feature_importance_dir,
            Config.result_analysis_dir,
            Config.training_analysis_dir
        ]

        for directory in dirs_to_create:
            os.makedirs(directory, exist_ok=True)
            print(f"Created directory: {directory}")

    @staticmethod
    def setup_gpu():
        """é…ç½®GPUè®¾ç½®ï¼Œè¿”å›(use_gpu, num_gpus)"""
        if not Config.use_gpu:
            print("GPU is disabled in config")
            return False, 0
        if not tf.test.is_built_with_cuda():
            print("No CUDA support found")
            return False, 0
        gpus = tf.config.list_physical_devices('GPU')
        if not gpus:
            print("No GPU devices found")
            return False, 0
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"Successfully configured {len(gpus)} GPU(s):")
            for i, gpu in enumerate(gpus):
                print(f"  GPU {i}: {gpu.name}")
            return True, len(gpus)
        except RuntimeError as e:
            print(f"GPU configuration failed: {e}")
            return False, 0

    # ============= æ•°æ®è¯»å–é…ç½® =============
    # é»˜è®¤æ¨¡å¼ï¼šé™åˆ¶æ¯ä¸ªæ°”è±¡æ–‡ä»¶è¯»å–çš„æœ€å¤§è¡Œæ•°ï¼ˆåŸºäºV3ä¼˜åŒ–ç»“æœï¼‰
    max_rows_per_weather_file = 5000000  # å‡å°‘åˆ°500ä¸‡è¡Œï¼Œé¿å…è¿‡åº¦é‡‡æ ·

    # è¡Œæ•°èŒƒå›´æ¨¡å¼ï¼šæŒ‡å®šè¯»å–ç‰¹å®šè¡Œæ•°èŒƒå›´ï¼ˆä¼˜å…ˆçº§é«˜äºmax_rows_per_weather_fileï¼‰
    weather_start_row = 0  # æ°”è±¡æ•°æ®èµ·å§‹è¡Œï¼ˆ0è¡¨ç¤ºä»ç¬¬ä¸€è¡Œå¼€å§‹ï¼‰
    weather_end_row = 50000  # æ°”è±¡æ•°æ®ç»“æŸè¡Œï¼ˆNoneè¡¨ç¤ºè¯»å–åˆ°æ–‡ä»¶æœ«å°¾ï¼‰
    use_weather_row_range = False  # æ˜¯å¦ä½¿ç”¨è¡Œæ•°èŒƒå›´é™åˆ¶

    # æ³¨æ„ï¼šä¸¤ç§æ¨¡å¼äº’æ–¥ä½¿ç”¨ï¼Œè¡Œæ•°èŒƒå›´æ¨¡å¼ä¼˜å…ˆçº§æ›´é«˜

    # æ–°å¢ï¼šæ•°æ®è´¨é‡æ§åˆ¶é…ç½®
    min_weather_coverage = 0.1  # æœ€å°æ°”è±¡æ•°æ®è¦†ç›–ç‡ï¼ˆ10%ï¼‰
    use_soil_primary = True  # æ˜¯å¦ä»¥åœŸå£¤ç‰¹å¾ä¸ºä¸»è¦é¢„æµ‹å› å­
    weather_fallback_strategy = 'interpolate'  # æ°”è±¡æ•°æ®ç¼ºå¤±æ—¶çš„å›é€€ç­–ç•¥


# é…ç½®å…¨å±€è¿›åº¦æ¡æ ·å¼
try:
    if hasattr(tqdm, '_instances'):
        tqdm._instances.clear()  # é˜²æ­¢å¤šä¸ªå®ä¾‹å†²çª
except AttributeError:
    # å¦‚æœtqdmç‰ˆæœ¬ä¸æ”¯æŒ_instanceså±æ€§ï¼Œè·³è¿‡
    pass

try:
    tqdm.pandas(
        bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
except Exception as e:
    print(f"Warning: Could not configure tqdm pandas: {str(e)}")

warnings.filterwarnings('ignore')

# ================= GPU æ£€æµ‹ä¸ç›‘æ§ =================
# å…¨å±€çŠ¶æ€
gpu_monitor_active = False


def detect_device():
    """è‡ªåŠ¨æ£€æµ‹ GPU å¯ç”¨æ€§"""
    if tf.test.is_built_with_cuda() and tf.config.list_physical_devices('GPU'):
        return True
    return False


def setup_multi_gpu():
    """è®¾ç½®å¤šGPUè®­ç»ƒç¯å¢ƒ"""
    try:
        # æ£€æŸ¥TensorFlowæ˜¯å¦æ”¯æŒCUDA
        if not tf.test.is_built_with_cuda():
            print("è­¦å‘Š: TensorFlowæœªç¼–è¯‘CUDAæ”¯æŒï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
            print("å»ºè®®: å®‰è£…æ”¯æŒCUDAçš„TensorFlowç‰ˆæœ¬")
            return False, 0, None
        # Windowsä¸‹å®˜æ–¹TensorFlowä¸æ”¯æŒå¤šGPU NCCLé€šä¿¡
        if platform.system() == 'Windows':
            print('Windowsä¸‹TensorFlowå®˜æ–¹ä¸æ”¯æŒå¤šGPU NCCLé€šä¿¡ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºå•å¡è®­ç»ƒã€‚')
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # åªç”¨ç¬¬ä¸€å¼ å¡
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                tf.config.experimental.set_memory_growth(gpus[0], True)
                print(f"å·²å¯ç”¨GPUå†…å­˜å¢é•¿ï¼Œä»…ä½¿ç”¨GPU: {gpus[0].name}")
                return True, 1, None
            else:
                print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                return False, 0, None
        # è®¾ç½®å¯è§GPUè®¾å¤‡
        if Config.gpu_visible_devices is not None:
            os.environ['CUDA_VISIBLE_DEVICES'] = Config.gpu_visible_devices
            print(f"è®¾ç½®å¯è§GPUè®¾å¤‡: {Config.gpu_visible_devices}")
        # æ£€æµ‹å¯ç”¨çš„GPU
        gpus = tf.config.list_physical_devices('GPU')
        if len(gpus) < 2:
            print(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPUï¼Œéœ€è¦è‡³å°‘2ä¸ªGPUè¿›è¡Œå¤šå¡è®­ç»ƒ")
            if len(gpus) == 1:
                print("å•GPUæ¨¡å¼ï¼šå°†ä½¿ç”¨å•ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
                return True, 1, None
            else:
                print("æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                return False, 0, None
        print(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU:")
        for i, gpu in enumerate(gpus):
            print(f"  GPU {i}: {gpu.name}")
        # åªè®¾ç½®å†…å­˜å¢é•¿ï¼Œä¸è®¾ç½®è™šæ‹Ÿè®¾å¤‡
        if Config.gpu_memory_growth:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("å·²å¯ç”¨GPUå†…å­˜å¢é•¿")
        # åˆ›å»ºMirroredStrategy
        if Config.multi_gpu_enabled and len(gpus) >= 2:
            strategy = tf.distribute.MirroredStrategy()
            print(f"æˆåŠŸåˆ›å»ºMirroredStrategyï¼Œå°†ä½¿ç”¨ {strategy.num_replicas_in_sync} ä¸ªGPU")
            return True, len(gpus), strategy
        else:
            print("å¤šGPUè®­ç»ƒå·²ç¦ç”¨æˆ–GPUæ•°é‡ä¸è¶³ï¼Œå°†ä½¿ç”¨å•GPU")
            return True, len(gpus), None
    except Exception as e:
        print(f"å¤šGPUè®¾ç½®å¤±è´¥: {str(e)}")
        print("å°†å›é€€åˆ°CPUæ¨¡å¼")
        return False, 0, None


def gpu_monitor(interval=5):
    """åå°çº¿ç¨‹ï¼šå®šæ—¶æ‰“å° GPU ä½¿ç”¨æƒ…å†µ"""
    global gpu_monitor_active
    while gpu_monitor_active:
        if GPUtil:
            gpus = GPUtil.getGPUs()
            if gpus:
                for i, gpu in enumerate(gpus):
                    print(
                        f"[GPU Monitor] GPU {i}: Usage: {gpu.load * 100:.1f}% | Memory: {gpu.memoryUsed}/{gpu.memoryTotal} MB")
        else:
            # å¤‡ç”¨ï¼šä½¿ç”¨ TensorFlow API
            try:
                for i in range(len(tf.config.list_physical_devices('GPU'))):
                    memory_info = tf.config.experimental.get_memory_info(f'GPU:{i}')
                    print(f"[GPU Monitor] GPU {i}: Current: {memory_info['current'] // 1024 ** 2} MiB | "
                          f"Peak: {memory_info['peak'] // 1024 ** 2} MiB")
            except:
                pass
        time.sleep(interval)


def memory_safe(func):
    """å†…å­˜å®‰å…¨è£…é¥°å™¨"""

    def wrapper(*args, **kwargs):
        process = psutil.Process()
        mem = process.memory_info().rss / 1024 ** 3  # GB
        if mem > 10:  # å½“å†…å­˜è¶…è¿‡10GBæ—¶æŠ¥è­¦
            print(f"Memory warning: {mem:.2f}GB used")
        return func(*args, **kwargs)

    return wrapper


# æ‰“å° GPU ä¿¡æ¯
print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
print(f"CUDAæ˜¯å¦å¯ç”¨: {tf.test.is_built_with_cuda()}")
if tf.test.is_built_with_cuda():
    gpus = tf.config.list_physical_devices('GPU')
    print(f"æ£€æµ‹åˆ°çš„GPUæ•°é‡: {len(gpus)}")
    for i, gpu in enumerate(gpus):
        print(f"GPU {i} è®¾å¤‡åç§°: {gpu.name}")

    # è®¾ç½®å¤šGPUç¯å¢ƒ
    multi_gpu_available, num_gpus, strategy = setup_multi_gpu()
    if multi_gpu_available:
        print(f"å¤šGPUè®­ç»ƒç¯å¢ƒè®¾ç½®æˆåŠŸï¼Œå°†ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
    else:
        print("å¤šGPUè®­ç»ƒç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨å•GPUæˆ–CPU")
        strategy = None


##############################################
# 1. æ•°æ®é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹
##############################################

def unify_coordinates_with_tolerance(weather_data, soil_data, tolerance=100):
    """
    ç»Ÿä¸€æ°”è±¡æ•°æ®å’ŒåœŸå£¤æ•°æ®çš„åæ ‡ç³»ç»Ÿï¼Œä»¥åœŸå£¤æ•°æ®xyåæ ‡ä¸ºåŸºå‡†
    ä½¿ç”¨é«˜æ•ˆçš„å‘é‡åŒ–ç®—æ³•å’Œç©ºé—´ç´¢å¼•ä¼˜åŒ–

    Args:
        weather_data (pd.DataFrame): æ°”è±¡æ•°æ®
        soil_data (pd.DataFrame): åœŸå£¤æ•°æ®ï¼ˆä½œä¸ºåæ ‡åŸºå‡†ï¼‰
        tolerance (float): åæ ‡å®¹å·®ï¼Œé»˜è®¤100ç±³

    Returns:
        pd.DataFrame: åæ ‡ç»Ÿä¸€åçš„æ°”è±¡æ•°æ®
    """
    print(f"å¼€å§‹åæ ‡ç»Ÿä¸€ï¼Œå®¹å·®: {tolerance}ç±³")
    print(f"æ°”è±¡æ•°æ®åŸå§‹å½¢çŠ¶: {weather_data.shape}")
    print(f"åœŸå£¤æ•°æ®å½¢çŠ¶: {soil_data.shape}")

    # ç¡®ä¿åæ ‡åˆ—å­˜åœ¨
    if 'x' not in weather_data.columns or 'y' not in weather_data.columns:
        raise ValueError("æ°”è±¡æ•°æ®ä¸­ç¼ºå°‘x, yåæ ‡åˆ—")
    if 'x' not in soil_data.columns or 'y' not in soil_data.columns:
        raise ValueError("åœŸå£¤æ•°æ®ä¸­ç¼ºå°‘x, yåæ ‡åˆ—")

    # è·å–åœŸå£¤æ•°æ®çš„å”¯ä¸€åæ ‡ç‚¹
    soil_coords = soil_data[['x', 'y']].drop_duplicates().reset_index(drop=True)
    print(f"åœŸå£¤æ•°æ®å”¯ä¸€åæ ‡ç‚¹æ•°: {len(soil_coords)}")

    # è·å–æ°”è±¡æ•°æ®çš„å”¯ä¸€åæ ‡ç‚¹
    weather_coords = weather_data[['x', 'y']].drop_duplicates().reset_index(drop=True)
    print(f"æ°”è±¡æ•°æ®å”¯ä¸€åæ ‡ç‚¹æ•°: {len(weather_coords)}")

    # ä½¿ç”¨é«˜æ•ˆçš„ç©ºé—´ç´¢å¼•ç®—æ³•
    print("ä½¿ç”¨é«˜æ•ˆç©ºé—´ç´¢å¼•ç®—æ³•è¿›è¡Œåæ ‡åŒ¹é…...")

    # å°†åæ ‡è½¬æ¢ä¸ºnumpyæ•°ç»„
    weather_coords_array = weather_coords[['x', 'y']].values
    soil_coords_array = soil_coords[['x', 'y']].values

    # åˆ›å»ºåæ ‡æ˜ å°„å­—å…¸
    coord_mapping = {}
    matched_count = 0

    # ä½¿ç”¨KDTreeè¿›è¡Œå¿«é€Ÿæœ€è¿‘é‚»æœç´¢
    try:
        from scipy.spatial import cKDTree
        print("ä½¿ç”¨scipy.spatial.cKDTreeè¿›è¡Œå¿«é€ŸåŒ¹é…...")

        # æ„å»ºåœŸå£¤åæ ‡çš„KDTree
        soil_tree = cKDTree(soil_coords_array)

        # åˆ†æ‰¹å¤„ç†æ°”è±¡åæ ‡
        batch_size = 100000  # è¿›ä¸€æ­¥å¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œæé«˜å¤„ç†æ•ˆç‡
        total_batches = (len(weather_coords) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(weather_coords))

            print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({start_idx}-{end_idx})")

            # è·å–å½“å‰æ‰¹æ¬¡çš„åæ ‡
            batch_weather = weather_coords_array[start_idx:end_idx]

            # ä½¿ç”¨KDTreeæŸ¥è¯¢æœ€è¿‘é‚»
            distances, indices = soil_tree.query(batch_weather, k=1)

            # å¤„ç†åŒ¹é…ç»“æœ
            for i, (dist, idx) in enumerate(zip(distances, indices)):
                if dist <= tolerance:
                    wx, wy = batch_weather[i]
                    soil_x, soil_y = soil_coords_array[idx]
                    coord_mapping[(wx, wy)] = (soil_x, soil_y)
                    matched_count += 1

    except ImportError:
        print("scipyä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å‘é‡åŒ–ç®—æ³•...")

        # åˆ†æ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡
        batch_size = 20000  # å¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œæé«˜å¤„ç†æ•ˆç‡
        total_batches = (len(weather_coords) + batch_size - 1) // batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(weather_coords))

            print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({start_idx}-{end_idx})")

            # è·å–å½“å‰æ‰¹æ¬¡çš„åæ ‡
            batch_weather = weather_coords_array[start_idx:end_idx]

            # å‘é‡åŒ–è®¡ç®—è·ç¦»
            for i, (wx, wy) in enumerate(batch_weather):
                # è®¡ç®—åˆ°æ‰€æœ‰åœŸå£¤åæ ‡ç‚¹çš„è·ç¦»
                distances = np.sqrt((soil_coords_array[:, 0] - wx) ** 2 + (soil_coords_array[:, 1] - wy) ** 2)
                min_distance = np.min(distances)
                min_idx = np.argmin(distances)

                # å¦‚æœè·ç¦»åœ¨å®¹å·®èŒƒå›´å†…ï¼Œä½¿ç”¨åœŸå£¤åæ ‡
                if min_distance <= tolerance:
                    soil_x, soil_y = soil_coords_array[min_idx]
                    coord_mapping[(wx, wy)] = (soil_x, soil_y)
                    matched_count += 1

    print(
        f"æˆåŠŸåŒ¹é…çš„åæ ‡ç‚¹æ•°: {matched_count}/{len(weather_coords)} ({matched_count / len(weather_coords) * 100:.1f}%)")

    # åº”ç”¨åæ ‡æ˜ å°„
    print("åº”ç”¨åæ ‡æ˜ å°„...")
    weather_corrected = weather_data.copy()
    weather_corrected['x_original'] = weather_corrected['x']
    weather_corrected['y_original'] = weather_corrected['y']

    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›´æ–°åæ ‡
    for (wx, wy), (sx, sy) in coord_mapping.items():
        mask = (weather_corrected['x'] == wx) & (weather_corrected['y'] == wy)
        weather_corrected.loc[mask, 'x'] = sx
        weather_corrected.loc[mask, 'y'] = sy

    # ç§»é™¤æ— æ³•åŒ¹é…çš„æ•°æ®ç‚¹
    print("ç§»é™¤æ— æ³•åŒ¹é…çš„æ•°æ®ç‚¹...")
    valid_coords = set(coord_mapping.values())
    weather_corrected = weather_corrected[
        weather_corrected.apply(lambda row: (row['x'], row['y']) in valid_coords, axis=1)
    ]

    print(f"åæ ‡ç»Ÿä¸€åæ°”è±¡æ•°æ®å½¢çŠ¶: {weather_corrected.shape}")
    print(f"åæ ‡ç»Ÿä¸€æˆåŠŸç‡: {len(weather_corrected) / len(weather_data) * 100:.1f}%")

    return weather_corrected


def quick_coordinate_unify(weather_data, soil_data, tolerance=100):
    """
    å¿«é€Ÿåæ ‡ç»Ÿä¸€æ–¹æ³•ï¼Œä½¿ç”¨ç®€å•çš„å››èˆäº”å…¥ç­–ç•¥
    """
    print(f"ä½¿ç”¨å¿«é€Ÿåæ ‡ç»Ÿä¸€ï¼Œå®¹å·®: {tolerance}ç±³")
    print(f"æ°”è±¡æ•°æ®å½¢çŠ¶: {weather_data.shape}")
    print(f"åœŸå£¤æ•°æ®å½¢çŠ¶: {soil_data.shape}")

    # è·å–åœŸå£¤æ•°æ®çš„åæ ‡èŒƒå›´
    soil_x_min, soil_x_max = soil_data['x'].min(), soil_data['x'].max()
    soil_y_min, soil_y_max = soil_data['y'].min(), soil_data['y'].max()

    print(f"åœŸå£¤æ•°æ®åæ ‡èŒƒå›´: x[{soil_x_min:.2f}, {soil_x_max:.2f}], y[{soil_y_min:.2f}, {soil_y_max:.2f}]")

    # è·å–æ°”è±¡æ•°æ®çš„åæ ‡èŒƒå›´
    weather_x_min, weather_x_max = weather_data['x'].min(), weather_data['x'].max()
    weather_y_min, weather_y_max = weather_data['y'].min(), weather_data['y'].max()

    print(f"æ°”è±¡æ•°æ®åæ ‡èŒƒå›´: x[{weather_x_min:.2f}, {weather_x_max:.2f}], y[{weather_y_min:.2f}, {weather_y_max:.2f}]")

    # åˆ›å»ºåæ ‡æ˜ å°„
    weather_corrected = weather_data.copy()
    weather_corrected['x_original'] = weather_corrected['x']
    weather_corrected['y_original'] = weather_corrected['y']

    # ä½¿ç”¨ç®€å•çš„åæ ‡å¯¹é½ç­–ç•¥
    # å°†æ°”è±¡åæ ‡å››èˆäº”å…¥åˆ°æœ€è¿‘çš„æ•´æ•°ï¼Œç„¶åä¸åœŸå£¤åæ ‡åŒ¹é…
    print("åº”ç”¨åæ ‡å¯¹é½ç­–ç•¥...")

    # è·å–åœŸå£¤æ•°æ®çš„å”¯ä¸€åæ ‡ç‚¹
    soil_coords = soil_data[['x', 'y']].drop_duplicates()
    soil_coords_set = set(zip(soil_coords['x'], soil_coords['y']))

    matched_count = 0
    total_count = len(weather_corrected)

    # åˆ†æ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡
    batch_size = 200000  # å¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œæé«˜å¤„ç†æ•ˆç‡
    for i in range(0, total_count, batch_size):
        end_idx = min(i + batch_size, total_count)
        batch = weather_corrected.iloc[i:end_idx]

        # å¯¹åæ ‡è¿›è¡Œå››èˆäº”å…¥
        rounded_x = np.round(batch['x']).astype(int)
        rounded_y = np.round(batch['y']).astype(int)

        # æ£€æŸ¥æ˜¯å¦åœ¨åœŸå£¤åæ ‡èŒƒå›´å†…
        valid_mask = (
                (rounded_x >= soil_x_min) & (rounded_x <= soil_x_max) &
                (rounded_y >= soil_y_min) & (rounded_y <= soil_y_max)
        )

        # æ›´æ–°æœ‰æ•ˆåæ ‡
        weather_corrected.iloc[i:end_idx, weather_corrected.columns.get_loc('x')] = np.where(
            valid_mask, rounded_x, weather_corrected.iloc[i:end_idx]['x']
        )
        weather_corrected.iloc[i:end_idx, weather_corrected.columns.get_loc('y')] = np.where(
            valid_mask, rounded_y, weather_corrected.iloc[i:end_idx]['y']
        )

        matched_count += valid_mask.sum()

        if i % (batch_size * 10) == 0:
            print(f"  å¤„ç†è¿›åº¦: {i}/{total_count} ({i / total_count * 100:.1f}%)")

    # ç§»é™¤æ— æ³•åŒ¹é…çš„æ•°æ®ç‚¹
    print("ç§»é™¤æ— æ³•åŒ¹é…çš„æ•°æ®ç‚¹...")
    valid_coords = set(zip(weather_corrected['x'], weather_corrected['y']))
    soil_coords_set = set(zip(soil_data['x'], soil_data['y']))

    # åªä¿ç•™åœ¨åœŸå£¤åæ ‡èŒƒå›´å†…çš„æ•°æ®
    weather_corrected = weather_corrected[
        (weather_corrected['x'] >= soil_x_min) & (weather_corrected['x'] <= soil_x_max) &
        (weather_corrected['y'] >= soil_y_min) & (weather_corrected['y'] <= soil_y_max)
        ]

    print(f"åæ ‡ç»Ÿä¸€åæ°”è±¡æ•°æ®å½¢çŠ¶: {weather_corrected.shape}")
    print(f"åæ ‡ç»Ÿä¸€æˆåŠŸç‡: {len(weather_corrected) / len(weather_data) * 100:.1f}%")

    return weather_corrected


def load_weather_data_from_folder(folder_path, start_row=None, end_row=None):
    """
    ä»æŒ‡å®šæ–‡ä»¶å¤¹ä¸­è¯»å–æ‰€æœ‰æ°”è±¡æ•°æ®CSVæ–‡ä»¶å¹¶åˆå¹¶
    æ”¯æŒæŒ‡å®šè¡Œæ•°èŒƒå›´è¯»å–

    Args:
        folder_path (str): æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        start_row (int, optional): èµ·å§‹è¡Œæ•°ï¼ˆ0-basedï¼ŒåŒ…å«æ­¤è¡Œï¼‰
        end_row (int, optional): ç»“æŸè¡Œæ•°ï¼ˆ0-basedï¼Œä¸åŒ…å«æ­¤è¡Œï¼ŒNoneè¡¨ç¤ºåˆ°æ–‡ä»¶æœ«å°¾ï¼‰

    Returns:
        pd.DataFrame: åˆå¹¶åçš„æ°”è±¡æ•°æ®
    """
    print(f"ä»æ–‡ä»¶å¤¹è¯»å–æ°”è±¡æ•°æ®: {folder_path}")

    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")

    # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    if not csv_files:
        raise FileNotFoundError(f"æ°”è±¡æ•°æ®æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰CSVæ–‡ä»¶: {folder_path}")

    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶: {csv_files}")

    # ç¡®å®šè¯»å–ç­–ç•¥
    if start_row is not None or end_row is not None:
        print(f"ä½¿ç”¨è¡Œæ•°èŒƒå›´è¯»å–: èµ·å§‹è¡Œ={start_row}, ç»“æŸè¡Œ={end_row}")
        use_range_mode = True
    else:
        print(f"ä½¿ç”¨é»˜è®¤æ¨¡å¼: æ¯ä¸ªæ–‡ä»¶æœ€å¤šè¯»å– {Config.max_rows_per_weather_file:,} è¡Œæ•°æ®")
        use_range_mode = False

    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰CSVæ–‡ä»¶
    weather_data_list = []

    for csv_file in tqdm(csv_files, desc="è¯»å–æ°”è±¡æ•°æ®æ–‡ä»¶"):
        file_path = os.path.join(folder_path, csv_file)
        try:
            print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {csv_file}")

            if use_range_mode:
                # è¡Œæ•°èŒƒå›´æ¨¡å¼
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        total_lines = sum(1 for _ in f)
                    print(f"  æ–‡ä»¶æ€»è¡Œæ•°: {total_lines:,}")

                    # è®¡ç®—å®é™…è¯»å–èŒƒå›´
                    actual_start = start_row if start_row is not None else 0
                    actual_end = end_row if end_row is not None else total_lines

                    # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
                    actual_start = max(0, actual_start)
                    actual_end = min(total_lines, actual_end)

                    if actual_start >= actual_end:
                        print(f"  âš ï¸ èµ·å§‹è¡Œ({actual_start}) >= ç»“æŸè¡Œ({actual_end})ï¼Œè·³è¿‡æ­¤æ–‡ä»¶")
                        continue

                    rows_to_skip = actual_start
                    rows_to_read = actual_end - actual_start

                    print(f"  è¯»å–èŒƒå›´: ç¬¬{actual_start + 1}è¡Œåˆ°ç¬¬{actual_end}è¡Œ (å…±{rows_to_read:,}è¡Œ)")

                except Exception as e:
                    print(f"  è­¦å‘Š: æ— æ³•ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼Œè·³è¿‡æ­¤æ–‡ä»¶: {str(e)}")
                    continue
            else:
                # é»˜è®¤æ¨¡å¼
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        total_lines = sum(1 for _ in f)
                    print(f"  æ–‡ä»¶æ€»è¡Œæ•°: {total_lines:,}")

                    # å¦‚æœæ–‡ä»¶è¡Œæ•°è¶…è¿‡é™åˆ¶ï¼Œåªè¯»å–å‰Nè¡Œ
                    if total_lines > Config.max_rows_per_weather_file:
                        print(f"  âš ï¸ æ–‡ä»¶è¿‡å¤§ï¼Œåªè¯»å–å‰ {Config.max_rows_per_weather_file:,} è¡Œ")
                        rows_to_read = Config.max_rows_per_weather_file
                    else:
                        rows_to_read = total_lines

                    rows_to_skip = 0

                except Exception as e:
                    print(f"  è­¦å‘Š: æ— æ³•ç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼Œä½¿ç”¨é»˜è®¤é™åˆ¶: {str(e)}")
                    rows_to_read = Config.max_rows_per_weather_file
                    rows_to_skip = 0

            # è¯»å–æŒ‡å®šè¡Œæ•°çš„æ•°æ®
            try:
                if use_range_mode and rows_to_skip > 0:
                    # ä½¿ç”¨skiprowså’Œnrowså‚æ•°
                    df = pd.read_csv(file_path, encoding='utf-8',
                                     skiprows=range(1, rows_to_skip + 1), nrows=rows_to_read)
                else:
                    # ä½¿ç”¨nrowså‚æ•°
                    df = pd.read_csv(file_path, encoding='utf-8', nrows=rows_to_read)
            except UnicodeDecodeError:
                try:
                    if use_range_mode and rows_to_skip > 0:
                        df = pd.read_csv(file_path, encoding='gbk',
                                         skiprows=range(1, rows_to_skip + 1), nrows=rows_to_read)
                    else:
                        df = pd.read_csv(file_path, encoding='gbk', nrows=rows_to_read)
                except UnicodeDecodeError:
                    if use_range_mode and rows_to_skip > 0:
                        df = pd.read_csv(file_path, encoding='latin1',
                                         skiprows=range(1, rows_to_skip + 1), nrows=rows_to_read)
                    else:
                        df = pd.read_csv(file_path, encoding='latin1', nrows=rows_to_read)

            # æ·»åŠ æ–‡ä»¶åä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            df['source_file'] = csv_file

            weather_data_list.append(df)
            print(f"  âœ… æˆåŠŸè¯»å–: {df.shape[0]:,} è¡Œ Ã— {df.shape[1]} åˆ—")

        except Exception as e:
            print(f"  âŒ è¯»å–æ–‡ä»¶ {csv_file} å¤±è´¥: {str(e)}")
            continue

    if not weather_data_list:
        raise ValueError("æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ°”è±¡æ•°æ®æ–‡ä»¶")

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print("åˆå¹¶æ°”è±¡æ•°æ®æ–‡ä»¶...")
    weather_data = pd.concat(weather_data_list, ignore_index=True)

    print(f"åˆå¹¶åçš„æ°”è±¡æ•°æ®å½¢çŠ¶: {weather_data.shape}")
    print(f"åˆå¹¶åçš„æ°”è±¡æ•°æ®åˆ—: {weather_data.columns.tolist()}")

    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ•°æ®
    if 'source_file' in weather_data.columns:
        print("å„æ–‡ä»¶æ•°æ®é‡ç»Ÿè®¡:")
        print(weather_data['source_file'].value_counts())
        # ç§»é™¤source_fileåˆ—ï¼Œé¿å…å½±å“åç»­å¤„ç†
        weather_data = weather_data.drop(columns=['source_file'])

    # ä¸è¿›è¡Œé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ°”è±¡æ•°æ®
    print(f"ä½¿ç”¨å…¨éƒ¨æ°”è±¡æ•°æ®: {weather_data.shape[0]:,} è¡Œ")

    return weather_data


@memory_safe
def load_data():
    print("\n1/4.Loading and preprocessing data...")
    try:
        # ============= åŠ è½½è®­ç»ƒæ•°æ® =============
        # 1) åŠ è½½è®­ç»ƒé›†æ°”è±¡æ•°æ®
        print("ä½¿ç”¨æ–‡ä»¶å¤¹æ¨¡å¼åŠ è½½è®­ç»ƒæ°”è±¡æ•°æ®...")
        weather_train = load_weather_data_from_folder(Config.weather_data_folder)
        print(f"Weather data shape: {weather_train.shape}")
        print(f"Weather data columns: {weather_train.columns.tolist()}")

        # 2) åŠ è½½è®­ç»ƒé›†åœŸå£¤æ•°æ®
        print("Loading training soil data from:", Config.soil_data_csv)
        try:
            soil_train = pd.read_csv(Config.soil_data_csv, encoding='utf-8')
        except UnicodeDecodeError:
            soil_train = pd.read_csv(Config.soil_data_csv, encoding='gbk')
        print(f"Soil data shape: {soil_train.shape}")
        print(f"Soil data columns: {soil_train.columns.tolist()}")

        # ä¸è¿›è¡Œé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨åœŸå£¤æ•°æ®
        print(f"ä½¿ç”¨å…¨éƒ¨åœŸå£¤æ•°æ®: {soil_train.shape[0]:,} è¡Œ")

        # æ£€æŸ¥åœŸå£¤æ•°æ®åˆ—åå¹¶é‡å‘½åä»¥åŒ¹é…äº§å“æ•°æ®
        if 'TZ' in soil_train.columns:
            print("æ£€æµ‹åˆ°åœŸå£¤æ•°æ®ä½¿ç”¨ TZ åˆ—åï¼Œé‡å‘½åä¸º tz...")
            soil_train = soil_train.rename(columns={'TZ': 'tz'})

        # ============= åæ ‡ç»Ÿä¸€ =============
        print("\n=== å¼€å§‹åæ ‡ç»Ÿä¸€å¤„ç† ===")

        # å…ˆé‡å‘½åæ°”è±¡æ•°æ®åæ ‡åˆ—ä»¥åŒ¹é…åœŸå£¤æ•°æ®
        if 'Lon' in weather_train.columns and 'Lat' in weather_train.columns:
            print("é‡å‘½åæ°”è±¡æ•°æ®åæ ‡åˆ—: Lon/Lat -> x/y")
            weather_train = weather_train.rename(columns={'Lon': 'x', 'Lat': 'y'})
        elif 'lon' in weather_train.columns and 'lat' in weather_train.columns:
            print("é‡å‘½åæ°”è±¡æ•°æ®åæ ‡åˆ—: lon/lat -> x/y")
            weather_train = weather_train.rename(columns={'lon': 'x', 'lat': 'y'})

        # ä½¿ç”¨å¿«é€Ÿåæ ‡ç»Ÿä¸€ï¼ˆè·³è¿‡å¤æ‚çš„åŒ¹é…ç®—æ³•ï¼‰
        print("ä½¿ç”¨å¿«é€Ÿåæ ‡ç»Ÿä¸€æ–¹æ³•...")
        weather_train = quick_coordinate_unify(weather_train, soil_train, tolerance=100)
        print("=== åæ ‡ç»Ÿä¸€å®Œæˆ ===\n")

        # 3) åŠ è½½è®­ç»ƒé›†äº§é‡/é€‚å®œåº¦æ•°æ®
        print("Loading training product data from:", Config.product_data_csv)
        if not os.path.exists(Config.product_data_csv):
            print(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {Config.product_data_csv}")
            print("å½“å‰ç›®å½•ä¸‹å¯ç”¨çš„csvæ–‡ä»¶:")
            for f in os.listdir(os.path.dirname(Config.product_data_csv)):
                if f.endswith('.csv'):
                    print(f)
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒé›†äº§é‡/é€‚å®œåº¦æ•°æ®æ–‡ä»¶: {Config.product_data_csv}")
        product_train = pd.read_csv(Config.product_data_csv, encoding='utf-8')
        print(f"Product data shape: {product_train.shape}")
        print(f"Product data columns: {product_train.columns.tolist()}")

        # ============= ç©ºé—´è¿‘é‚»åˆå¹¶ =============
        # ä½¿ç”¨å…¨å±€å®šä¹‰çš„å‡½æ•°

        # ç»Ÿä¸€åˆ—åå°å†™
        product_train.columns = product_train.columns.str.strip().str.lower()
        weather_train.columns = weather_train.columns.str.strip().str.lower()
        soil_train.columns = soil_train.columns.str.strip().str.lower()

        # æ¸…ç†æ°”è±¡æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼Œåªä¿ç•™å®Œæ•´çš„æ°”è±¡æ•°æ®
        print("æ¸…ç†æ°”è±¡æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ...")
        original_weather_shape = weather_train.shape
        print(f"åŸå§‹æ°”è±¡æ•°æ®å½¢çŠ¶: {original_weather_shape}")

        # è¯†åˆ«æ°”è±¡ç‰¹å¾åˆ—ï¼ˆæ’é™¤åæ ‡ã€å¹´ä»½ã€æœˆä»½ç­‰éæ°”è±¡ç‰¹å¾ï¼‰
        weather_feature_cols = [col for col in weather_train.columns
                                if any(keyword in col.lower() for keyword in
                                       ['tsun', 'tave', 'tmax', 'tmin', 'rain', 'gtave', 'gtmax', 'gtmin', 'sevp'])]

        print(f"æ°”è±¡ç‰¹å¾åˆ—æ•°é‡: {len(weather_feature_cols)}")
        print(f"æ°”è±¡ç‰¹å¾åˆ—: {weather_feature_cols[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªåˆ—å

        # åˆ é™¤ä»»ä½•æ°”è±¡ç‰¹å¾åˆ—æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        if weather_feature_cols:
            # ä¿ç•™æ‰€æœ‰æ°”è±¡ç‰¹å¾åˆ—éƒ½ä¸ä¸ºNaNçš„è¡Œ
            weather_train = weather_train.dropna(subset=weather_feature_cols)
            print(f"åˆ é™¤ç¼ºå¤±å€¼åæ°”è±¡æ•°æ®å½¢çŠ¶: {weather_train.shape}")
            print(f"åˆ é™¤äº† {original_weather_shape[0] - weather_train.shape[0]} è¡Œæœ‰ç¼ºå¤±å€¼çš„æ•°æ®")
            print(f"ä¿ç•™ç‡: {weather_train.shape[0] / original_weather_shape[0] * 100:.2f}%")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ°”è±¡ç‰¹å¾åˆ—ï¼Œè·³è¿‡ç¼ºå¤±å€¼æ¸…ç†")

        # æ¸…ç†äº§å“æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        print("\næ¸…ç†äº§å“æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ...")
        original_product_shape = product_train.shape
        print(f"åŸå§‹äº§å“æ•°æ®å½¢çŠ¶: {original_product_shape}")

        # åˆ é™¤äº§å“æ•°æ®ä¸­æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        product_train = product_train.dropna()
        print(f"åˆ é™¤ç¼ºå¤±å€¼åäº§å“æ•°æ®å½¢çŠ¶: {product_train.shape}")
        print(f"åˆ é™¤äº† {original_product_shape[0] - product_train.shape[0]} è¡Œæœ‰ç¼ºå¤±å€¼çš„æ•°æ®")
        print(f"ä¿ç•™ç‡: {product_train.shape[0] / original_product_shape[0] * 100:.2f}%")

        # æ¸…ç†åœŸå£¤æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        print("\næ¸…ç†åœŸå£¤æ•°æ®ï¼šåˆ é™¤æœ‰ç¼ºå¤±å€¼çš„è¡Œ...")
        original_soil_shape = soil_train.shape
        print(f"åŸå§‹åœŸå£¤æ•°æ®å½¢çŠ¶: {original_soil_shape}")

        # åˆ é™¤åœŸå£¤æ•°æ®ä¸­æœ‰ç¼ºå¤±å€¼çš„è¡Œ
        soil_train = soil_train.dropna()
        print(f"åˆ é™¤ç¼ºå¤±å€¼ååœŸå£¤æ•°æ®å½¢çŠ¶: {soil_train.shape}")
        print(f"åˆ é™¤äº† {original_soil_shape[0] - soil_train.shape[0]} è¡Œæœ‰ç¼ºå¤±å€¼çš„æ•°æ®")
        print(f"ä¿ç•™ç‡: {soil_train.shape[0] / original_soil_shape[0] * 100:.2f}%")

        # ç»Ÿä¸€å¹´ä»½ç±»å‹åˆ°æ•´æ•°ï¼Œé¿å… 1995.0 ä¸ 1995 ä¸ç›¸ç­‰å¯¼è‡´åˆå¹¶ä¸ºç©º
        for df_name, df in [('product_train', product_train), ('weather_train', weather_train)]:
            if 'yyyy' in df.columns:
                try:
                    df['yyyy'] = pd.to_numeric(df['yyyy'], errors='coerce').round().astype('Int64')
                    print(f"{df_name} å¹´ä»½ç¤ºä¾‹: {df['yyyy'].dropna().unique()[:5]}")
                except Exception as e:
                    print(f"è­¦å‘Š: è§„èŒƒåŒ– {df_name}.yyyy åˆ°æ•´æ•°å¤±è´¥: {e}")

        # ä¸è¿›è¡Œé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œåˆå¹¶
        print("ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œåˆå¹¶...")
        print(f"äº§å“æ•°æ®å½¢çŠ¶: {product_train.shape}")
        print(f"æ°”è±¡æ•°æ®å½¢çŠ¶: {weather_train.shape}")

        # äº§é‡ä¸æ°”è±¡åˆå¹¶ï¼šä¼˜å…ˆç²¾ç¡®é”®åˆå¹¶ï¼Œå…¶æ¬¡ç©ºé—´è¿‘é‚»åˆå¹¶
        print("å°è¯•æŒ‰ ['x','y','yyyy'] ç²¾ç¡®é”®åˆå¹¶ product ä¸ weather ...")

        # æ£€æŸ¥æ°”è±¡æ•°æ®åˆ—åå¹¶é‡å‘½åä»¥åŒ¹é…äº§å“æ•°æ®
        print(f"æ°”è±¡æ•°æ®åˆ—å: {weather_train.columns.tolist()}")

        # æ£€æŸ¥å¹¶é‡å‘½ååæ ‡åˆ—
        if 'Lon' in weather_train.columns and 'Lat' in weather_train.columns:
            print("æ£€æµ‹åˆ°æ°”è±¡æ•°æ®ä½¿ç”¨ Lon/Lat åˆ—åï¼Œé‡å‘½åä¸º x/y...")
            weather_train = weather_train.rename(columns={'Lon': 'x', 'Lat': 'y'})
        elif 'lon' in weather_train.columns and 'lat' in weather_train.columns:
            print("æ£€æµ‹åˆ°æ°”è±¡æ•°æ®ä½¿ç”¨ lon/lat åˆ—åï¼Œé‡å‘½åä¸º x/y...")
            weather_train = weather_train.rename(columns={'lon': 'x', 'lat': 'y'})

        # æ£€æŸ¥å¹¶é‡å‘½åå¹´ä»½åˆ—
        if 'YYYY' in weather_train.columns:
            print("æ£€æµ‹åˆ°æ°”è±¡æ•°æ®ä½¿ç”¨ YYYY åˆ—åï¼Œé‡å‘½åä¸º yyyy...")
            weather_train = weather_train.rename(columns={'YYYY': 'yyyy'})
        elif 'yyyy' in weather_train.columns:
            print("æ°”è±¡æ•°æ®å¹´ä»½åˆ—å·²ç»æ˜¯ yyyyï¼Œæ— éœ€é‡å‘½å")

        print(f"é‡å‘½ååæ°”è±¡æ•°æ®åˆ—å: {weather_train.columns.tolist()}")

        # ä¸ºä¿æŒåç»­ç‰¹å¾å‘½åä¸€è‡´ï¼Œå°† weather åˆ—åŠ ä¸Š right_ å‰ç¼€ï¼Œå†åŸºäºå¯¹åº”é”®åˆå¹¶
        weather_pref = weather_train.add_prefix('right_')
        merged_pw = pd.merge(
            product_train,
            weather_pref,
            left_on=['x', 'y', 'yyyy'],
            right_on=['right_x', 'right_y', 'right_yyyy'],
            how='inner'
        )
        if merged_pw.shape[0] == 0:
            print("ç²¾ç¡®é”®åˆå¹¶ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°ç©ºé—´+æ—¶é—´è¿‘é‚»åˆå¹¶ï¼ˆåæ ‡è¯¯å·®æ§åˆ¶åœ¨50å…¬é‡Œå†…ï¼‰...")
            merged_pw = spatial_temporal_merge(product_train, weather_train, xy_cols=['x', 'y'], time_col='yyyy',
                                               tolerance=50000)
        print(f"After product-weather merge: {merged_pw.shape}")
        print(f"merged_pw columns: {merged_pw.columns.tolist()}")
        # åŠ¨æ€æ£€æµ‹å¹¶é‡å‘½åæ‰€æœ‰x, yåˆ—ï¼Œä¸»é”®x, yåªä¿ç•™productçš„x, y
        x_cols = [col for col in merged_pw.columns if col.lower() == 'x']
        y_cols = [col for col in merged_pw.columns if col.lower() == 'y']
        xw_cols = [col for col in merged_pw.columns if col.lower() == 'right_x']
        yw_cols = [col for col in merged_pw.columns if col.lower() == 'right_y']

        print("x_cols:", x_cols)
        print("y_cols:", y_cols)
        print("xw_cols:", xw_cols)
        print("yw_cols:", yw_cols)

        if x_cols and y_cols and xw_cols and yw_cols:
            merged_pw = merged_pw.rename(columns={
                x_cols[0]: 'x_product',
                y_cols[0]: 'y_product',
                xw_cols[0]: 'x_weather',
                yw_cols[0]: 'y_weather',
            })
            merged_pw['x'] = merged_pw['x_product']
            merged_pw['y'] = merged_pw['y_product']
            keep_cols = [col for col in merged_pw.columns if col not in ['x_weather', 'y_weather']]
            merged_pw = merged_pw[keep_cols]
        else:
            raise ValueError(
                f"merged_pwä¸­x, yåˆ—æ•°é‡å¼‚å¸¸: x_cols={x_cols}, y_cols={y_cols}, xw_cols={xw_cols}, yw_cols={yw_cols}ï¼Œè¯·æ£€æŸ¥åˆå¹¶é€»è¾‘ï¼")
        if 'x' not in merged_pw.columns or 'y' not in merged_pw.columns:
            raise ValueError("merged_pwä¸­æ²¡æœ‰æ ‡å‡†çš„x, yåˆ—ï¼Œè¯·æ£€æŸ¥åˆå¹¶é€»è¾‘ï¼")
        # äº§é‡ä¸åœŸå£¤ï¼ˆæŒ‰xyï¼‰ç©ºé—´è¿‘é‚»åˆå¹¶
        # ä¸åœŸå£¤è¡¨åšç©ºé—´æœ€è¿‘é‚»åˆå¹¶ï¼Œæ”¾å®½å®¹å·®ä»¥é¿å…ç©ºé›†
        merged_pws = spatial_merge(merged_pw, soil_train, on=['x', 'y'], tolerance=50000)
        print(f"After product-weather-soil merge: {merged_pws.shape}")
        data_train = merged_pws
        data_train.columns = data_train.columns.str.strip().str.lower()
        print(f"Merged training data shape: {data_train.shape}")

        # 4) åˆå¹¶æ°”è±¡å’Œäº§é‡æ•°æ®ï¼ˆx, y, YYYYä¸ºä¸»é”®ï¼‰
        # data_train = pd.merge(weather_train, product_train, on=['x', 'y', 'YYYY'], how='inner')
        # 5) å†ä¸åœŸå£¤æ•°æ®åˆå¹¶ï¼ˆx, yä¸ºä¸»é”®ï¼‰
        # data_train = pd.merge(data_train, soil_train, on=['x', 'y'], how='inner')

        # 6) æå–ç‰¹å¾åˆ—
        print("\nExtracting features...")
        exclude_columns = ['x', 'y', 'suit', 'per_qu', 'per_mu', 'yyyy']  # å…¨éƒ¨å°å†™
        feature_columns = [col for col in data_train.columns if col not in exclude_columns]
        print(f"Selected feature columns ({len(feature_columns)}):")
        print(feature_columns)
        X_train = data_train[feature_columns].copy()

        # 7) ç›®æ ‡å˜é‡ - æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŸå§‹data_trainï¼Œåç»­ä¼šåœ¨æ•°æ®æ‹†åˆ†åé‡æ–°æå–
        y_cls_train = data_train['suit'].copy()
        y_reg_train = data_train[['per_qu', 'per_mu']].copy()

        # === æ£€æŸ¥ç›®æ ‡å˜é‡æ•°æ®è´¨é‡ ===
        print(f"ç›®æ ‡å˜é‡åŸå§‹ç»Ÿè®¡ä¿¡æ¯:")
        print(
            f"per_mu: min={y_reg_train['per_mu'].min():.4f}, max={y_reg_train['per_mu'].max():.4f}, mean={y_reg_train['per_mu'].mean():.4f}")
        print(
            f"per_qu: min={y_reg_train['per_qu'].min():.4f}, max={y_reg_train['per_qu'].max():.4f}, mean={y_reg_train['per_qu'].mean():.4f}")
        print(f"suit: unique values={y_reg_train['suit'].unique() if hasattr(y_reg_train, 'suit') else 'N/A'}")

        # === å¼‚å¸¸å€¼è£å‰ªï¼ˆåœ¨log1på˜æ¢ä¹‹å‰ï¼‰- per_mué¢„æµ‹ä¼˜åŒ– ===
        for col in ['per_mu', 'per_qu']:
            q1 = y_reg_train[col].quantile(0.005)  # ä»0.01é™ä½åˆ°0.005ï¼Œä¿ç•™æ›´å¤šæ•°æ®
            q99 = y_reg_train[col].quantile(0.995)  # ä»0.99æé«˜åˆ°0.995ï¼Œä¿ç•™æ›´å¤šæ•°æ®
            print(f"{col} å¼‚å¸¸å€¼è£å‰ª: [{q1:.4f}, {q99:.4f}]")
            mask = (y_reg_train[col] >= q1) & (y_reg_train[col] <= q99)
            y_reg_train = y_reg_train[mask]
            X_train = X_train[mask]
            y_cls_train = y_cls_train[mask]

        # ============= ç®€åŒ–å¤„ç†ï¼šä¸åŠ è½½æµ‹è¯•é›†ï¼Œåªä½¿ç”¨è®­ç»ƒé›† =============
        print("\nç®€åŒ–å¤„ç†ï¼šä¸åŠ è½½æµ‹è¯•é›†ï¼Œåªä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œè®­ç»ƒ...")
        print("åç»­å¯ä»¥ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å»é¢„æµ‹æµ‹è¯•é›†æ•°æ®")

        # ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†
        print("ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†...")
        from sklearn.model_selection import train_test_split

        # ä½¿ç”¨20%çš„æ•°æ®ä½œä¸ºéªŒè¯é›†
        test_size = 0.2
        data_train_final, data_val = train_test_split(
            data_train,
            test_size=test_size,
            random_state=Config.random_seed,
            stratify=data_train['suit'] if 'suit' in data_train.columns else None
        )

        print(f"è®­ç»ƒé›†å¤§å°: {data_train_final.shape}")
        print(f"éªŒè¯é›†å¤§å°: {data_val.shape}")

        # åˆ›å»ºéªŒè¯é›†ç‰¹å¾çŸ©é˜µ
        X_val = pd.DataFrame()

        # ä¸ºæ¯ä¸ªè®­ç»ƒé›†ç‰¹å¾åˆ—åœ¨éªŒè¯é›†ä¸­æ‰¾åˆ°å¯¹åº”åˆ—
        for feature in feature_columns:
            if feature in data_val.columns:
                X_val[feature] = data_val[feature]
            elif feature.startswith('right_'):
                # å¤„ç†right_å‰ç¼€çš„ç‰¹å¾
                base_name = feature[6:]  # å»æ‰'right_'å‰ç¼€
                if base_name in data_val.columns:
                    X_val[feature] = data_val[base_name]
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”åˆ—ï¼Œç”¨0å¡«å……
                    X_val[feature] = 0
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”åˆ—ï¼Œç”¨0å¡«å……
                X_val[feature] = 0

        print(f"éªŒè¯é›†ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_val.shape}")

        # ç¡®ä¿éªŒè¯é›†å’Œè®­ç»ƒé›†ç‰¹å¾åˆ—å®Œå…¨ä¸€è‡´
        missing_cols = set(feature_columns) - set(X_val.columns)
        if missing_cols:
            print(f"ä¸ºéªŒè¯é›†æ·»åŠ ç¼ºå¤±ç‰¹å¾: {len(missing_cols)} ä¸ª")
            for col in missing_cols:
                X_val[col] = 0

        # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
        X_val = X_val[feature_columns]

        print(f"æœ€ç»ˆéªŒè¯é›†ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_val.shape}")
        print(f"è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train.shape}")

        # éªŒè¯éªŒè¯é›†ä¸ä¸ºç©º
        if X_val.shape[0] == 0:
            raise ValueError("éªŒè¯é›†ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­è®­ç»ƒï¼")

        # éªŒè¯ç‰¹å¾åˆ—ä¸€è‡´
        if list(X_train.columns) != list(X_val.columns):
            raise ValueError(
                f"è®­ç»ƒé›†å’ŒéªŒè¯é›†ç‰¹å¾åˆ—ä¸ä¸€è‡´ï¼\nè®­ç»ƒé›†: {list(X_train.columns)}\néªŒè¯é›†: {list(X_val.columns)}")

        # ============= ç‰¹å¾å·¥ç¨‹ =============
        print("\nPreparing features...")

        # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡ - ä»æ‹†åˆ†åçš„æ•°æ®ä¸­æå–
        X_train = data_train_final[feature_columns].copy()

        # é‡æ–°æå–ç›®æ ‡å˜é‡ï¼Œç¡®ä¿ä¸ç‰¹å¾çŸ©é˜µçš„è¡Œæ•°ä¸€è‡´
        y_cls_train = data_train_final['suit'].copy()
        y_reg_train = data_train_final[['per_qu', 'per_mu']].copy()

        # === æ£€æŸ¥ç›®æ ‡å˜é‡æ•°æ®è´¨é‡ï¼ˆæ‹†åˆ†åï¼‰ ===
        print(f"æ‹†åˆ†åç›®æ ‡å˜é‡ç»Ÿè®¡ä¿¡æ¯:")
        print(
            f"per_mu: min={y_reg_train['per_mu'].min():.4f}, max={y_reg_train['per_mu'].max():.4f}, mean={y_reg_train['per_mu'].mean():.4f}")
        print(
            f"per_qu: min={y_reg_train['per_qu'].min():.4f}, max={y_reg_train['per_qu'].max():.4f}, mean={y_reg_train['per_qu'].mean():.4f}")
        print(f"suit: unique values={y_cls_train.unique()}")

        # å¯¹ç›®æ ‡å˜é‡è¿›è¡Œlog1på˜æ¢ï¼ˆåªè¿›è¡Œä¸€æ¬¡ï¼‰
        print("å¯¹ç›®æ ‡å˜é‡è¿›è¡Œlog1på˜æ¢...")
        y_reg_train = np.log1p(y_reg_train)

        # æ£€æŸ¥log1på˜æ¢åçš„æ•°æ®è´¨é‡
        print(f"log1på˜æ¢åç›®æ ‡å˜é‡ç»Ÿè®¡ä¿¡æ¯:")
        print(
            f"per_mu: min={y_reg_train['per_mu'].min():.4f}, max={y_reg_train['per_mu'].max():.4f}, mean={y_reg_train['per_mu'].mean():.4f}")
        print(
            f"per_qu: min={y_reg_train['per_qu'].min():.4f}, max={y_reg_train['per_qu'].max():.4f}, mean={y_reg_train['per_qu'].mean():.4f}")

        # å¯¹log1på˜æ¢åçš„æ•°æ®è¿›è¡Œè½»å¾®è£å‰ªï¼Œé¿å…æå€¼ - per_mué¢„æµ‹ä¼˜åŒ–
        for col in y_reg_train.columns:
            q_low = y_reg_train[col].quantile(0.0005)  # è¿›ä¸€æ­¥é™ä½ä¸‹ç•Œ
            q_high = y_reg_train[col].quantile(0.9995)  # è¿›ä¸€æ­¥æé«˜ä¸Šç•Œ
            print(f"{col} log1påè£å‰ª: [{q_low:.4f}, {q_high:.4f}]")
            y_reg_train[col] = np.clip(y_reg_train[col], q_low, q_high)

        # å¤„ç†æ‰€æœ‰objectç±»å‹ç‰¹å¾ï¼šåŒºé—´å­—ç¬¦ä¸²è½¬å‡å€¼ï¼Œæ™®é€šå­—ç¬¦ä¸²ç”¨LabelEncoderç¼–ç 
        import re
        from sklearn.preprocessing import LabelEncoder
        print("\nå¤„ç†æ‰€æœ‰objectç±»å‹ç‰¹å¾ï¼šåŒºé—´å­—ç¬¦ä¸²è½¬å‡å€¼ï¼Œæ™®é€šå­—ç¬¦ä¸²ç”¨LabelEncoderç¼–ç ")
        for col in feature_columns:
            if X_train[col].dtype == object or X_val[col].dtype == object:
                def is_range(s):
                    s = str(s).strip().replace('ï¼', '-').replace('â€”', '-').replace('â€“', '-')
                    return bool(re.match(r'^-?\d+(\.\d+)?\s*-\s*-?\d+(\.\d+)?$', s))

                sample = pd.concat([X_train[col], X_val[col]]).astype(str)
                if sample.apply(is_range).any():
                    print(f"å°†åŒºé—´å­—ç¬¦ä¸²åˆ— {col} è½¬ä¸ºå‡å€¼")

                    def range_to_mean(x):
                        try:
                            s = str(x).strip().replace('ï¼', '-').replace('â€”', '-').replace('â€“', '-')
                            if is_range(s):
                                low, high = map(float, re.split(r'\s*-\s*', s))
                                return (low + high) / 2
                            return float(s)
                        except Exception as e:
                            print(f"[è½¬æ¢å¼‚å¸¸] åˆ—: {col}, åŸå€¼: {x}, é”™è¯¯: {e}")
                            return np.nan

                    X_train[col] = X_train[col].apply(range_to_mean)
                    X_val[col] = X_val[col].apply(range_to_mean)
                    mean_value = X_train[col].mean()
                    X_train[col].fillna(mean_value, inplace=True)
                    X_val[col].fillna(mean_value, inplace=True)
                    X_train[col] = X_train[col].astype(float)
                    X_val[col] = X_val[col].astype(float)
                else:
                    print(f"Label encoding column: {col}")
                    le = LabelEncoder()
                    all_values = pd.concat([X_train[col].astype(str), X_val[col].astype(str)], axis=0)
                    le.fit(all_values)
                    X_train[col] = le.transform(X_train[col].astype(str))
                    X_val[col] = le.transform(X_val[col].astype(str))
                    X_train[col] = X_train[col].astype(float)
                    X_val[col] = X_val[col].astype(float)
        print("\nX_train dtypes before standardization:")
        print(X_train.dtypes)
        print("\næ‰€æœ‰objectåˆ—å”¯ä¸€å€¼æ£€æŸ¥ï¼š")
        for col in X_train.columns:
            if X_train[col].dtype == object:
                print(f"{col}: {X_train[col].unique()[:20]}")
                import sys
                sys.exit(1)

        # æ³¨é‡Šæ‰åŸæœ‰çš„èŒƒå›´å€¼ç‰¹å¾å¤„ç†å’Œå…¶å®ƒX_train/X_testèµ‹å€¼
        '''
        # å¤„ç†èŒƒå›´å€¼ç‰¹å¾
        print("\nProcessing range value features...")
        def process_range(x):
            try:
                if isinstance(x, str) and '-' in x:
                    low, high = map(float, x.split('-'))
                    return (low + high) / 2
                return float(x)
            except:
                return np.nan

        for range_col in Config.range_columns:
            if range_col in X_train.columns:
                print(f"Processing {range_col}...")
                X_train[range_col] = X_train[range_col].apply(process_range)
                # X_test[range_col] = X_test[range_col].apply(process_range)  # æ³¨é‡Šæ‰ï¼Œä¸å†ä½¿ç”¨æµ‹è¯•é›†
                mean_value = X_train[range_col].mean()
                X_train[range_col].fillna(mean_value, inplace=True)
                # X_test[range_col].fillna(mean_value, inplace=True)  # æ³¨é‡Šæ‰ï¼Œä¸å†ä½¿ç”¨æµ‹è¯•é›†
        '''

        # å¤„ç†åˆ†ç±»ç‰¹å¾
        '''
        print("\nEncoding categorical features...")
        encoders = {}
        for cat_col in Config.categorical_columns:
            if cat_col in X_train.columns:
                print(f"Encoding {cat_col}...")
                le = LabelEncoder()
                X_train[cat_col] = le.fit_transform(X_train[cat_col].astype(str))
                # X_test[cat_col] = le.transform(X_test[cat_col].astype(str))  # æ³¨é‡Šæ‰ï¼Œä¸å†ä½¿ç”¨æµ‹è¯•é›†
                encoders[cat_col] = le
            # ä¿å­˜ç¼–ç å™¨
                encoder_file = os.path.join(Config.label_encoder_dir, f"{cat_col}_encoder.pkl")
            try:
                    joblib.dump(le, encoder_file)
                    print(f"Saved encoder for {cat_col} to {encoder_file}")
            except Exception as e:
                    print(f"Warning: Could not save encoder for {cat_col}: {str(e)}")
        '''

        # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾ç±»å‹
        print("\nX_train dtypes before standardization:")
        print(X_train.dtypes)
        print("\næ‰€æœ‰objectåˆ—å”¯ä¸€å€¼æ£€æŸ¥ï¼š")
        for col in X_train.columns:
            if X_train[col].dtype == object:
                print(f"{col}: {X_train[col].unique()[:20]}")
                import sys
                sys.exit(1)

        # ç‰¹å¾æ ‡å‡†åŒ–
        print("\nStandardizing features...")
        if len(X_train) == 0:
            raise ValueError("è®­ç»ƒé›†ä¸ºç©ºï¼šåˆå¹¶å¤±è´¥æˆ–å®¹å·®è¿‡å°ã€‚è¯·æ£€æŸ¥åæ ‡/å¹´ä»½ä¸€è‡´æ€§æˆ–å¢å¤§å®¹å·®ï¼")
        import tempfile
        temp_train_path = os.path.join(tempfile.gettempdir(), 'xtrain_temp.csv')
        temp_val_path = os.path.join(tempfile.gettempdir(), 'xval_temp.csv')
        X_train.to_csv(temp_train_path, index=False)
        X_val.to_csv(temp_val_path, index=False)

        chunksize = 100_000
        scaler = StandardScaler()
        # åˆ†å—partial_fit X_trainï¼ˆè‹¥ç¬¬ä¸€å—å³ä¸ºç©ºï¼Œç›´æ¥æŠ¥é”™ï¼Œé¿å…åç»­æ ‡å‡†åŒ–äºŒæ¬¡æŠ¥é”™ï¼‰
        any_chunk = False
        for chunk in pd.read_csv(temp_train_path, chunksize=chunksize):
            if len(chunk) == 0:
                continue
            any_chunk = True
            scaler.partial_fit(chunk)
        if not any_chunk:
            raise ValueError("è®­ç»ƒé›†åˆ†å—è¯»å–ä¸ºç©ºï¼šè¯·æ£€æŸ¥åˆå¹¶ç»“æœæ˜¯å¦ä¸º0è¡Œï¼")
        # åˆ†å—transform X_train
        scaled_chunks = []
        for chunk in pd.read_csv(temp_train_path, chunksize=chunksize):
            if len(chunk) == 0:
                continue
            chunk_scaled = scaler.transform(chunk)
            chunk_scaled = pd.DataFrame(chunk_scaled, columns=feature_columns)
            scaled_chunks.append(chunk_scaled)
        X_train_scaled = pd.concat(scaled_chunks, ignore_index=True)
        # åˆ†å—transform X_val
        scaled_chunks = []
        for chunk in pd.read_csv(temp_val_path, chunksize=chunksize):
            if len(chunk) == 0:
                continue
            chunk_scaled = scaler.transform(chunk)
            chunk_scaled = pd.DataFrame(chunk_scaled, columns=feature_columns)
            scaled_chunks.append(chunk_scaled)

        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æ•°æ®éœ€è¦æ ‡å‡†åŒ–
        if scaled_chunks:
            X_val_scaled = pd.concat(scaled_chunks, ignore_index=True)
        else:
            print("è­¦å‘Šï¼šéªŒè¯é›†ä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„éªŒè¯é›†ç‰¹å¾çŸ©é˜µ")
            X_val_scaled = pd.DataFrame(columns=feature_columns)

        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        print("Saving scaler...")
        try:
            os.makedirs(os.path.dirname(Config.scaler_file), exist_ok=True)
            joblib.dump(scaler, Config.scaler_file)
            print(f"Scaler successfully saved to: {Config.scaler_file}")
        except Exception as e:
            print(f"Warning: Could not save scaler: {str(e)}")

        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        print("Saving label encoder...")
        try:
            le = LabelEncoder()
            y_cls_train_encoded = le.fit_transform(y_cls_train)
            os.makedirs(os.path.dirname(Config.label_encoder_file), exist_ok=True)
            joblib.dump(le, Config.label_encoder_file)
            print(f"Label encoder successfully saved to: {Config.label_encoder_file}")
        except Exception as e:
            print(f"Warning: Could not save label encoder: {str(e)}")
            y_cls_train_encoded = y_cls_train

        print("Data loading and preprocessing completed successfully.")
        # === å¼ºåˆ¶æ³¨é‡Šæ‰objectç‰¹å¾å¤„ç†åæ‰€æœ‰å¯¹X_trainå’ŒX_testçš„é‡æ–°èµ‹å€¼ ===
        # X_train = data_train[feature_columns].copy()
        # X_test = data_test[feature_columns].copy()
        # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡
        # X_train = data_train[feature_columns].copy()
        # X_test = data_test[feature_columns].copy()
        # å¼ºåˆ¶æ‰€æœ‰ç‰¹å¾åˆ—éƒ½è½¬ä¸ºfloatï¼Œå‡ºé”™æ—¶æ‰“å°å¼‚å¸¸å’ŒåŸå€¼
        import sys
        for col in X_train_scaled.columns:
            try:
                X_train_scaled[col] = X_train_scaled[col].astype(float)
            except Exception as e:
                print(f"[æœ€ç»ˆfloatè½¬æ¢å¼‚å¸¸] åˆ—: {col}, é”™è¯¯: {e}")
                print("æ ·æœ¬å€¼:", X_train_scaled[col].unique()[:10])
                sys.exit(1)

        # å¯¹éªŒè¯é›†ä¹Ÿè¿›è¡Œç±»å‹è½¬æ¢
        for col in X_val_scaled.columns:
            try:
                X_val_scaled[col] = X_val_scaled[col].astype(float)
            except Exception as e:
                print(f"[éªŒè¯é›†floatè½¬æ¢å¼‚å¸¸] åˆ—: {col}, é”™è¯¯: {e}")
                X_val_scaled[col] = 0.0  # ç”¨0å¡«å……å¼‚å¸¸å€¼

        # è‡ªåŠ¨æŸ¥æ‰¾åæ ‡å’Œå¹´ä»½åˆ—ï¼Œé¿å…KeyError
        coord_cols = []
        for cand in ['x_product', 'x', 'right_x']:
            if cand in data_val.columns:
                coord_cols.append(cand)
        for cand in ['y_product', 'y', 'right_y']:
            if cand in data_val.columns:
                coord_cols.append(cand)
        for cand in ['yyyy', 'right_yyyy', 'year']:
            if cand in data_val.columns:
                coord_cols.append(cand)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åæ ‡åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not coord_cols:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°åæ ‡åˆ—ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            coord_cols = ['x', 'y', 'yyyy']
            # ä¸ºdata_valæ·»åŠ é»˜è®¤åæ ‡åˆ—
            data_val['x'] = range(len(data_val))
            data_val['y'] = range(len(data_val))
            data_val['yyyy'] = 2015  # é»˜è®¤å¹´ä»½

        # ====== å¿«é€ŸéªŒè¯é‡‡æ · ======
        # X_train = X_train.iloc[:5000]
        # y_cls_train = y_cls_train.iloc[:5000]
        # y_reg_train = y_reg_train.iloc[:5000]
        # X_test = X_test.iloc[:5000]
        # åªä¿ç•™å‰10ä¸ªç‰¹å¾
        # X_train = X_train.iloc[:, :10]
        # X_test = X_test.iloc[:, :10]
        # åŒæ­¥é‡‡æ ·æ ‡å‡†åŒ–å’Œç¼–ç åçš„å˜é‡
        # X_train_scaled = X_train_scaled.iloc[:5000]
        # y_cls_train_encoded = y_cls_train_encoded[:5000]
        # X_test_scaled = X_test_scaled.iloc[:5000]
        return X_train_scaled, y_cls_train_encoded, y_reg_train, X_val_scaled, data_val[coord_cols].values, data_val

    except Exception as e:
        print(f"Error in load_data: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        tb = traceback.extract_tb(e.__traceback__)
        if tb:
            print(f"Error location: {tb[-1].filename}:{tb[-1].lineno}")
        raise


##############################################
# 2. XGBoost ç‰¹å¾æå–
##############################################


def safe_reset_index(obj):
    if isinstance(obj, (pd.DataFrame, pd.Series)):
        return obj.reset_index(drop=True)
    else:
        return pd.Series(obj).reset_index(drop=True)


def extract_xgboost_features(X_train, X_val, y_cls_train, y_cls_val, y_reg_train, y_reg_val):
    """
    ä½¿ç”¨XGBoostè¿›è¡Œç‰¹å¾æå–ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ã€‚
    è¿”å›å…¨éƒ¨ç‰¹å¾å’Œæ ‡ç­¾ã€‚
    """
    print("\nExtracting XGBoost features...")
    try:
        # ä¸è¿›è¡Œé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
        print(f"ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®: {len(X_train):,} ä¸ªæ ·æœ¬")
        print(f"ä½¿ç”¨å…¨éƒ¨éªŒè¯æ•°æ®: {len(X_val):,} ä¸ªæ ·æœ¬")

        X_train_sample = safe_reset_index(X_train)
        X_val_sample = safe_reset_index(X_val)
        y_cls_train_sample = safe_reset_index(y_cls_train)
        y_cls_val_sample = safe_reset_index(y_cls_val)
        y_reg_train_sample = safe_reset_index(y_reg_train)
        y_reg_val_sample = safe_reset_index(y_reg_val)
        dtrain = xgb.DMatrix(X_train_sample, label=y_cls_train_sample)
        dval = xgb.DMatrix(X_val_sample, label=y_cls_val_sample)

        xgb_params = Config.xgb_params.copy()
        unique_classes = np.unique(y_cls_train_sample)
        num_classes = len(unique_classes)

        # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨å›å½’è€Œä¸æ˜¯åˆ†ç±»
        if num_classes == 1:
            print(f"Only one class found: {unique_classes[0]}, switching to regression mode")
            xgb_params['objective'] = 'reg:squarederror'
            if 'num_class' in xgb_params:
                del xgb_params['num_class']
        else:
            xgb_params['num_class'] = num_classes
            # ç¡®ä¿æ ‡ç­¾ä»0å¼€å§‹
            y_cls_train_sample = y_cls_train_sample - min(unique_classes)
            y_cls_val_sample = y_cls_val_sample - min(unique_classes)
            dtrain = xgb.DMatrix(X_train_sample, label=y_cls_train_sample)
            dval = xgb.DMatrix(X_val_sample, label=y_cls_val_sample)

        print(f"Number of classes: {num_classes}")

        print("Training XGBoost model...")
        with tqdm(total=Config.xgb_params['n_estimators'], desc="Training XGBoost") as pbar:
            class TqdmCallback(xgb.callback.TrainingCallback):
                def after_iteration(self, model, epoch, evals_log):
                    pbar.update(1)
                    return False

            xgb_model = xgb.train(
                xgb_params, dtrain,
                num_boost_round=Config.xgb_params['n_estimators'],
                evals=[(dval, 'val')],
                callbacks=[TqdmCallback()],
                verbose_eval=False
            )

        # ä¿å­˜XGBoostæ¨¡å‹
        print("Saving XGBoost model...")
        try:
            os.makedirs(os.path.dirname(Config.xgboost_model_file), exist_ok=True)
            xgb_model.save_model(Config.xgboost_model_file)
            print(f"XGBoost model successfully saved to: {Config.xgboost_model_file}")
        except Exception as e:
            print(f"Error saving XGBoost model: {str(e)}")
            raise

        print("Extracting leaf features (chunked)...")

        def get_leaf_features_chunked(xgb_model, X, chunk_size=100_000):
            n = len(X)
            leaf_feats = []
            for start in range(0, n, chunk_size):
                end = min(start + chunk_size, n)
                if isinstance(X, (pd.DataFrame, pd.Series)):
                    X_chunk = X.iloc[start:end]
                else:
                    X_chunk = X[start:end]
                try:
                    dmat = xgb.DMatrix(X_chunk)
                    leaf = xgb_model.predict(dmat, pred_leaf=True)
                    leaf_feat = np.mean(leaf, axis=1).reshape(-1, 1)
                except Exception as e:
                    print(f"è­¦å‘Š: åˆ†å—æå–å¶å­ç‰¹å¾å¤±è´¥: {str(e)}")
                    # å¦‚æœåˆ†å—å¤±è´¥ï¼Œå°è¯•æ›´å°çš„å—
                    smaller_chunk_size = chunk_size // 2
                    if smaller_chunk_size > 0:
                        return get_leaf_features_chunked(xgb_model, X, smaller_chunk_size)
                    else:
                        raise e
                leaf_feats.append(leaf_feat)
            return np.vstack(leaf_feats)

        leaf_feat_train = get_leaf_features_chunked(xgb_model, X_train_sample)
        leaf_feat_val = get_leaf_features_chunked(xgb_model, X_val_sample)

        # åˆ†å—æ‹¼æ¥ç‰¹å¾ï¼Œé˜²æ­¢np.hstackçˆ†å†…å­˜
        def hstack_chunked(X, leaf_feat, chunk_size=100_000):
            n = len(X)
            out_chunks = []
            for start in range(0, n, chunk_size):
                end = min(start + chunk_size, n)
                if isinstance(X, (pd.DataFrame, pd.Series)):
                    X_chunk = X.iloc[start:end].values
                else:
                    X_chunk = X[start:end]
                leaf_chunk = leaf_feat[start:end]
                out_chunks.append(np.hstack([X_chunk, leaf_chunk]))
            return np.vstack(out_chunks)

        X_train_combined = hstack_chunked(X_train_sample, leaf_feat_train)
        X_val_combined = hstack_chunked(X_val_sample, leaf_feat_val)
        print("XGBoost feature extraction completed successfully.")
        # åªè¿”å›é‡‡æ ·åçš„ç‰¹å¾å’Œæ ‡ç­¾
        return X_train_combined, X_val_combined, xgb_model, y_cls_train_sample, y_cls_val_sample, y_reg_train_sample, y_reg_val_sample
    except Exception as e:
        print(f"Error in extract_xgboost_features: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        tb = traceback.extract_tb(e.__traceback__)
        if tb:
            print(f"Error location: {tb[-1].filename}:{tb[-1].lineno}")
        raise


##############################################
# 3. æ„å»ºå¤šè¾“å‡ºæ··åˆæ¨¡å‹ï¼ˆå«æ³¨æ„åŠ›æœºåˆ¶ï¼‰
##############################################

def get_custom_objects():
    """è·å–è‡ªå®šä¹‰å±‚æ˜ å°„ï¼Œç”¨äºæ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
    return {
        'MultiHeadAttention': MultiHeadAttention
    }


def save_model_with_custom_objects(model, filepath):
    """ä¿å­˜åŒ…å«è‡ªå®šä¹‰å±‚çš„æ¨¡å‹"""
    try:
        model.save(filepath)
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {filepath}")
    except Exception as e:
        print(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
        raise


def load_model_with_custom_objects(filepath):
    """åŠ è½½åŒ…å«è‡ªå®šä¹‰å±‚çš„æ¨¡å‹"""
    try:
        custom_objects = get_custom_objects()
        model = tf.keras.models.load_model(filepath, custom_objects=custom_objects)
        print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {filepath}")
        return model
    except Exception as e:
        print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise


class MultiHeadAttention(Layer):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, num_heads=4, head_dim=32, dropout=0.1, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dropout = dropout
        self.output_dim = num_heads * head_dim

    def build(self, input_shape):
        self.query_dense = Dense(self.output_dim)
        self.key_dense = Dense(self.output_dim)
        self.value_dense = Dense(self.output_dim)
        self.combine_heads = Dense(input_shape[-1])
        self.attention_dropout = Dropout(self.dropout)
        super(MultiHeadAttention, self).build(input_shape)

    def get_config(self):
        """è·å–é…ç½®ä¿¡æ¯ï¼Œç”¨äºæ¨¡å‹åºåˆ—åŒ–"""
        config = super(MultiHeadAttention, self).get_config()
        config.update({
            'num_heads': self.num_heads,
            'head_dim': self.head_dim,
            'dropout': self.dropout
        })
        return config

    @classmethod
    def from_config(cls, config):
        """ä»é…ç½®åˆ›å»ºå±‚å®ä¾‹ï¼Œç”¨äºæ¨¡å‹ååºåˆ—åŒ–"""
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„é¢å¤–å‚æ•°
        filtered_config = {k: v for k, v in config.items()
                           if k in ['num_heads', 'head_dim', 'dropout', 'name', 'trainable']}
        return cls(**filtered_config)

    def call(self, inputs, training=None, mask=None):
        # è°ƒæ•´è¾“å…¥å½¢çŠ¶
        batch_size = tf.shape(inputs)[0]
        seq_len = 1  # å› ä¸ºæˆ‘ä»¬çš„è¾“å…¥æ˜¯å•ä¸ªå‘é‡

        # å°†è¾“å…¥é‡å¡‘ä¸ºåºåˆ—
        x = tf.expand_dims(inputs, axis=1)  # [batch_size, 1, feature_dim]

        # ç”ŸæˆæŸ¥è¯¢ã€é”®ã€å€¼
        query = self.query_dense(x)  # [batch_size, 1, output_dim]
        key = self.key_dense(x)  # [batch_size, 1, output_dim]
        value = self.value_dense(x)  # [batch_size, 1, output_dim]

        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        def reshape_to_heads(x):
            return tf.reshape(x, (batch_size, seq_len, self.num_heads, self.head_dim))

        query = reshape_to_heads(query)
        key = reshape_to_heads(key)
        value = reshape_to_heads(value)

        # è½¬ç½®ä»¥è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
        query = tf.transpose(query, perm=[0, 2, 1, 3])  # [batch_size, num_heads, 1, head_dim]
        key = tf.transpose(key, perm=[0, 2, 1, 3])  # [batch_size, num_heads, 1, head_dim]
        value = tf.transpose(value, perm=[0, 2, 1, 3])  # [batch_size, num_heads, 1, head_dim]

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        matmul_qk = tf.matmul(query, key, transpose_b=True)  # [batch_size, num_heads, 1, 1]
        dk = tf.cast(self.head_dim, tf.float32)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

        # åº”ç”¨softmax
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        attention_weights = self.attention_dropout(attention_weights)

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        output = tf.matmul(attention_weights, value)  # [batch_size, num_heads, 1, head_dim]

        # è½¬ç½®å›åŸå§‹å½¢çŠ¶
        output = tf.transpose(output, perm=[0, 2, 1, 3])  # [batch_size, 1, num_heads, head_dim]
        output = tf.reshape(output, (batch_size, seq_len, self.output_dim))  # [batch_size, 1, output_dim]

        # åˆå¹¶å¤šå¤´è¾“å‡º
        output = self.combine_heads(output)  # [batch_size, 1, input_dim]

        # å»é™¤åºåˆ—ç»´åº¦
        output = tf.squeeze(output, axis=1)  # [batch_size, input_dim]

        return output


def build_hybrid_model(input_dim, num_classes, params=None, strategy=None):
    """æ„å»ºæ”¹è¿›çš„æ··åˆç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ"""
    if params is None:
        params = {
            'lr': Config.dl_params['learning_rate'],
            'neurons1': 256,  # å¢åŠ ç¥ç»å…ƒæ•°é‡ï¼Œæé«˜æ¨¡å‹å®¹é‡
            'neurons2': 128,  # å¢åŠ ç¥ç»å…ƒæ•°é‡
            'dropout_rate': Config.dl_params['dropout_rate'],  # ä½¿ç”¨é…ç½®çš„dropout
            'batch_size': Config.dl_params['batch_size'],  # ä½¿ç”¨é…ç½®çš„batch size
            'attention_units': 64,  # å¢åŠ æ³¨æ„åŠ›å•å…ƒ
            'l2_lambda': Config.dl_params.get('l2_reg', 1e-4),  # ä½¿ç”¨é…ç½®çš„L2æ­£åˆ™åŒ–
            'l1_lambda': Config.dl_params.get('l1_reg', 1e-5),  # æ·»åŠ L1æ­£åˆ™åŒ–
            'optimizer_type': 'adam',
            'activation': 'relu'  # ä½¿ç”¨reluæ¿€æ´»å‡½æ•°
        }

    def residual_block(x, units, dropout_rate, l2_lambda, l1_lambda, activation):
        """æ”¹è¿›çš„æ®‹å·®å—ï¼Œå¢å¼ºæ­£åˆ™åŒ–"""
        shortcut = Dense(units, activation=None,
                         kernel_regularizer=tf.keras.regularizers.l1_l2(l1_lambda, l2_lambda),
                         kernel_initializer='glorot_uniform')(x)
        x = Dense(units, activation=activation,
                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1_lambda, l2_lambda),
                  kernel_initializer='glorot_uniform')(x)
        x = BatchNormalization()(x)
        x = Dropout(dropout_rate)(x)
        x = Dense(units, activation=None,
                  kernel_regularizer=tf.keras.regularizers.l1_l2(l1_lambda, l2_lambda),
                  kernel_initializer='glorot_uniform')(x)
        x = BatchNormalization()(x)
        x = Add()([shortcut, x])
        x = tf.keras.activations.get(activation)(x)
        return x

    def create_model():
        # è¾“å…¥å±‚
        inputs = Input(shape=(input_dim,))
        # å…ˆæŠ•å½±åˆ°neurons1ç»´ï¼Œå¢å¼ºæ­£åˆ™åŒ–ï¼Œä½¿ç”¨Xavieråˆå§‹åŒ–
        x = Dense(params['neurons1'], activation=params['activation'],
                  kernel_regularizer=tf.keras.regularizers.l1_l2(params['l1_lambda'], params['l2_lambda']),
                  kernel_initializer='glorot_uniform')(inputs)
        x = BatchNormalization()(x)
        x = Dropout(params['dropout_rate'] * 0.7)(x)  # æé«˜åˆå§‹dropout

        # å¢åŠ æ®‹å·®å—æ•°é‡ï¼Œæé«˜æ¨¡å‹å­¦ä¹ èƒ½åŠ›
        for _ in range(3):  # å¢åŠ åˆ°3ä¸ªæ®‹å·®å—
            x = residual_block(x, params['neurons1'], params['dropout_rate'],
                               params['l2_lambda'], params['l1_lambda'], params['activation'])

        # ç‰¹å¾é™ç»´
        x = residual_block(x, params['neurons2'], params['dropout_rate'],
                           params['l2_lambda'], params['l1_lambda'], params['activation'])

        # ç®€åŒ–çš„ç‰¹å¾å¤„ç†å±‚ï¼Œå‡å°‘å¤æ‚åº¦
        x = Dense(params['neurons2'], activation=params['activation'],
                  kernel_regularizer=tf.keras.regularizers.l1_l2(params['l1_lambda'], params['l2_lambda']))(x)
        x = BatchNormalization()(x)
        x = Dropout(params['dropout_rate'] * 0.8)(x)  # æé«˜dropout

        # ä¼˜åŒ–çš„æ³¨æ„åŠ›å±‚ï¼Œå¹³è¡¡æ€§èƒ½å’Œå¤æ‚åº¦
        attention_output = MultiHeadAttention(
            num_heads=8,  # å¢åŠ æ³¨æ„åŠ›å¤´æ•°
            head_dim=max(8, params['attention_units'] // 8),  # å¢åŠ å¤´ç»´åº¦
            dropout=params['dropout_rate'] * 0.5  # é™ä½æ³¨æ„åŠ›dropout
        )(x)
        # ç‰¹å¾èåˆ
        x = Concatenate(axis=1)([x, attention_output])
        # åˆ†ç±»åˆ†æ”¯ - ç®€åŒ–ç»“æ„
        classification_branch = Dense(params['neurons2'] // 2,
                                      activation=params['activation'],
                                      kernel_regularizer=tf.keras.regularizers.l1_l2(params['l1_lambda'],
                                                                                     params['l2_lambda']))(x)
        classification_branch = BatchNormalization()(classification_branch)
        classification_branch = Dropout(params['dropout_rate'] * 0.9)(classification_branch)  # æé«˜dropout
        classification_output = Dense(num_classes,
                                      activation='softmax',
                                      name='classification')(classification_branch)

        # å›å½’åˆ†æ”¯ - ç®€åŒ–ç»“æ„ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        regression_branch = Dense(params['neurons2'] // 2,  # å‡å°‘ç¥ç»å…ƒ
                                  activation=params['activation'],
                                  kernel_regularizer=tf.keras.regularizers.l1_l2(params['l1_lambda'],
                                                                                 params['l2_lambda']))(x)
        regression_branch = BatchNormalization()(regression_branch)
        regression_branch = Dropout(params['dropout_rate'] * 0.8)(regression_branch)  # æé«˜dropout

        # ç®€åŒ–çš„å›å½’å±‚
        regression_branch = Dense(params['neurons2'] // 4,  # è¿›ä¸€æ­¥å‡å°‘
                                  activation=params['activation'],
                                  kernel_regularizer=tf.keras.regularizers.l1_l2(params['l1_lambda'],
                                                                                 params['l2_lambda']))(
            regression_branch)
        regression_branch = BatchNormalization()(regression_branch)
        regression_branch = Dropout(params['dropout_rate'] * 0.7)(regression_branch)  # æé«˜dropout
        # å›å½’è¾“å‡ºå±‚ï¼šä½¿ç”¨çº¿æ€§æ¿€æ´»ï¼Œæ·»åŠ åˆç†çš„è¾“å‡ºçº¦æŸ
        # ä½¿ç”¨æ›´ä¿å®ˆçš„åç½®åˆå§‹åŒ–ï¼Œé¿å…åˆå§‹é¢„æµ‹å€¼è¿‡å¤§
        regression_output = Dense(2, activation='linear',
                                  kernel_initializer='glorot_uniform',
                                  bias_initializer=tf.keras.initializers.Constant([4.5, 7.0]))(
            regression_branch)  # æ›´ä¿å®ˆçš„åˆå§‹å€¼

        # æ·»åŠ è¾“å‡ºçº¦æŸï¼šé™åˆ¶é¢„æµ‹å€¼åœ¨log1på˜æ¢åçš„åˆç†èŒƒå›´å†…
        # çœŸå®å€¼èŒƒå›´: per_mu [4.67, 5.69], per_qu [7.16, 8.19]
        # æ”¾å®½çº¦æŸèŒƒå›´ï¼Œé¿å…è¿‡åº¦é™åˆ¶æ¨¡å‹å­¦ä¹ 
        regression_output = tf.keras.layers.Lambda(
            lambda x: tf.clip_by_value(x, 3.0, 10.0),
            name='regression'
        )(regression_output)
        # æ„å»ºæ¨¡å‹
        model = Model(inputs=inputs,
                      outputs=[classification_output, regression_output])
        # ç¼–è¯‘æ¨¡å‹ï¼Œä¼˜åŒ–è®­ç»ƒç¨³å®šæ€§
        if params['optimizer_type'] == 'adam':
            # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–æ¢¯åº¦è£å‰ªå’Œæƒé‡è¡°å‡
            optimizer = tf.keras.optimizers.Adam(
                learning_rate=params['lr'],
                clipnorm=1.0,  # æ”¾å®½æ¢¯åº¦è£å‰ªï¼Œæé«˜å­¦ä¹ èƒ½åŠ›
                clipvalue=1.0,  # æ”¾å®½æ¢¯åº¦è£å‰ª
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                decay=1e-6  # é™ä½æƒé‡è¡°å‡
            )
        elif params['optimizer_type'] == 'adamw':
            # TensorFlow 2.10.1ä¸æ”¯æŒAdamWï¼Œä½¿ç”¨Adamæ›¿ä»£
            print("âš ï¸ TensorFlow 2.10.1ä¸æ”¯æŒAdamWï¼Œä½¿ç”¨Adamæ›¿ä»£")
            optimizer = tf.keras.optimizers.Adam(
                learning_rate=params['lr'],
                clipnorm=1.0,
                clipvalue=1.0,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                decay=1e-4  # ä½¿ç”¨decayæ¨¡æ‹Ÿæƒé‡è¡°å‡
            )
        else:
            print(f"[è­¦å‘Š] æœªæ‰¾åˆ°ä¼˜åŒ–å™¨ {params['optimizer_type']}ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºAdam")
            optimizer = tf.keras.optimizers.Adam(
                learning_rate=params['lr'],
                clipnorm=1.0,
                clipvalue=1.0,
                beta_1=0.9,
                beta_2=0.999,
                epsilon=1e-8,
                decay=1e-6
            )
        if Config.use_amp and tf.test.is_built_with_cuda():
            optimizer = mixed_precision.LossScaleOptimizer(optimizer)
        # æŸå¤±å‡½æ•°é€‰æ‹©
        if Config.loss_type == 'huber':
            reg_loss = tf.keras.losses.Huber()
        elif Config.loss_type == 'mse':
            reg_loss = 'mse'
        elif Config.loss_type == 'mae':
            reg_loss = 'mae'
        elif Config.loss_type == 'logcosh':
            reg_loss = tf.keras.losses.LogCosh()
        else:
            reg_loss = tf.keras.losses.Huber()
        model.compile(
            optimizer=optimizer,
            loss={
                'classification': 'sparse_categorical_crossentropy',
                'regression': reg_loss
            },
            metrics={
                'classification': 'accuracy',
                'regression': ['mae', 'mse']
            },
            loss_weights={
                'classification': 0.1,  # æé«˜åˆ†ç±»æŸå¤±æƒé‡ï¼Œå¹³è¡¡ä¸¤ä¸ªä»»åŠ¡
                'regression': 1.0  # ä¿æŒå›å½’æŸå¤±æƒé‡ï¼Œä¸“æ³¨äºper_mué¢„æµ‹
            }
        )
        return model

    # å¦‚æœæä¾›äº†strategyï¼Œåœ¨strategy scopeå†…åˆ›å»ºæ¨¡å‹
    if strategy is not None:
        with strategy.scope():
            return create_model()
    else:
        return create_model()


##############################################
# 4. è¶…å‚æ•°è°ƒä¼˜ï¼ˆOptunaï¼‰
##############################################

class TrainingMonitor:
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§ç±»"""

    def __init__(self, log_dir):
        self.log_dir = log_dir
        self.model = None
        self.current_epoch = 0  # æ·»åŠ å½“å‰epochè®¡æ•°å™¨
        self.metrics_history = {
            'epoch': [],
            'training_loss': [],
            'validation_loss': [],
            'training_cls_acc': [],
            'validation_cls_acc': [],
            'training_reg_mae': [],
            'validation_reg_mae': [],
            'training_reg_r2': [],
            'validation_reg_r2': [],
            'learning_rate': [],
            'gpu_memory_usage': [],
            'batch_time': []
        }

        # è¿‡æ‹Ÿåˆæ£€æµ‹å‚æ•°
        self.overfitting_detected = False
        self.overfitting_epoch = None
        self.gap_threshold = 0.1  # è®­ç»ƒå’ŒéªŒè¯æŸå¤±å·®è·é˜ˆå€¼
        self.consecutive_increases = 0  # è¿ç»­éªŒè¯æŸå¤±å¢åŠ æ¬¡æ•°
        self.max_consecutive_increases = 3  # æœ€å¤§å…è®¸è¿ç»­å¢åŠ æ¬¡æ•°

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(log_dir, exist_ok=True)

        # åˆå§‹åŒ–å¯è§†åŒ–
        try:
            plt.style.use(['seaborn-v0_8-darkgrid'])
        except:
            print("Warning: Could not set seaborn style, using default style")
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle('Training Progress', fontsize=16)

    def set_model(self, model):
        """è®¾ç½®è¦ç›‘æ§çš„æ¨¡å‹"""
        self.model = model

    def update_metrics(self, epoch, logs):
        """æ›´æ–°è®­ç»ƒæŒ‡æ ‡ï¼Œå¢å¼ºè¿‡æ‹Ÿåˆæ£€æµ‹"""
        self.current_epoch = epoch

        # è®°å½•åŸºæœ¬æŒ‡æ ‡
        self.metrics_history['epoch'].append(epoch)
        self.metrics_history['training_loss'].append(logs.get('loss', 0))
        self.metrics_history['validation_loss'].append(logs.get('val_loss', 0))
        self.metrics_history['training_cls_acc'].append(logs.get('classification_accuracy', 0))
        self.metrics_history['validation_cls_acc'].append(logs.get('val_classification_accuracy', 0))
        self.metrics_history['training_reg_mae'].append(logs.get('regression_mae', 0))
        self.metrics_history['validation_reg_mae'].append(logs.get('val_regression_mae', 0))

        # è¿‡æ‹Ÿåˆæ£€æµ‹
        self._detect_overfitting()

        # è®¡ç®—å¹¶è®°å½•RÂ²åˆ†æ•° - ç®€åŒ–å¤„ç†ï¼Œé¿å…å¤æ‚çš„RÂ²è®¡ç®—
        self.metrics_history['training_reg_r2'].append(0)  # æš‚æ—¶è®¾ä¸º0ï¼Œé¿å…è®¡ç®—é”™è¯¯
        self.metrics_history['validation_reg_r2'].append(0)  # æš‚æ—¶è®¾ä¸º0ï¼Œé¿å…è®¡ç®—é”™è¯¯

        # è®°å½•å­¦ä¹ ç‡
        if self.model and hasattr(self.model.optimizer, 'lr'):
            lr = float(self.model.optimizer.lr.numpy())
            self.metrics_history['learning_rate'].append(lr)
        else:
            self.metrics_history['learning_rate'].append(0)

        # è®°å½•GPUä½¿ç”¨æƒ…å†µ
        if GPUtil and tf.test.is_built_with_cuda():
            try:
                gpu = GPUtil.getGPUs()[0]
                self.metrics_history['gpu_memory_usage'].append(gpu.memoryUsed)
            except:
                self.metrics_history['gpu_memory_usage'].append(0)
        else:
            self.metrics_history['gpu_memory_usage'].append(0)

        # è®°å½•æ‰¹å¤„ç†æ—¶é—´
        self.metrics_history['batch_time'].append(time.time())

        # ç¡®ä¿æ‰€æœ‰æ•°ç»„é•¿åº¦ä¸€è‡´
        min_length = min(len(v) for v in self.metrics_history.values())
        for key in self.metrics_history:
            self.metrics_history[key] = self.metrics_history[key][:min_length]

        # æ›´æ–°å¯è§†åŒ–
        self.update_plots()

        # ä¿å­˜æŒ‡æ ‡
        self.save_metrics()

    def _detect_overfitting(self):
        """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
        if len(self.metrics_history['training_loss']) < 5:
            return

        # è®¡ç®—è®­ç»ƒå’ŒéªŒè¯æŸå¤±çš„å·®è·
        train_loss = self.metrics_history['training_loss'][-1]
        val_loss = self.metrics_history['validation_loss'][-1]
        loss_gap = val_loss - train_loss

        # æ£€æµ‹è¿‡æ‹Ÿåˆä¿¡å·
        if loss_gap > self.gap_threshold:
            self.consecutive_increases += 1
            if self.consecutive_increases >= self.max_consecutive_increases:
                self.overfitting_detected = True
                self.overfitting_epoch = self.current_epoch
                print(
                    f"âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆï¼Epoch {self.current_epoch}: è®­ç»ƒæŸå¤±={train_loss:.4f}, éªŒè¯æŸå¤±={val_loss:.4f}, å·®è·={loss_gap:.4f}")
        else:
            self.consecutive_increases = 0

        # æ£€æµ‹éªŒè¯æŸå¤±è¿ç»­å¢åŠ 
        if len(self.metrics_history['validation_loss']) >= 3:
            recent_val_losses = self.metrics_history['validation_loss'][-3:]
            if all(recent_val_losses[i] <= recent_val_losses[i + 1] for i in range(len(recent_val_losses) - 1)):
                print(f"âš ï¸ éªŒè¯æŸå¤±è¿ç»­å¢åŠ ï¼æœ€è¿‘3ä¸ªepoch: {recent_val_losses}")

    def get_overfitting_status(self):
        """è·å–è¿‡æ‹ŸåˆçŠ¶æ€"""
        return {
            'overfitting_detected': self.overfitting_detected,
            'overfitting_epoch': self.overfitting_epoch,
            'consecutive_increases': self.consecutive_increases,
            'current_gap': self.metrics_history['validation_loss'][-1] - self.metrics_history['training_loss'][
                -1] if len(self.metrics_history['validation_loss']) > 0 else 0
        }

    def update_plots(self):
        """æ›´æ–°è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–"""
        # æ¸…é™¤å½“å‰å›¾å½¢
        for ax in self.axes.flat:
            ax.clear()

        # æŸå¤±æ›²çº¿
        ax = self.axes[0, 0]
        ax.plot(self.metrics_history['epoch'], self.metrics_history['training_loss'], label='Training')
        ax.plot(self.metrics_history['epoch'], self.metrics_history['validation_loss'], label='Validation')
        ax.set_title('Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True)

        # åˆ†ç±»å‡†ç¡®ç‡
        ax = self.axes[0, 1]
        ax.plot(self.metrics_history['epoch'], self.metrics_history['training_cls_acc'], label='Training')
        ax.plot(self.metrics_history['epoch'], self.metrics_history['validation_cls_acc'], label='Validation')
        ax.set_title('Classification Accuracy')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Accuracy')
        ax.legend()
        ax.grid(True)

        # å›å½’MAE
        ax = self.axes[1, 0]
        ax.plot(self.metrics_history['epoch'], self.metrics_history['training_reg_mae'], label='Training')
        ax.plot(self.metrics_history['epoch'], self.metrics_history['validation_reg_mae'], label='Validation')
        ax.set_title('Regression MAE')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('MAE')
        ax.legend()
        ax.grid(True)

        # GPUä½¿ç”¨æƒ…å†µ
        ax = self.axes[1, 1]
        if len(self.metrics_history['gpu_memory_usage']) > 0:
            ax.plot(self.metrics_history['epoch'], self.metrics_history['gpu_memory_usage'])
            ax.set_title('GPU Memory Usage')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Memory (MB)')
            ax.grid(True)

        # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
        plt.tight_layout()
        plt.savefig(os.path.join(self.log_dir, 'training_progress.png'))
        plt.close()

    def save_metrics(self):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡"""
        try:
            # ä¿å­˜ä¸ºCSV
            df = pd.DataFrame(self.metrics_history)
            df.to_csv(os.path.join(self.log_dir, 'training_metrics.csv'), index=False)

            # ä¿å­˜ä¸ºJSON
            with open(os.path.join(self.log_dir, 'training_metrics.json'), 'w') as f:
                json.dump(self.metrics_history, f, indent=4)
        except Exception as e:
            print(f"Warning: Failed to save metrics: {str(e)}")


class OptunaCallback(tf.keras.callbacks.Callback):
    """Optunaæ—©åœå›è°ƒ"""

    def __init__(self, trial, monitor='val_loss', patience=3):  # å‡å°‘patienceï¼ŒåŠ å¿«æ—©åœ
        super(OptunaCallback, self).__init__()
        self.trial = trial
        self.monitor = monitor
        self.patience = patience
        self.best_value = float('inf')
        self.wait = 0

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}
        current_value = logs.get(self.monitor)
        if current_value < self.best_value:
            self.best_value = current_value
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.model.stop_training = True
                self.trial.report(self.best_value, epoch)
                raise optuna.TrialPruned()


class OptunaR2EarlyStopping(tf.keras.callbacks.Callback):
    """Optuna RÂ²æ—©åœå›è°ƒï¼Œå½“RÂ²è¾¾åˆ°0.8æ—¶åœæ­¢è°ƒå‚è®­ç»ƒ"""

    def __init__(self, X_val, y_val, min_r2=0.8, trial=None):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.min_r2 = min_r2
        self.trial = trial
        self.best_r2 = 0.0
        self.stopped_epoch = 0

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}

        # è®¡ç®—RÂ² - ä¿®å¤ç‰ˆæœ¬
        try:
            y_pred = self.model.predict(self.X_val, verbose=0)

            # å¦‚æœæ˜¯å¤šè¾“å‡ºæ¨¡å‹ï¼Œå–å›å½’éƒ¨åˆ†çš„é¢„æµ‹
            if isinstance(y_pred, (list, tuple)) and len(y_pred) == 2:
                y_pred_reg = y_pred[1]  # å›å½’è¾“å‡º
            elif isinstance(y_pred, dict):
                y_pred_reg = y_pred['regression']
            else:
                y_pred_reg = y_pred

            # æå–per_muçš„é¢„æµ‹å€¼ï¼ˆç¬¬ä¸€åˆ—ï¼Œç´¢å¼•0ï¼‰
            if y_pred_reg.ndim > 1 and y_pred_reg.shape[1] > 0:
                y_pred_per_mu = y_pred_reg[:, 0]  # per_muæ˜¯ç¬¬ä¸€åˆ—
            else:
                y_pred_per_mu = y_pred_reg

            # ç¡®ä¿y_pred_per_muæ˜¯numpyæ•°ç»„
            if hasattr(y_pred_per_mu, 'values'):
                y_pred_per_mu = y_pred_per_mu.values
            elif not isinstance(y_pred_per_mu, np.ndarray):
                y_pred_per_mu = np.array(y_pred_per_mu)

            # å¤„ç†çœŸå®å€¼ - per_muæ˜¯ç¬¬ä¸€åˆ—ï¼ˆç´¢å¼•0ï¼‰
            if hasattr(self.y_val, 'iloc'):
                # pandas DataFrame
                y_true_per_mu = self.y_val.iloc[:, 0] if self.y_val.shape[1] > 0 else self.y_val.iloc[:, 0]
            else:
                # numpy array
                y_true_per_mu = self.y_val[:, 0] if self.y_val.ndim > 1 else self.y_val

            # ç¡®ä¿çœŸå®å€¼ä¹Ÿæ˜¯numpyæ•°ç»„
            if hasattr(y_true_per_mu, 'values'):
                y_true_per_mu = y_true_per_mu.values
            elif not isinstance(y_true_per_mu, np.ndarray):
                y_true_per_mu = np.array(y_true_per_mu)

            # ç¡®ä¿é•¿åº¦åŒ¹é…
            min_len = min(len(y_true_per_mu), len(y_pred_per_mu))
            y_true_per_mu = y_true_per_mu[:min_len]
            y_pred_per_mu = y_pred_per_mu[:min_len]

            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if epoch == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªepochæ‰“å°è°ƒè¯•ä¿¡æ¯
                print(f"ğŸ” RÂ²è®¡ç®—è°ƒè¯•ä¿¡æ¯:")
                print(f"   çœŸå®å€¼èŒƒå›´: [{y_true_per_mu.min():.4f}, {y_true_per_mu.max():.4f}]")
                print(f"   é¢„æµ‹å€¼èŒƒå›´: [{y_pred_per_mu.min():.4f}, {y_pred_per_mu.max():.4f}]")
                print(f"   çœŸå®å€¼å‡å€¼: {y_true_per_mu.mean():.4f}, æ ‡å‡†å·®: {y_true_per_mu.std():.4f}")
                print(f"   é¢„æµ‹å€¼å‡å€¼: {y_pred_per_mu.mean():.4f}, æ ‡å‡†å·®: {y_pred_per_mu.std():.4f}")

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if len(y_true_per_mu) == 0 or len(y_pred_per_mu) == 0:
                print(f"è­¦å‘Š: æ•°æ®ä¸ºç©ºï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            elif np.std(y_true_per_mu) < 1e-8:
                print(f"è­¦å‘Š: çœŸå®å€¼æ–¹å·®ä¸º0ï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            elif np.std(y_pred_per_mu) < 1e-8:
                print(f"è­¦å‘Š: é¢„æµ‹å€¼æ–¹å·®ä¸º0ï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            else:
                # è®¡ç®—RÂ²
                from sklearn.metrics import r2_score
                r2 = r2_score(y_true_per_mu, y_pred_per_mu)

                # é™åˆ¶RÂ²åœ¨åˆç†èŒƒå›´å†…
                r2 = max(-100, min(1, r2))

        except Exception as e:
            print(f"RÂ²è®¡ç®—é”™è¯¯: {e}")
            r2 = -1000

        if r2 > self.best_r2:
            self.best_r2 = r2

        print(f"Epoch {epoch + 1}: RÂ²={r2:.4f}, Best RÂ²={self.best_r2:.4f}, é˜ˆå€¼={self.min_r2}")

        # å¦‚æœRÂ²è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢è®­ç»ƒå¹¶æŠ¥å‘Šç»™Optuna
        if r2 >= self.min_r2:
            print(f"ğŸ¯ RÂ²è¾¾åˆ°{self.min_r2}ï¼Œåœæ­¢è°ƒå‚è®­ç»ƒï¼Œè¿›å…¥æœ€ç»ˆæ¨¡å‹è®­ç»ƒï¼")
            print(f"å½“å‰RÂ²: {r2:.4f}, é˜ˆå€¼: {self.min_r2}")
            self.model.stop_training = True
            self.stopped_epoch = epoch

            # æ›´æ–°å…¨å±€å˜é‡
            global GLOBAL_R2_ACHIEVED, GLOBAL_BEST_PARAMS, GLOBAL_BEST_R2
            GLOBAL_R2_ACHIEVED = True
            GLOBAL_BEST_R2 = r2
            if self.trial:
                GLOBAL_BEST_PARAMS = self.trial.params
                print(f"ğŸ¯ å…¨å±€æœ€ä½³å‚æ•°å·²ä¿å­˜: {GLOBAL_BEST_PARAMS}")
                print(f"ğŸ¯ å…¨å±€RÂ²çŠ¶æ€: GLOBAL_R2_ACHIEVED={GLOBAL_R2_ACHIEVED}, GLOBAL_BEST_R2={GLOBAL_BEST_R2}")

            # æŠ¥å‘Šç»™Optunaï¼Œä½¿ç”¨è´Ÿçš„RÂ²ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼ˆå› ä¸ºOptunaé»˜è®¤æœ€å°åŒ–ï¼‰
            if self.trial:
                self.trial.report(-r2, epoch)
                # è®¾ç½®trialä¸ºæˆåŠŸçŠ¶æ€
                self.trial.set_user_attr('r2_achieved', r2)
                self.trial.set_user_attr('early_stopped', True)
                print(f"ğŸ¯ Trial {self.trial.number} å·²æ ‡è®°ä¸ºRÂ²è¾¾åˆ°0.8")

            # æŠ›å‡ºç‰¹æ®Šå¼‚å¸¸æ¥åœæ­¢Optunaä¼˜åŒ–
            raise optuna.TrialPruned()

        # è®°å½•RÂ²åˆ°logsä¸­
        logs['val_r2'] = r2


def objective(trial, X_train_combined, y_cls_train_final, y_reg_train_final, X_val_combined, y_cls_val, y_reg_val,
              num_classes, strategy=None):
    """Optunaä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ"""
    # ä»è¶…å‚æ•°ç©ºé—´ä¸­é‡‡æ ·
    params = {
        'lr': trial.suggest_float(
            'lr',
            *Config.optuna_params['param_ranges']['lr'],
            log=True
        ),
        'neurons1': trial.suggest_int(
            'neurons1',
            *Config.optuna_params['param_ranges']['neurons1']
        ),
        'neurons2': trial.suggest_int(
            'neurons2',
            *Config.optuna_params['param_ranges']['neurons2']
        ),
        'dropout_rate': trial.suggest_float(
            'dropout_rate',
            *Config.optuna_params['param_ranges']['dropout_rate']
        ),
        'batch_size': trial.suggest_categorical(
            'batch_size',
            Config.optuna_params['param_ranges']['batch_size']
        ),
        'attention_units': trial.suggest_int(
            'attention_units',
            *Config.optuna_params['param_ranges']['attention_units']
        ),
        'l2_lambda': trial.suggest_float(
            'l2_lambda',
            *Config.optuna_params['param_ranges']['l2_lambda'],
            log=True
        ),
        'optimizer_type': trial.suggest_categorical(
            'optimizer_type',
            Config.optuna_params['param_ranges']['optimizer_type']
        ),
        'activation': trial.suggest_categorical(
            'activation',
            Config.optuna_params['param_ranges']['activation']
        )
    }

    # æ„å»ºæ¨¡å‹
    model = build_hybrid_model(
        input_dim=X_train_combined.shape[1],
        num_classes=num_classes,
        params=params,
        strategy=strategy
    )

    # è®­ç»ƒæ¨¡å‹
    try:
        history = model.fit(
            X_train_combined,
            {
                'classification': y_cls_train_final,
                'output_clipped': y_reg_train_final
            },
            validation_data=(
                X_val_combined,
                {
                    'classification': y_cls_val,
                    'output_clipped': y_reg_val
                }
            ),
            epochs=Config.dl_params['epochs'],
            batch_size=params['batch_size'],
            callbacks=[
                OptunaCallback(trial),
                EarlyStopping(
                    monitor='val_loss',
                    patience=Config.dl_params['early_stop_patience'],
                    restore_best_weights=True
                )
            ],
            verbose=0
        )

        # è¿”å›éªŒè¯é›†æ€§èƒ½
        val_loss = min(history.history['val_loss'])
        return val_loss
    except Exception as e:
        print(f"[Optuna] Trial failed due to exception: {e}")
        raise optuna.TrialPruned()


##############################################
# 5. æœ€ç»ˆè®­ç»ƒ & é¢„æµ‹ & å¯è§†åŒ–
##############################################

def plot_training_history(history):
    """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿"""
    # åˆ›å»ºå­å›¾
    plt.figure(figsize=(15, 10))

    # å­å›¾1ï¼šåˆ†ç±»æŸå¤± vs å›å½’æŸå¤±
    plt.subplot(2, 2, 1)
    plt.plot(history.history['classification_loss'], label='Train Cls Loss')
    plt.plot(history.history['val_classification_loss'], label='Val Cls Loss')
    plt.plot(history.history['regression_loss'], label='Train Yield Loss')
    plt.plot(history.history['val_regression_loss'], label='Val Yield Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Classification & Regression Loss')
    plt.legend()
    plt.grid(True)

    # å­å›¾2ï¼šåˆ†ç±»å‡†ç¡®åº¦ vs å›å½’MAE
    plt.subplot(2, 2, 2)
    plt.plot(history.history['classification_accuracy'], label='Train Cls Acc')
    plt.plot(history.history['val_classification_accuracy'], label='Val Cls Acc')
    plt.plot(history.history['regression_mae'], label='Train Yield MAE')
    plt.plot(history.history['val_regression_mae'], label='Val Yield MAE')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy / MAE')
    plt.title('Classification Accuracy & Yield MAE')
    plt.legend()
    plt.grid(True)

    # å­å›¾3ï¼šæ€»æŸå¤±
    plt.subplot(2, 2, 3)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Total Loss')
    plt.title('Total Loss')
    plt.grid(True)
    plt.legend()

    # å­å›¾4ï¼šMSE
    plt.subplot(2, 2, 4)
    plt.plot(history.history['regression_mse'], label='Train MSE')
    plt.plot(history.history['val_regression_mse'], label='Val MSE')
    plt.xlabel('Epochs')
    plt.ylabel('MSE')
    plt.title('Mean Squared Error')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.savefig(Config.accuracy_loss_map_file, dpi=300)
    plt.close()


def plot_prediction_maps(
        X_val: Union[pd.DataFrame, np.ndarray],
        cls_pred: np.ndarray,
        reg_pred: np.ndarray,
        val_ids: np.ndarray,
        data_val: Union[pd.DataFrame, np.ndarray]
) -> None:
    if not isinstance(X_val, pd.DataFrame):
        X_val = pd.DataFrame(X_val)
    # è‡ªåŠ¨é€‚é…åˆ—å
    x_col = next((c for c in ['x', 'x_product', 'right_x', 'X'] if c in X_val.columns), None)
    y_col = next((c for c in ['y', 'y_product', 'right_y', 'Y'] if c in X_val.columns), None)
    assert x_col and y_col, f"X_val must contain a valid x/y column, got: {X_val.columns.tolist()}"
    # åˆ›å»ºåœ°ç†æ•°æ®æ¡†
    gdf = gpd.GeoDataFrame(
        {
            'ID': np.arange(len(X_val)),
            'X': np.ravel(X_val[x_col].values),
            'Y': np.ravel(X_val[y_col].values),
            'suitability': np.ravel(np.argmax(cls_pred, axis=1)),
            'yield_per_cell': np.ravel(reg_pred[:, 0]),
            'yield_per_mu': np.ravel(reg_pred[:, 1])
        }
    )
    # æ·»åŠ å‡ ä½•åˆ—
    gdf['geometry'] = gpd.points_from_xy(gdf['X'], gdf['Y'])
    # åˆ›å»ºä¸‰ä¸ªå­å›¾
    fig, axes = plt.subplots(1, 3, figsize=(20, 8))
    # é€‚å®œæ€§ç­‰çº§åœ°å›¾
    gdf.plot(
        column='suitability',
        ax=axes[0],
        legend=True,
        cmap='RdYlGn',
        legend_kwds={'label': 'é€‚å®œæ€§ç­‰çº§'}
    )
    axes[0].set_title('é€‚å®œæ€§ç­‰çº§åˆ†å¸ƒ')
    # åŒºäº§é‡é¢„æµ‹åœ°å›¾
    gdf.plot(
        column='yield_per_cell',
        ax=axes[1],
        legend=True,
        cmap='viridis',
        legend_kwds={'label': 'åŒºäº§é‡ (kg/cell)'}
    )
    axes[1].set_title('åŒºäº§é‡é¢„æµ‹åˆ†å¸ƒ')
    # äº©äº§é‡é¢„æµ‹åœ°å›¾
    gdf.plot(
        column='yield_per_mu',
        ax=axes[2],
        legend=True,
        cmap='viridis',
        legend_kwds={'label': 'äº©äº§é‡ (kg/mu)'}
    )
    axes[2].set_title('äº©äº§é‡é¢„æµ‹åˆ†å¸ƒ')
    # ä¸ºæ¯ä¸ªå­å›¾æ·»åŠ åæ ‡è½´æ ‡ç­¾
    for ax in axes:
        ax.set_xlabel('ç»åº¦')
        ax.set_ylabel('çº¬åº¦')
        ax.grid(True)
    plt.tight_layout()
    plt.savefig(Config.suitability_map_file, dpi=300)
    plt.close()


def evaluate_model(model, X_val, y_cls_val, y_reg_val):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # è·å–é¢„æµ‹ç»“æœ
    cls_pred, reg_pred = model.predict(X_val)
    cls_pred_labels = np.argmax(cls_pred, axis=1)
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(y_reg_val, pd.DataFrame):
        y_reg_val = y_reg_val.values
    if isinstance(reg_pred, pd.DataFrame):
        reg_pred = reg_pred.values
    # === åå˜æ¢log1p ===
    y_reg_val_true = np.expm1(y_reg_val)
    reg_pred_true = np.expm1(reg_pred)
    # åˆ†ç±»æŒ‡æ ‡
    cls_accuracy = accuracy_score(y_cls_val, cls_pred_labels)
    cls_report = classification_report(y_cls_val, cls_pred_labels)
    # å›å½’æŒ‡æ ‡ - åªå…³æ³¨per_muï¼ˆç¬¬ä¸€ä¸ªè¾“å‡ºï¼‰
    reg_r2 = [r2_score(y_reg_val_true[:, 0], reg_pred_true[:, 0])]  # åªè®¡ç®—per_muçš„RÂ²
    reg_mae = [mean_absolute_error(y_reg_val_true[:, i], reg_pred_true[:, i]) for i in range(y_reg_val_true.shape[1])]
    reg_rmse = [np.sqrt(mean_squared_error(y_reg_val_true[:, i], reg_pred_true[:, i])) for i in
                range(y_reg_val_true.shape[1])]
    # åˆ›å»ºè¯„ä¼°æŠ¥å‘Š
    evaluation_report = {
        'classification': {
            'accuracy': cls_accuracy,
            'detailed_report': cls_report
        },
        'regression': {
            'r2_scores': {
                f'output_{i}': score for i, score in enumerate(reg_r2)
            },
            'mae_scores': {
                f'output_{i}': score for i, score in enumerate(reg_mae)
            },
            'rmse_scores': {
                f'output_{i}': score for i, score in enumerate(reg_rmse)
            }
        }
    }
    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    report_file = Config.evaluation_file
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_report, f, indent=4, ensure_ascii=False)
    # æ‰“å°è¯„ä¼°ç»“æœ
    print("\næ¨¡å‹è¯„ä¼°æŠ¥å‘Š:")
    print("\nåˆ†ç±»æ€§èƒ½:")
    print(f"å‡†ç¡®ç‡: {cls_accuracy:.4f}")
    print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(cls_report)
    print("\nå›å½’æ€§èƒ½:")
    print(f"\nper_mu (æƒé‡=1):")
    print(f"RÂ² åˆ†æ•°: {reg_r2[0]:.4f}")
    print(f"MAE: {reg_mae[0]:.4f}")
    print(f"RMSE: {reg_rmse[0]:.4f}")
    print(f"\nper_qu (æƒé‡=0):")
    print(f"MAE: {reg_mae[1]:.4f}")
    print(f"RMSE: {reg_rmse[1]:.4f}")
    print("æ³¨æ„ï¼šRÂ²å€¼åªè®¡ç®—per_muï¼Œper_quæƒé‡ä¸º0")
    return evaluation_report


def plot_confusion_matrix(y_true, y_pred, classes):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=classes,
        yticklabels=classes
    )
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(Config.confusion_matrix_file, dpi=300)
    plt.close()


def plot_regression_scatter(y_true, y_pred, output_names):
    """ç»˜åˆ¶å›å½’æ•£ç‚¹å›¾"""
    # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹ä¸€è‡´
    if isinstance(y_true, pd.DataFrame):
        y_true = y_true.values
    if isinstance(y_pred, pd.DataFrame):
        y_pred = y_pred.values

    # ç¡®ä¿æ˜¯2Dæ•°ç»„
    if y_true.ndim == 1:
        y_true = y_true.reshape(-1, 1)
    if y_pred.ndim == 1:
        y_pred = y_pred.reshape(-1, 1)

    n_outputs = y_true.shape[1]
    fig, axes = plt.subplots(1, n_outputs, figsize=(15, 5))

    if n_outputs == 1:
        axes = [axes]

    for i, (ax, name) in enumerate(zip(axes, output_names)):
        ax.scatter(y_true[:, i], y_pred[:, i], alpha=0.5)
        ax.plot([y_true[:, i].min(), y_true[:, i].max()],
                [y_true[:, i].min(), y_true[:, i].max()],
                'r--', lw=2)
        ax.set_xlabel('çœŸå®å€¼')
        ax.set_ylabel('é¢„æµ‹å€¼')
        ax.set_title(f'{name} é¢„æµ‹vsçœŸå®å€¼')

        # æ·»åŠ RÂ²å€¼ - åªå…³æ³¨per_mu
        if i == 0:  # åªå¯¹per_muè®¡ç®—RÂ²
            r2 = r2_score(y_true[:, i], y_pred[:, i])
            ax.text(0.05, 0.95, f'RÂ² (per_mu) = {r2:.4f}',
                    transform=ax.transAxes,
                    verticalalignment='top')
        else:  # per_quä¸æ˜¾ç¤ºRÂ²
            ax.text(0.05, 0.95, f'per_qu (æƒé‡=0)',
                    transform=ax.transAxes,
                    verticalalignment='top')

    plt.tight_layout()
    plt.savefig(Config.regression_scatter_file, dpi=300)
    plt.close()


def plot_feature_importance(model, feature_names):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(12, 6))
        plt.title('ç‰¹å¾é‡è¦æ€§')
        plt.bar(range(len(importances)),
                importances[indices])
        plt.xticks(range(len(importances)),
                   [feature_names[i] for i in indices],
                   rotation=45,
                   ha='right')
        plt.tight_layout()
        plt.savefig(Config.feature_importance_map_file, dpi=300)
        plt.close()


def emergency_feature_selection(X_train, y_train, X_val, y_val, max_features=20):
    """
    ç´§æ€¥ç‰¹å¾é€‰æ‹©ï¼šä½¿ç”¨å¤šç§æ–¹æ³•é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
    """
    print("ğŸš¨ æ‰§è¡Œç´§æ€¥ç‰¹å¾é€‰æ‹©...")

    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression, mutual_info_regression
    from sklearn.linear_model import Ridge

    # æ–¹æ³•1ï¼šåŸºäºæ–¹å·®çš„ç‰¹å¾é€‰æ‹©
    print("1. åŸºäºæ–¹å·®é€‰æ‹©ç‰¹å¾...")
    var_selector = VarianceThreshold(threshold=0.01)
    X_train_var = var_selector.fit_transform(X_train)
    X_val_var = var_selector.transform(X_val)

    # æ–¹æ³•2ï¼šåŸºäºFç»Ÿè®¡é‡çš„ç‰¹å¾é€‰æ‹©
    print("2. åŸºäºFç»Ÿè®¡é‡é€‰æ‹©ç‰¹å¾...")
    try:
        f_selector = SelectKBest(score_func=f_regression, k=min(30, X_train_var.shape[1]))
        X_train_f = f_selector.fit_transform(X_train_var, y_train[:, 0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡
        X_val_f = f_selector.transform(X_val_var)
        f_scores = f_selector.scores_
        f_features = f_selector.get_support(indices=True)
        print(f"Fç»Ÿè®¡é‡é€‰æ‹©äº† {len(f_features)} ä¸ªç‰¹å¾")
    except Exception as e:
        print(f"Fç»Ÿè®¡é‡é€‰æ‹©å¤±è´¥: {e}")
        X_train_f = X_train_var
        X_val_f = X_val_var
        f_features = list(range(X_train_var.shape[1]))

    # æ–¹æ³•3ï¼šåŸºäºäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©
    print("3. åŸºäºäº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾...")
    try:
        mi_selector = SelectKBest(score_func=mutual_info_regression, k=min(20, X_train_f.shape[1]))
        X_train_mi = mi_selector.fit_transform(X_train_f, y_train[:, 0])
        X_val_mi = mi_selector.transform(X_val_f)
        mi_scores = mi_selector.scores_
        mi_features = mi_selector.get_support(indices=True)
        print(f"äº’ä¿¡æ¯é€‰æ‹©äº† {len(mi_features)} ä¸ªç‰¹å¾")
    except Exception as e:
        print(f"äº’ä¿¡æ¯é€‰æ‹©å¤±è´¥: {e}")
        X_train_mi = X_train_f
        X_val_mi = X_val_f
        mi_features = list(range(X_train_f.shape[1]))

    # æ–¹æ³•4ï¼šåŸºäºRidgeå›å½’çš„ç‰¹å¾é€‰æ‹©
    print("4. åŸºäºRidgeå›å½’é€‰æ‹©ç‰¹å¾...")
    try:
        ridge = Ridge(alpha=1.0)
        ridge.fit(X_train_mi, y_train[:, 0])
        ridge_coefs = np.abs(ridge.coef_)

        # é€‰æ‹©ç³»æ•°æœ€å¤§çš„ç‰¹å¾
        top_indices = np.argsort(ridge_coefs)[-max_features:]
        X_train_final = X_train_mi[:, top_indices]
        X_val_final = X_val_mi[:, top_indices]

        print(f"Ridgeå›å½’é€‰æ‹©äº† {len(top_indices)} ä¸ªç‰¹å¾")
        print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {X_train_final.shape[1]}")

        return X_train_final, X_val_final, top_indices

    except Exception as e:
        print(f"Ridgeå›å½’é€‰æ‹©å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•çš„ç‰¹å¾é€‰æ‹©
        X_train_final = X_train_mi[:, :max_features]
        X_val_final = X_val_mi[:, :max_features]
        return X_train_final, X_val_final, list(range(max_features))


def analyze_feature_importance(
        model,
        X_train: Union[pd.DataFrame, np.ndarray, list],
        y_reg_train: Union[pd.DataFrame, np.ndarray, list],
        feature_names: list,
        threshold: float = 0.02
) -> pd.DataFrame:
    assert isinstance(feature_names, list), f"feature_names must be a list, got {type(feature_names)}"
    assert 0 <= threshold <= 1, "threshold must be between 0 and 1"
    try:
        print("\nè®¡ç®—ç‰¹å¾é‡è¦æ€§...")

        # ç¡®ä¿X_trainæ˜¯numpyæ•°ç»„
        if isinstance(X_train, pd.DataFrame):
            X_train = X_train.values
        elif isinstance(X_train, list):
            X_train = np.array(X_train)
        elif hasattr(X_train, 'values'):
            X_train = X_train.values

        # ä¸è¿›è¡Œç‰¹å¾é‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾è¿›è¡Œé‡è¦æ€§è®¡ç®—
        print(f"ä½¿ç”¨å…¨éƒ¨ç‰¹å¾è¿›è¡Œé‡è¦æ€§è®¡ç®—: {X_train.shape[1]} ä¸ªç‰¹å¾")

        # æ£€æŸ¥ç‰¹å¾ç›¸å…³æ€§ï¼Œé¿å…é‡å¤ç‰¹å¾å½±å“é‡è¦æ€§è®¡ç®—
        print("ğŸ” æ£€æŸ¥ç‰¹å¾ç›¸å…³æ€§...")
        try:
            if hasattr(X_train, 'values'):
                X_train_array = X_train.values
            else:
                X_train_array = np.array(X_train)

            # è®¡ç®—ç‰¹å¾é—´ç›¸å…³æ€§
            correlation_matrix = np.corrcoef(X_train_array.T)
            high_corr_pairs = []
            for i in range(len(feature_names)):
                for j in range(i + 1, len(feature_names)):
                    if abs(correlation_matrix[i, j]) > 0.95:  # é«˜ç›¸å…³æ€§é˜ˆå€¼
                        high_corr_pairs.append((feature_names[i], feature_names[j], correlation_matrix[i, j]))

            if high_corr_pairs:
                print(f"âš ï¸ å‘ç° {len(high_corr_pairs)} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾ (>0.95):")
                for feat1, feat2, corr in high_corr_pairs[:10]:  # åªæ˜¾ç¤ºå‰10å¯¹
                    print(f"  {feat1} <-> {feat2}: {corr:.4f}")
                if len(high_corr_pairs) > 10:
                    print(f"  ... è¿˜æœ‰ {len(high_corr_pairs) - 10} å¯¹é«˜ç›¸å…³æ€§ç‰¹å¾")
            else:
                print("âœ… æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾")

        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾ç›¸å…³æ€§æ£€æŸ¥å¤±è´¥: {str(e)}")

        # ä½¿ç”¨XGBoostçš„ç‰¹å¾é‡è¦æ€§
        if hasattr(model, 'get_score') or hasattr(model, 'feature_importances_'):
            print("ä½¿ç”¨XGBoostçš„ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹æ³•...")
            try:
                # ä¼˜å…ˆä½¿ç”¨feature_importances_å±æ€§
                if hasattr(model, 'feature_importances_'):
                    feature_importance = model.feature_importances_
                    print(f"âœ… æˆåŠŸè·å–XGBoost feature_importances_: {len(feature_importance)} ä¸ªç‰¹å¾")
                else:
                    # ä½¿ç”¨get_scoreæ–¹æ³•
                    importance_scores = {}
                    for importance_type in ['weight', 'gain', 'cover']:
                        try:
                            scores = model.get_score(importance_type=importance_type)
                            if scores:
                                for feature, score in scores.items():
                                    if feature not in importance_scores:
                                        importance_scores[feature] = 0
                                    importance_scores[feature] += score
                        except Exception as e:
                            print(f"Warning: æ— æ³•è·å– {importance_type} ç±»å‹çš„ç‰¹å¾é‡è¦æ€§: {str(e)}")

                    if importance_scores:
                        # å°†ç‰¹å¾é‡è¦æ€§æ˜ å°„åˆ°feature_names
                        feature_importance = []
                        for feature in feature_names:
                            # å°è¯•åŒ¹é…ç‰¹å¾åï¼ˆå¯èƒ½åŒ…å«å¶å­ç‰¹å¾ï¼‰
                            if feature in importance_scores:
                                feature_importance.append(importance_scores[feature])
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…åŸå§‹ç‰¹å¾å
                                original_feature = feature.replace('xgb_leaf_', '') if feature.startswith(
                                    'xgb_leaf_') else feature
                                if original_feature in importance_scores:
                                    feature_importance.append(importance_scores[original_feature])
                                else:
                                    feature_importance.append(0.0)
                        print(f"âœ… æˆåŠŸè·å–XGBoost get_score: {len(feature_importance)} ä¸ªç‰¹å¾")
                    else:
                        raise ValueError("æ— æ³•è·å–XGBoostç‰¹å¾é‡è¦æ€§åˆ†æ•°")

            except Exception as e:
                print(f"XGBoostç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {str(e)}")
                # å›é€€åˆ°åŸºäºæ–¹å·®çš„ç‰¹å¾é‡è¦æ€§
                try:
                    if hasattr(X_train, 'values'):
                        X_train_array = X_train.values
                    else:
                        X_train_array = np.array(X_train)

                    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ–¹å·®ä½œä¸ºé‡è¦æ€§
                    feature_variances = np.var(X_train_array, axis=0)
                    if np.sum(feature_variances) > 0:
                        feature_importance = feature_variances / np.sum(feature_variances)
                    else:
                        feature_importance = np.ones(len(feature_names)) / len(feature_names)
                    print(f"âœ… ä½¿ç”¨æ–¹å·®ä½œä¸ºç‰¹å¾é‡è¦æ€§: {len(feature_importance)} ä¸ªç‰¹å¾")
                except Exception as e2:
                    print(f"æ–¹å·®ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {str(e2)}")
                    feature_importance = [1.0 / len(feature_names)] * len(feature_names)
        else:
            print("ä½¿ç”¨SHAPå€¼è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
            try:
                import shap
                sample_size = min(Config.feature_importance['sample_size'] // 200, len(X_train))  # ä½¿ç”¨é…ç½®çš„1/200ä½œä¸ºSHAPæ ·æœ¬
                X_sample = X_train[:sample_size]
                if hasattr(model, "get_booster") or hasattr(model, "feature_importances_"):
                    explainer = shap.TreeExplainer(model)
                    shap_values = explainer.shap_values(X_sample)
                else:
                    explainer = shap.KernelExplainer(
                        model.predict if hasattr(model, 'predict') else model,
                        X_sample,
                        link="identity"
                    )
                    shap_values = explainer.shap_values(X_sample)
                if isinstance(shap_values, list):
                    feature_importance = np.mean([np.abs(sv).mean(0) for sv in shap_values], axis=0)
                else:
                    feature_importance = np.abs(shap_values).mean(0)
            except ImportError:
                print("SHAPæœªå®‰è£…ï¼Œä½¿ç”¨åŸºäºæ’åˆ—çš„ç‰¹å¾é‡è¦æ€§...")
                try:
                    from sklearn.ensemble import RandomForestRegressor
                    # ä½¿ç”¨æ›´å°çš„æ ·æœ¬è¿›è¡ŒRandomForestè®­ç»ƒï¼Œé¿å…å†…å­˜é—®é¢˜
                    sample_size = min(10000, len(X_train))
                    if sample_size < len(X_train):
                        indices = np.random.choice(len(X_train), sample_size, replace=False)
                        X_sample = X_train.iloc[indices] if hasattr(X_train, 'iloc') else X_train[indices]
                        y_sample = y_reg_train.iloc[indices] if hasattr(y_reg_train, 'iloc') else y_reg_train[indices]
                    else:
                        X_sample = X_train
                        y_sample = y_reg_train

                    # å®‰å…¨åœ°å¤„ç†ç›®æ ‡å˜é‡
                    if hasattr(y_sample, 'iloc'):
                        y_target = y_sample.iloc[:, 0].values
                    elif hasattr(y_sample, 'values'):
                        y_target = y_sample.values[:, 0] if y_sample.values.ndim > 1 else y_sample.values
                    else:
                        y_target = y_sample[:, 0] if np.array(y_sample).ndim > 1 else y_sample

                    # ç¡®ä¿X_trainæ˜¯numpyæ•°ç»„
                    if hasattr(X_sample, 'values'):
                        X_train_array = X_sample.values
                    else:
                        X_train_array = np.array(X_sample)

                    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                    y_target = np.asarray(y_target, dtype=np.float64)
                    X_train_array = np.asarray(X_train_array, dtype=np.float64)

                    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
                    if X_train_array.shape[0] != len(y_target):
                        print(f"æ•°æ®å½¢çŠ¶ä¸åŒ¹é…: X={X_train_array.shape}, y={y_target.shape}")
                        raise ValueError("æ•°æ®å½¢çŠ¶ä¸åŒ¹é…")

                    rf = RandomForestRegressor(n_estimators=10, random_state=Config.random_seed, n_jobs=1, max_depth=10)
                    rf.fit(X_train_array, y_target)
                    feature_importance = rf.feature_importances_
                    print(f"âœ… æˆåŠŸä½¿ç”¨RandomForestè®¡ç®—ç‰¹å¾é‡è¦æ€§: {len(feature_importance)} ä¸ªç‰¹å¾")
                except Exception as e:
                    print(f"RandomForestç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥: {str(e)}")
                    # ä½¿ç”¨ç®€å•çš„æ–¹å·®ç‰¹å¾é‡è¦æ€§
                    try:
                        from sklearn.feature_selection import mutual_info_regression
                        # ä½¿ç”¨æ›´å°çš„æ ·æœ¬è¿›è¡Œäº’ä¿¡æ¯è®¡ç®—ï¼Œé¿å…å†…å­˜é—®é¢˜
                        sample_size = min(5000, len(X_train))
                        if sample_size < len(X_train):
                            indices = np.random.choice(len(X_train), sample_size, replace=False)
                            X_sample = X_train.iloc[indices] if hasattr(X_train, 'iloc') else X_train[indices]
                            y_sample = y_reg_train.iloc[indices] if hasattr(y_reg_train, 'iloc') else y_reg_train[
                                indices]
                        else:
                            X_sample = X_train
                            y_sample = y_reg_train

                        # å®‰å…¨åœ°å¤„ç†ç›®æ ‡å˜é‡
                        if hasattr(y_sample, 'iloc'):
                            y_target = y_sample.iloc[:, 0].values
                        elif hasattr(y_sample, 'values'):
                            y_target = y_sample.values[:, 0] if y_sample.values.ndim > 1 else y_sample.values
                        else:
                            y_target = np.array(y_sample)[:, 0] if np.array(y_sample).ndim > 1 else np.array(y_sample)

                        # å®‰å…¨åœ°å¤„ç†ç‰¹å¾æ•°æ®
                        if hasattr(X_sample, 'values'):
                            X_train_array = X_sample.values
                        else:
                            X_train_array = np.array(X_sample)

                        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                        X_train_array = np.asarray(X_train_array, dtype=np.float64)
                        y_target = np.asarray(y_target, dtype=np.float64)

                        # æ£€æŸ¥æ•°æ®å½¢çŠ¶
                        if X_train_array.shape[0] != len(y_target):
                            print(f"æ•°æ®å½¢çŠ¶ä¸åŒ¹é…: X={X_train_array.shape}, y={y_target.shape}")
                            raise ValueError("æ•°æ®å½¢çŠ¶ä¸åŒ¹é…")

                        feature_importance = mutual_info_regression(X_train_array, y_target,
                                                                    random_state=Config.random_seed)
                        print(f"âœ… æˆåŠŸä½¿ç”¨äº’ä¿¡æ¯è®¡ç®—ç‰¹å¾é‡è¦æ€§: {len(feature_importance)} ä¸ªç‰¹å¾")
                    except Exception as e2:
                        print(f"äº’ä¿¡æ¯ç‰¹å¾é‡è¦æ€§è®¡ç®—ä¹Ÿå¤±è´¥: {str(e2)}")
        # æœ€åå›é€€åˆ°åŸºäºæ–¹å·®çš„ç‰¹å¾é‡è¦æ€§
        try:
            # ä½¿ç”¨ç‰¹å¾æ–¹å·®ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
            if hasattr(X_train, 'values'):
                X_train_array = X_train.values
            else:
                X_train_array = np.array(X_train)

            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ–¹å·®
            feature_variances = np.var(X_train_array, axis=0)
            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            if np.sum(feature_variances) > 0:
                feature_importance = feature_variances / np.sum(feature_variances)
            else:
                feature_importance = np.ones(len(feature_names)) / len(feature_names)
        except Exception as e3:
            print(f"æ–¹å·®ç‰¹å¾é‡è¦æ€§è®¡ç®—ä¹Ÿå¤±è´¥: {str(e3)}")
            # æœ€åå›é€€åˆ°ç´§æ€¥ç‰¹å¾é€‰æ‹©
            try:
                print("å°è¯•ç´§æ€¥ç‰¹å¾é€‰æ‹©...")
                X_train_emergency, _, selected_indices = emergency_feature_selection(
                    X_train, y_reg_train, X_train, y_reg_train, max_features=Config.feature_importance['max_features']
                )
                # åˆ›å»ºåŸºäºç´§æ€¥é€‰æ‹©çš„ç‰¹å¾é‡è¦æ€§
                feature_importance = np.zeros(len(feature_names))
                for idx in selected_indices:
                    if idx < len(feature_importance):
                        feature_importance[idx] = 1.0
                # å½’ä¸€åŒ–
                if feature_importance.sum() > 0:
                    feature_importance = feature_importance / feature_importance.sum()
                else:
                    feature_importance = np.ones(len(feature_names)) / len(feature_names)
                print(f"ç´§æ€¥ç‰¹å¾é€‰æ‹©é€‰æ‹©äº† {len(selected_indices)} ä¸ªç‰¹å¾")
            except Exception as e4:
                print(f"ç´§æ€¥ç‰¹å¾é€‰æ‹©ä¹Ÿå¤±è´¥: {e4}")
                feature_importance = np.ones(len(feature_names)) / len(feature_names)

        # flattenå’Œæ–­è¨€ï¼Œç»Ÿä¸€DataFrameåˆ›å»º
        feature_importance = np.asarray(feature_importance).flatten()
        if len(feature_importance) != len(feature_names):
            print(f"è­¦å‘Šï¼šç‰¹å¾é‡è¦æ€§é•¿åº¦({len(feature_importance)})ä¸ç‰¹å¾åé•¿åº¦({len(feature_names)})ä¸åŒ¹é…")
            print(f"è°ƒæ•´ç‰¹å¾ååˆ—è¡¨é•¿åº¦ä»¥åŒ¹é…ç‰¹å¾é‡è¦æ€§é•¿åº¦")
            # å¦‚æœç‰¹å¾é‡è¦æ€§é•¿åº¦å¤§äºç‰¹å¾åé•¿åº¦ï¼Œæ·»åŠ ç¼ºå¤±çš„ç‰¹å¾å
            if len(feature_importance) > len(feature_names):
                missing_count = len(feature_importance) - len(feature_names)
                additional_names = [f'feature_{i}' for i in
                                    range(len(feature_names), len(feature_names) + missing_count)]
                feature_names = feature_names + additional_names
                print(f"æ·»åŠ äº† {missing_count} ä¸ªç‰¹å¾å")
            # å¦‚æœç‰¹å¾é‡è¦æ€§é•¿åº¦å°äºç‰¹å¾åé•¿åº¦ï¼Œæˆªæ–­ç‰¹å¾ååˆ—è¡¨
            elif len(feature_importance) < len(feature_names):
                feature_names = feature_names[:len(feature_importance)]
                print(f"æˆªæ–­ç‰¹å¾ååˆ—è¡¨åˆ° {len(feature_importance)} ä¸ªç‰¹å¾")

        # ä¿æŒåŸå§‹ç‰¹å¾åï¼Œä¸è¿›è¡Œæ˜ å°„ä¿®æ”¹
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        })
        # åç»­æ‰€æœ‰æ“ä½œéƒ½ç”¨importance_df
        importance_df = importance_df.sort_values('importance', ascending=False)
        total_importance = importance_df['importance'].sum()
        if total_importance > 0:
            importance_df['relative_importance'] = importance_df['importance'] / total_importance
        else:
            importance_df['relative_importance'] = 0
        key_features = importance_df[importance_df['relative_importance'] > threshold]['feature'].tolist()
        min_features = Config.feature_importance.get('min_features', 3)
        max_features = Config.feature_importance.get('max_features', 100)  # æ–°å¢æœ€å¤§ç‰¹å¾æ•°é™åˆ¶

        if len(key_features) < min_features:
            key_features = importance_df.nlargest(min_features, 'importance')['feature'].tolist()
        elif len(key_features) > max_features:
            # å¦‚æœç‰¹å¾è¿‡å¤šï¼Œé€‰æ‹©æœ€é‡è¦çš„max_featuresä¸ª
            key_features = importance_df.nlargest(max_features, 'importance')['feature'].tolist()
            print(f"ç‰¹å¾æ•°é‡è¿‡å¤šï¼Œå·²é™åˆ¶ä¸ºå‰{max_features}ä¸ªæœ€é‡è¦çš„ç‰¹å¾")
        print(f"\næ‰¾åˆ° {len(key_features)} ä¸ªå…³é”®ç‰¹å¾:")
        for idx, feature in enumerate(key_features, 1):
            importance = importance_df.loc[importance_df['feature'] == feature, 'importance'].values[0]
            relative_importance = importance_df.loc[importance_df['feature'] == feature, 'relative_importance'].values[
                0]
            print(f"{idx}. {feature} (é‡è¦æ€§: {importance:.4f}, ç›¸å¯¹é‡è¦æ€§: {relative_importance:.4f})")
        os.makedirs(os.path.dirname(Config.feature_importance_file), exist_ok=True)
        importance_df.to_csv(Config.feature_importance_file, index=False, encoding='utf-8')
        print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³: {Config.feature_importance_file}")
        os.makedirs(os.path.dirname(Config.key_features_list_file), exist_ok=True)
        key_features_data = {
            'key_features': key_features,
            'importance_threshold': threshold,
            'feature_count': len(key_features),
            'feature_importances': {
                feature: float(importance_df.loc[importance_df['feature'] == feature, 'importance'].values[0])
                for feature in key_features
            },
            'relative_importances': {
                feature: float(importance_df.loc[importance_df['feature'] == feature, 'relative_importance'].values[0])
                for feature in key_features
            }
        }
        with open(Config.key_features_list_file, 'w', encoding='utf-8') as f:
            json.dump(key_features_data, f, indent=4, ensure_ascii=False)
        print(f"å…³é”®ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜è‡³: {Config.key_features_list_file}")
        plt.figure(figsize=(12, 6))
        plt.title('ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ')
        plt.bar(range(len(importance_df)), importance_df['importance'].values)
        plt.xticks(range(len(importance_df)),
                   importance_df['feature'].values,
                   rotation=45,
                   ha='right')
        plt.tight_layout()
        os.makedirs(os.path.dirname(Config.feature_importance_plot), exist_ok=True)
        plt.savefig(Config.feature_importance_plot, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {Config.feature_importance_plot}")

        # ä¿å­˜åŸå§‹ç‰¹å¾åˆ—ååˆ°æ–‡ä»¶ï¼Œç¡®ä¿ä¸è®­ç»ƒé›†ä¸€è‡´
        original_feature_names_file = os.path.join(Config.feature_importance_dir, "original_feature_names.txt")
        with open(original_feature_names_file, 'w', encoding='utf-8') as f:
            f.write("åŸå§‹è®­ç»ƒé›†ç‰¹å¾åˆ—åï¼ˆä¸feature_importance_AA.csvä¸­çš„featureåˆ—ä¸€è‡´ï¼‰ï¼š\n")
            f.write("=" * 60 + "\n")
            for i, feature in enumerate(feature_names, 1):
                f.write(f"{i:3d}. {feature}\n")
        print(f"åŸå§‹ç‰¹å¾åˆ—åå·²ä¿å­˜è‡³: {original_feature_names_file}")
        return importance_df
    except Exception as e:
        print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå‡ºé”™: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        tb = traceback.extract_tb(e.__traceback__)
        if tb:
            print(f"é”™è¯¯ä½ç½®: {tb[-1].filename}:{tb[-1].lineno}")
        feature_importance = np.ones(len(feature_names)) / len(feature_names)
        feature_importance = np.asarray(feature_importance).flatten()
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': feature_importance
        })
        return importance_df


def save_predictions_and_importance(ids, suitability, area_yield, mu_yield, feature_importance):
    """ä¿å­˜é¢„æµ‹ç»“æœå’Œç‰¹å¾é‡è¦æ€§"""
    with tqdm(total=2, desc="ä¿å­˜ç»“æœ") as pbar:
        # ä¿å­˜é¢„æµ‹ç»“æœ
        results = pd.DataFrame({
            'ID': ids,
            Config.target_columns['classification']: suitability,
            Config.target_columns['regression'][0]: area_yield,
            Config.target_columns['regression'][1]: mu_yield
        })

        # æ·»åŠ é¢„æµ‹ç»“æœçš„æè¿°æ€§ç»Ÿè®¡
        print("\nä¿å­˜çš„é¢„æµ‹ç»“æœç»Ÿè®¡:")
        print("\né€‚å®œåº¦åˆ†å¸ƒ:")
        print(results[Config.target_columns['classification']].value_counts())
        print("\näº§é‡ç»Ÿè®¡:")
        print(results[[Config.target_columns['regression'][0], Config.target_columns['regression'][1]]].describe())

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(Config.result_file), exist_ok=True)
        results.to_csv(Config.result_file, index=False, encoding='utf-8')
        pbar.update(1)

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        os.makedirs(os.path.dirname(Config.feature_importance_file), exist_ok=True)
        feature_importance.to_csv(Config.feature_importance_file, index=False, encoding='utf-8')
        print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(feature_importance.head(10))
        pbar.update(1)

    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {Config.result_file}")
    print(f"ç‰¹å¾é‡è¦æ€§åˆ†æå·²ä¿å­˜åˆ°: {Config.feature_importance_file}")


def has_invalid(arr):
    return np.any(np.isnan(arr)) or np.any(np.isinf(arr))


def validate_training_data(X_train, y_reg_train, y_cls_train, X_val, y_reg_val, y_cls_val):
    """éªŒè¯è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œä¸€è‡´æ€§"""
    print("\nğŸ” éªŒè¯è®­ç»ƒæ•°æ®è´¨é‡...")

    # æ£€æŸ¥ç‰¹å¾æ•°æ®
    print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train.shape}")
    print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val.shape}")

    # æ£€æŸ¥æ ‡ç­¾æ•°æ®
    print(f"è®­ç»ƒé›†å›å½’æ ‡ç­¾å½¢çŠ¶: {y_reg_train.shape}")
    print(f"éªŒè¯é›†å›å½’æ ‡ç­¾å½¢çŠ¶: {y_reg_val.shape}")
    print(f"è®­ç»ƒé›†åˆ†ç±»æ ‡ç­¾å½¢çŠ¶: {y_cls_train.shape}")
    print(f"éªŒè¯é›†åˆ†ç±»æ ‡ç­¾å½¢çŠ¶: {y_cls_val.shape}")

    # æ£€æŸ¥NaNå’ŒInfå€¼
    x_train_nan = np.isnan(X_train.values).sum() if hasattr(X_train, 'values') else np.isnan(X_train).sum()
    x_val_nan = np.isnan(X_val.values).sum() if hasattr(X_val, 'values') else np.isnan(X_val).sum()
    y_reg_train_nan = np.isnan(y_reg_train.values).sum() if hasattr(y_reg_train, 'values') else np.isnan(
        y_reg_train).sum()
    y_reg_val_nan = np.isnan(y_reg_val.values).sum() if hasattr(y_reg_val, 'values') else np.isnan(y_reg_val).sum()

    print(f"è®­ç»ƒé›†ç‰¹å¾NaNæ•°é‡: {x_train_nan}")
    print(f"éªŒè¯é›†ç‰¹å¾NaNæ•°é‡: {x_val_nan}")
    print(f"è®­ç»ƒé›†å›å½’æ ‡ç­¾NaNæ•°é‡: {y_reg_train_nan}")
    print(f"éªŒè¯é›†å›å½’æ ‡ç­¾NaNæ•°é‡: {y_reg_val_nan}")

    # æ£€æŸ¥æ ‡ç­¾å€¼èŒƒå›´
    if hasattr(y_reg_train, 'values'):
        y_reg_train_values = y_reg_train.values
    else:
        y_reg_train_values = y_reg_train

    if hasattr(y_reg_val, 'values'):
        y_reg_val_values = y_reg_val.values
    else:
        y_reg_val_values = y_reg_val

    print(f"è®­ç»ƒé›†per_muèŒƒå›´: [{y_reg_train_values[:, 1].min():.4f}, {y_reg_train_values[:, 1].max():.4f}]")
    print(f"è®­ç»ƒé›†per_quèŒƒå›´: [{y_reg_train_values[:, 0].min():.4f}, {y_reg_train_values[:, 0].max():.4f}]")
    print(f"éªŒè¯é›†per_muèŒƒå›´: [{y_reg_val_values[:, 1].min():.4f}, {y_reg_val_values[:, 1].max():.4f}]")
    print(f"éªŒè¯é›†per_quèŒƒå›´: [{y_reg_val_values[:, 0].min():.4f}, {y_reg_val_values[:, 0].max():.4f}]")

    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"è®­ç»ƒé›†åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_cls_train, return_counts=True)}")
    print(f"éªŒè¯é›†åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ: {np.unique(y_cls_val, return_counts=True)}")

    # æ£€æŸ¥ç‰¹å¾åˆ†å¸ƒ
    if hasattr(X_train, 'values'):
        x_train_values = X_train.values
    else:
        x_train_values = X_train

    print(f"è®­ç»ƒé›†ç‰¹å¾ç»Ÿè®¡: å‡å€¼={np.mean(x_train_values):.6f}, æ ‡å‡†å·®={np.std(x_train_values):.6f}")
    print(f"è®­ç»ƒé›†ç‰¹å¾èŒƒå›´: [{np.min(x_train_values):.6f}, {np.max(x_train_values):.6f}]")

    print("âœ… æ•°æ®è´¨é‡éªŒè¯å®Œæˆ")


class R2EarlyStoppingAndSave(tf.keras.callbacks.Callback):
    """æ”¹è¿›çš„RÂ²æ—©åœå›è°ƒï¼Œå¢å¼ºé˜²è¿‡æ‹Ÿåˆèƒ½åŠ›"""

    def __init__(self, X_val, y_val, min_r2=0.05, patience=15, save_path=None,  # é™ä½RÂ²é˜ˆå€¼ï¼Œå¢åŠ patience
                 min_delta=0.0001, restore_best_weights=True, monitor='val_loss'):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.min_r2 = min_r2
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.monitor = monitor
        self.save_path = save_path
        self.wait = 0
        self.best_r2 = -np.inf
        self.best_weights = None
        self.best_epoch = 0
        self.stopped_epoch = 0
        self.model_saved = False
        self.bad_epochs = 0  # æ·»åŠ ç¼ºå¤±çš„åˆå§‹åŒ–

    def on_epoch_end(self, epoch, logs=None):
        # è·å–æ¨¡å‹é¢„æµ‹ç»“æœ
        predictions = self.model.predict(self.X_val, verbose=0)
        y_pred = predictions[1]  # å›å½’è¾“å‡º
        y_true = self.y_val
        if hasattr(y_true, 'values'):
            y_true = y_true.values

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"Epoch {epoch + 1} - é¢„æµ‹å€¼èŒƒå›´: min={np.min(y_pred):.6f}, max={np.max(y_pred):.6f}")
        print(f"Epoch {epoch + 1} - çœŸå®å€¼èŒƒå›´: min={np.min(y_true):.6f}, max={np.max(y_true):.6f}")
        print(f"Epoch {epoch + 1} - é¢„æµ‹å€¼å½¢çŠ¶: {y_pred.shape}, çœŸå®å€¼å½¢çŠ¶: {y_true.shape}")

        # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰é—®é¢˜
        if has_invalid(y_pred) or has_invalid(y_true):
            print(f"[è­¦å‘Š] epoch {epoch + 1} æ£€æµ‹åˆ°æ— æ•ˆæ•°å€¼ï¼ˆNaN/Infï¼‰ï¼Œæå‰ç»ˆæ­¢æœ¬trial")
            self.model.stop_training = True
            return

        # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦éƒ½æ˜¯åŒä¸€ä¸ªå€¼ï¼ˆæ¨¡å‹æ²¡æœ‰å­¦ä¹ ï¼‰
        if np.std(y_pred) < 1e-6:
            print(f"[è­¦å‘Š] epoch {epoch + 1} é¢„æµ‹å€¼å‡ ä¹æ²¡æœ‰å˜åŒ–ï¼ˆstd={np.std(y_pred):.8f}ï¼‰ï¼Œæ¨¡å‹å¯èƒ½æ²¡æœ‰å­¦ä¹ ")

        # æ³¨æ„ï¼šy_predå’Œy_trueéƒ½å·²ç»æ˜¯log1på˜æ¢åçš„å€¼ï¼Œä¸éœ€è¦å†æ¬¡å˜æ¢
        # ç›´æ¥è®¡ç®—RÂ²åˆ†æ•°

        # è®¡ç®—R2åˆ†æ•° - åªå…³æ³¨per_muï¼ˆç¬¬äºŒä¸ªè¾“å‡ºï¼‰
        try:
            # ç¡®ä¿y_predæ˜¯numpyæ•°ç»„
            if hasattr(y_pred, 'values'):
                y_pred = y_pred.values
            elif not isinstance(y_pred, np.ndarray):
                y_pred = np.array(y_pred)

            # ç¡®ä¿y_trueæ˜¯numpyæ•°ç»„
            if hasattr(y_true, 'values'):
                y_true = y_true.values
            elif not isinstance(y_true, np.ndarray):
                y_true = np.array(y_true)

            # åªè®¡ç®—per_muçš„RÂ²å€¼ï¼ˆç¬¬ä¸€åˆ—ï¼‰
            if y_true.ndim > 1 and y_true.shape[1] > 0:
                y_true_per_mu = y_true[:, 0]  # per_muæ˜¯ç¬¬ä¸€åˆ—
            else:
                y_true_per_mu = y_true.flatten()

            if y_pred.ndim > 1 and y_pred.shape[1] > 0:
                y_pred_per_mu = y_pred[:, 0]  # per_muæ˜¯ç¬¬ä¸€åˆ—
            else:
                y_pred_per_mu = y_pred.flatten()

            # ç¡®ä¿é•¿åº¦åŒ¹é…
            min_len = min(len(y_true_per_mu), len(y_pred_per_mu))
            y_true_per_mu = y_true_per_mu[:min_len]
            y_pred_per_mu = y_pred_per_mu[:min_len]

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if len(y_true_per_mu) == 0 or len(y_pred_per_mu) == 0:
                print(f"[è­¦å‘Š] epoch {epoch + 1} æ•°æ®ä¸ºç©ºï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            elif np.std(y_true_per_mu) < 1e-8:
                print(f"[è­¦å‘Š] epoch {epoch + 1} çœŸå®å€¼æ–¹å·®ä¸º0ï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            elif np.std(y_pred_per_mu) < 1e-8:
                print(f"[è­¦å‘Š] epoch {epoch + 1} é¢„æµ‹å€¼æ–¹å·®ä¸º0ï¼Œè®¾ç½®RÂ²ä¸º-1000")
                r2 = -1000
            else:
                r2 = r2_score(y_true_per_mu, y_pred_per_mu)
                # é™åˆ¶RÂ²åœ¨åˆç†èŒƒå›´å†…
                r2 = max(-100, min(1, r2))
                print(f"Epoch {epoch + 1} - per_mu RÂ²: {r2:.4f}")
        except Exception as e:
            print(f"[é”™è¯¯] epoch {epoch + 1} è®¡ç®—R2åˆ†æ•°å¤±è´¥: {e}")
            r2 = -1000  # è®¾ç½®ä¸€ä¸ªå¾ˆä½çš„R2åˆ†æ•°

        logs = logs or {}
        logs['val_r2'] = r2

        # å¦‚æœRÂ²å¤§äº0.1ä¸”æ˜¯å½“å‰æœ€ä½³ï¼Œä¿å­˜æ¨¡å‹
        if r2 > self.min_r2 and r2 > self.best_r2 and not self.model_saved:
            self.best_r2 = r2
            if self.save_path:
                save_model_with_custom_objects(self.model, self.save_path)
                self.model_saved = True
                print(f"âœ… Epoch {epoch + 1}: RÂ²={r2:.4f} >= {self.min_r2}, æ¨¡å‹å·²ä¿å­˜!")
            self.bad_epochs = 0
        elif r2 > self.best_r2:
            self.best_r2 = r2
            self.bad_epochs = 0
        else:
            self.bad_epochs += 1

        if r2 < self.min_r2:
            print(f"Epoch {epoch + 1}: val_r2={r2:.4f} < {self.min_r2}, bad_epochs={self.bad_epochs}")
        else:
            print(f"Epoch {epoch + 1}: val_r2={r2:.4f} >= {self.min_r2}")

        if self.bad_epochs >= self.patience:
            print(f"Early stopping: val_r2ä½äº{self.min_r2}å·²è¿ç»­{self.patience}ä¸ªepoch")
            self.model.stop_training = True


def train_final_model(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val, strategy=None, skip_optuna=False,
                      best_params=None):
    """è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ"""

    # å¦‚æœè·³è¿‡Optunaä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨æä¾›çš„å‚æ•°
    if skip_optuna and best_params is not None:
        print(f"\nğŸ¯ è·³è¿‡Optunaä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨RÂ²è¾¾åˆ°0.8æ—¶çš„æœ€ä½³å‚æ•°...")
        print(f"ä½¿ç”¨å‚æ•°: {best_params}")

        # æ„å»ºæœ€ç»ˆæ¨¡å‹
        num_classes = len(np.unique(y_cls_train))
        best_model = build_hybrid_model(
            input_dim=X_train.shape[1],
            num_classes=num_classes,
            params=best_params,
            strategy=strategy
        )

        # åˆ›å»ºmonitorå¯¹è±¡ï¼ˆå¦‚æœæœªå®šä¹‰ï¼‰
        if 'monitor' not in locals():
            monitor = TrainingMonitor(Config.logs_dir)

        # å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœæœªå®šä¹‰ï¼‰
        def create_lr_scheduler(initial_lr):
            """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""

            def lr_scheduler(epoch, lr):
                if epoch < 5:
                    return lr
                elif epoch < 15:
                    return lr * 0.8
                elif epoch < 25:
                    return lr * 0.6
                elif epoch < 35:
                    return lr * 0.4
                else:
                    return lr * 0.2

            return lr_scheduler

        # è®¾ç½®æœ€ç»ˆè®­ç»ƒçš„å›è°ƒå‡½æ•°
        final_callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=Config.dl_params['early_stop_patience'],
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                Config.final_model_file,
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            tf.keras.callbacks.LearningRateScheduler(
                create_lr_scheduler(best_params['lr'])
            ),
            TensorBoard(
                log_dir=Config.logs_dir,
                histogram_freq=1
            ),
            tf.keras.callbacks.LambdaCallback(
                on_epoch_end=lambda epoch, logs: monitor.update_metrics(epoch, logs)
            ),
            R2EarlyStoppingAndSave(
                X_val, y_reg_val, min_r2=0.1, patience=5, save_path=Config.final_model_file
            )
        ]

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        history = best_model.fit(
            X_train,
            {
                'classification': y_cls_train,
                'regression': y_reg_train
            },
            validation_data=(
                X_val,
                {
                    'classification': y_cls_val,
                    'regression': y_reg_val
                }
            ),
            epochs=Config.dl_params['epochs'],
            batch_size=best_params['batch_size'],
            callbacks=final_callbacks,
            verbose=1
        )

        # ä¿å­˜è®­ç»ƒå†å²
        history_file = os.path.join(Config.logs_dir, 'final_model_history.json')
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump({
                'params': best_params,
                'history': history.history,
                'r2_achieved': True,
                'skip_optuna': True
            }, f, indent=4, ensure_ascii=False, default=json_fallback)

        return best_model, history

    print("\nå¼€å§‹Optunaè¶…å‚æ•°ä¼˜åŒ–...")

    # æ·»åŠ è¾“å…¥æ•°æ®éªŒè¯
    print("è¾“å…¥æ•°æ®éªŒè¯:")
    print(
        f"X_train shape: {X_train.shape}, dtype: {X_train.dtypes.iloc[0] if hasattr(X_train, 'dtypes') else 'unknown'}")
    print(f"X_val shape: {X_val.shape}, dtype: {X_val.dtypes.iloc[0] if hasattr(X_val, 'dtypes') else 'unknown'}")
    print(f"y_reg_train shape: {y_reg_train.shape if hasattr(y_reg_train, 'shape') else len(y_reg_train)}")
    print(f"y_reg_val shape: {y_reg_val.shape if hasattr(y_reg_val, 'shape') else len(y_reg_val)}")

    # æ·»åŠ ç›®æ ‡å˜é‡ç»Ÿè®¡ä¿¡æ¯
    print(f"è®­ç»ƒé›†ç›®æ ‡å˜é‡ç»Ÿè®¡:")
    if hasattr(y_reg_train, 'describe'):
        print(y_reg_train.describe())
    else:
        # å®‰å…¨åœ°å¤„ç†pandas DataFrameå’Œnumpy array
        if hasattr(y_reg_train, 'iloc'):
            y_reg_train_array = y_reg_train.values
        else:
            y_reg_train_array = np.array(y_reg_train)
        print(
            f"per_mu: min={np.min(y_reg_train_array[:, 0]):.4f}, max={np.max(y_reg_train_array[:, 0]):.4f}, mean={np.mean(y_reg_train_array[:, 0]):.4f}")
        print(
            f"per_qu: min={np.min(y_reg_train_array[:, 1]):.4f}, max={np.max(y_reg_train_array[:, 1]):.4f}, mean={np.mean(y_reg_train_array[:, 1]):.4f}")

    print(f"éªŒè¯é›†ç›®æ ‡å˜é‡ç»Ÿè®¡:")
    if hasattr(y_reg_val, 'describe'):
        print(y_reg_val.describe())
    else:
        # å®‰å…¨åœ°å¤„ç†pandas DataFrameå’Œnumpy array
        if hasattr(y_reg_val, 'iloc'):
            y_reg_val_array = y_reg_val.values
        else:
            y_reg_val_array = np.array(y_reg_val)
        print(
            f"per_mu: min={np.min(y_reg_val_array[:, 0]):.4f}, max={np.max(y_reg_val_array[:, 0]):.4f}, mean={np.mean(y_reg_val_array[:, 0]):.4f}")
        print(
            f"per_qu: min={np.min(y_reg_val_array[:, 1]):.4f}, max={np.max(y_reg_val_array[:, 1]):.4f}, mean={np.mean(y_reg_val_array[:, 1]):.4f}")

    # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰NaNæˆ–Inf
    if hasattr(X_train, 'isnull'):
        train_nan_count = X_train.isnull().sum().sum()
        val_nan_count = X_val.isnull().sum().sum()
        print(f"NaNæ£€æŸ¥ - è®­ç»ƒé›†: {train_nan_count}, éªŒè¯é›†: {val_nan_count}")

        if train_nan_count > 0 or val_nan_count > 0:
            print("å‘ç°NaNå€¼ï¼Œè¿›è¡Œæ¸…ç†...")
            X_train = X_train.fillna(0)
            X_val = X_val.fillna(0)

    # æ£€æŸ¥æ ‡ç­¾æ•°æ®
    if hasattr(y_reg_train, 'shape'):
        train_label_nan = np.isnan(y_reg_train).sum()
        val_label_nan = np.isnan(y_reg_val).sum()
        print(f"æ ‡ç­¾NaNæ£€æŸ¥ - è®­ç»ƒ: {train_label_nan}, éªŒè¯: {val_label_nan}")

        # ç¡®ä¿æ¯”è¾ƒçš„æ˜¯æ ‡é‡å€¼
        train_nan_count = train_label_nan.sum() if hasattr(train_label_nan, 'sum') else train_label_nan
        val_nan_count = val_label_nan.sum() if hasattr(val_label_nan, 'sum') else val_label_nan

        if train_nan_count > 0 or val_nan_count > 0:
            print("æ¸…ç†æ ‡ç­¾NaNå€¼...")
            y_reg_train = np.nan_to_num(y_reg_train, nan=0.0)
            y_reg_val = np.nan_to_num(y_reg_val, nan=0.0)

    # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ‰é™çš„æ•°å€¼
    print("æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")

    # æ£€æŸ¥ç‰¹å¾æ•°æ®
    if hasattr(X_train, 'values'):
        X_train_values = X_train.values
    else:
        X_train_values = X_train

    if hasattr(X_val, 'values'):
        X_val_values = X_val.values
    else:
        X_val_values = X_val

    # æ£€æŸ¥å¹¶æ¸…ç†ç‰¹å¾ä¸­çš„NaNå’ŒInf
    if np.isnan(X_train_values).any() or np.isinf(X_train_values).any():
        print("æ¸…ç†è®­ç»ƒç‰¹å¾ä¸­çš„NaN/Inf...")
        X_train_values = np.nan_to_num(X_train_values, nan=0.0, posinf=0.0, neginf=0.0)
        if hasattr(X_train, 'values'):
            X_train = pd.DataFrame(X_train_values, columns=X_train.columns, index=X_train.index)
        else:
            X_train = X_train_values

    if np.isnan(X_val_values).any() or np.isinf(X_val_values).any():
        print("æ¸…ç†éªŒè¯ç‰¹å¾ä¸­çš„NaN/Inf...")
        X_val_values = np.nan_to_num(X_val_values, nan=0.0, posinf=0.0, neginf=0.0)
        if hasattr(X_val, 'values'):
            X_val = pd.DataFrame(X_val_values, columns=X_val.columns, index=X_val.index)
        else:
            X_val = X_val_values

    # æ£€æŸ¥å¹¶æ¸…ç†æ ‡ç­¾ä¸­çš„NaNå’ŒInf
    if hasattr(y_reg_train, 'shape'):
        train_nan_count = np.isnan(y_reg_train).sum()
        train_inf_count = np.isinf(y_reg_train).sum()
        # ç¡®ä¿æ¯”è¾ƒçš„æ˜¯æ ‡é‡å€¼
        if hasattr(train_nan_count, 'sum'):
            train_nan_count = train_nan_count.sum()
        if hasattr(train_inf_count, 'sum'):
            train_inf_count = train_inf_count.sum()

        if train_nan_count > 0 or train_inf_count > 0:
            print(f"æ¸…ç†è®­ç»ƒæ ‡ç­¾ä¸­çš„NaN({train_nan_count})/Inf({train_inf_count})...")
            y_reg_train = np.nan_to_num(y_reg_train, nan=0.0, posinf=0.0, neginf=0.0)

    if hasattr(y_reg_val, 'shape'):
        val_nan_count = np.isnan(y_reg_val).sum()
        val_inf_count = np.isinf(y_reg_val).sum()
        # ç¡®ä¿æ¯”è¾ƒçš„æ˜¯æ ‡é‡å€¼
        if hasattr(val_nan_count, 'sum'):
            val_nan_count = val_nan_count.sum()
        if hasattr(val_inf_count, 'sum'):
            val_inf_count = val_inf_count.sum()

        if val_nan_count > 0 or val_inf_count > 0:
            print(f"æ¸…ç†éªŒè¯æ ‡ç­¾ä¸­çš„NaN({val_nan_count})/Inf({val_inf_count})...")
            y_reg_val = np.nan_to_num(y_reg_val, nan=0.0, posinf=0.0, neginf=0.0)

    print("æ•°æ®æ¸…ç†å®Œæˆï¼Œå¼€å§‹Optunaä¼˜åŒ–...")

    # åˆ›å»ºè®­ç»ƒç›‘æ§å™¨
    monitor = TrainingMonitor(Config.logs_dir)

    # å®šä¹‰ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆç§»åˆ°å¤–éƒ¨é¿å…é—­åŒ…ä½œç”¨åŸŸé—®é¢˜ï¼‰
    def create_objective(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val, strategy, monitor):
        def objective(trial):
            # ä»è¶…å‚æ•°ç©ºé—´ä¸­é‡‡æ ·ï¼ˆä¸Config.optuna_paramsä¸€è‡´ï¼Œåªå…è®¸'adam'å’Œ'relu'ï¼‰
            params = {
                'lr': trial.suggest_float('lr', *Config.optuna_params['param_ranges']['lr'], log=True),
                'neurons1': trial.suggest_int('neurons1', *Config.optuna_params['param_ranges']['neurons1']),
                'neurons2': trial.suggest_int('neurons2', *Config.optuna_params['param_ranges']['neurons2']),
                'dropout_rate': trial.suggest_float('dropout_rate',
                                                    *Config.optuna_params['param_ranges']['dropout_rate']),
                'batch_size': trial.suggest_categorical(
                    'batch_size',
                    Config.optuna_params['param_ranges']['batch_size']
                ),
                'attention_units': trial.suggest_int(
                    'attention_units',
                    *Config.optuna_params['param_ranges']['attention_units']
                ),
                'l1_lambda': trial.suggest_float(
                    'l1_lambda',
                    *Config.optuna_params['param_ranges']['l1_lambda'],
                    log=True),
                'l2_lambda': trial.suggest_float(
                    'l2_lambda',
                    *Config.optuna_params['param_ranges']['l2_lambda'],
                    log=True),
                'optimizer_type': trial.suggest_categorical(
                    'optimizer_type',
                    Config.optuna_params['param_ranges']['optimizer_type']
                ),
                'activation': trial.suggest_categorical(
                    'activation',
                    Config.optuna_params['param_ranges']['activation']
                )
            }

            # æ„å»ºæ¨¡å‹
            num_classes = len(np.unique(y_cls_train))
            model = build_hybrid_model(X_train.shape[1], num_classes, params, strategy)

            # è®¾ç½®æ”¹è¿›çš„å›è°ƒå‡½æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=Config.dl_params['early_stop_patience'],
                    restore_best_weights=True,
                    min_delta=Config.dl_params['min_delta'],
                    verbose=1
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.3,  # æ›´æ¿€è¿›çš„å­¦ä¹ ç‡è¡°å‡
                    patience=2,  # è¿›ä¸€æ­¥ç¼©çŸ­è€å¿ƒï¼ŒåŠ å¿«è®­ç»ƒ
                    min_lr=1e-7,  # æ›´å°çš„æœ€å°å­¦ä¹ ç‡
                    verbose=1,
                    mode='min'
                ),
                ModelCheckpoint(
                    os.path.join(Config.model_dir, f'model_trial_{trial.number}.h5'),
                    monitor='val_loss',
                    save_best_only=True,
                    verbose=0
                ),
                tf.keras.callbacks.LambdaCallback(
                    on_epoch_end=lambda epoch, logs: monitor.update_metrics(epoch, logs)
                ),
                # æ·»åŠ RÂ²æ—©åœå›è°ƒï¼Œå½“RÂ²è¾¾åˆ°0.8æ—¶åœæ­¢è°ƒå‚
                OptunaR2EarlyStopping(
                    X_val, y_reg_val, min_r2=0.8, trial=trial
                )
            ]

            # è®­ç»ƒæ¨¡å‹
            try:
                history = model.fit(
                    X_train,
                    {
                        'classification': y_cls_train,
                        'regression': y_reg_train
                    },
                    validation_data=(
                        X_val,
                        {
                            'classification': y_cls_val,
                            'regression': y_reg_val
                        }
                    ),
                    epochs=Config.dl_params['epochs'],
                    batch_size=params['batch_size'],
                    callbacks=callbacks,
                    verbose=0
                )

                # è®¡ç®—éªŒè¯é›†æ€§èƒ½
                val_loss = min(history.history['val_loss'])

                # æŠ¥å‘Šä¸­é—´å€¼
                trial.report(val_loss, step=Config.dl_params['epochs'])

                # å¤„ç†æå‰åœæ­¢
                if trial.should_prune():
                    raise optuna.TrialPruned()

                return val_loss

            except optuna.TrialPruned as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºRÂ²è¾¾åˆ°0.8è€Œåœæ­¢çš„
                if hasattr(trial, 'user_attrs') and trial.user_attrs.get('r2_achieved', 0) >= 0.8:
                    print(f"ğŸ¯ RÂ²è¾¾åˆ°{trial.user_attrs.get('r2_achieved', 0):.4f}ï¼Œåœæ­¢è°ƒå‚è®­ç»ƒï¼")
                    # è¿”å›ä¸€ä¸ªå¾ˆå°çš„æŸå¤±å€¼ï¼Œè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªæˆåŠŸçš„è¯•éªŒ
                    return 0.001
                else:
                    # æ­£å¸¸çš„å‰ªæ
                    raise e

        return objective

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    def create_lr_scheduler(initial_lr):
        """åˆ›å»ºæ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ"""

        # ä½¿ç”¨ReduceLROnPlateauå›è°ƒï¼Œæ›´æ™ºèƒ½çš„å­¦ä¹ ç‡è°ƒæ•´
        def lr_scheduler(epoch, lr):
            # æ›´ä¸¥æ ¼çš„å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
            if epoch < 5:
                return lr
            elif epoch < 15:
                return lr * 0.8
            elif epoch < 25:
                return lr * 0.6
            elif epoch < 35:
                return lr * 0.4
            else:
                return lr * 0.2

        return lr_scheduler

    # åˆ›å»ºå’Œè¿è¡ŒOptunaç ”ç©¶
    study = optuna.create_study(
        direction='minimize',
        pruner=optuna.pruners.MedianPruner(
            n_startup_trials=5,
            n_warmup_steps=5,
            interval_steps=1
        )
    )

    # åˆ›å»ºobjectiveå‡½æ•°å¹¶ä¼ é€’ç»™study.optimize
    objective_func = create_objective(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val, strategy, monitor)

    # æ·»åŠ RÂ²è¾¾åˆ°0.8æ—¶çš„æ—©æœŸåœæ­¢æœºåˆ¶
    r2_achieved = False
    best_r2 = 0.0
    global_best_params = None  # å­˜å‚¨å…¨å±€æœ€ä½³å‚æ•°

    def optimize_with_r2_stop():
        nonlocal r2_achieved, best_r2

        for trial in study.trials:
            if r2_achieved:
                print(f"ğŸ¯ RÂ²å·²è¾¾åˆ°{best_r2:.4f}ï¼Œåœæ­¢è°ƒå‚è®­ç»ƒï¼")
                break

            try:
                result = objective_func(trial)

                # æ£€æŸ¥trialæ˜¯å¦å› ä¸ºRÂ²è¾¾åˆ°0.8è€Œåœæ­¢
                if hasattr(trial, 'user_attrs') and trial.user_attrs.get('r2_achieved', 0) >= 0.8:
                    r2_achieved = True
                    best_r2 = trial.user_attrs.get('r2_achieved', 0)
                    print(f"ğŸ¯ è¯•éªŒ {trial.number}: RÂ²è¾¾åˆ°{best_r2:.4f}ï¼Œåœæ­¢è°ƒå‚è®­ç»ƒï¼")
                    break

            except optuna.TrialPruned:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºRÂ²è¾¾åˆ°0.8è€Œå‰ªæ
                if hasattr(trial, 'user_attrs') and trial.user_attrs.get('r2_achieved', 0) >= 0.8:
                    r2_achieved = True
                    best_r2 = trial.user_attrs.get('r2_achieved', 0)
                    print(f"ğŸ¯ è¯•éªŒ {trial.number}: RÂ²è¾¾åˆ°{best_r2:.4f}ï¼Œåœæ­¢è°ƒå‚è®­ç»ƒï¼")
                    break
                continue
            except Exception as e:
                print(f"è¯•éªŒ {trial.number} å¤±è´¥: {e}")
                continue

    # è¿è¡Œä¼˜åŒ–
    if not r2_achieved:
        study.optimize(
            objective_func,
            n_trials=Config.optuna_params['n_trials'],
            timeout=Config.optuna_params['timeout']
        )

    print("\næœ€ä½³è¶…å‚æ•°:", study.best_trial.params)

    # åˆå§‹åŒ–å˜é‡
    global_best_params = study.best_trial.params  # é»˜è®¤ä½¿ç”¨Optunaæœ€ä½³å‚æ•°
    r2_achieved = False
    best_r2 = 0

    # æ£€æŸ¥æ˜¯å¦æœ‰è¯•éªŒè¾¾åˆ°RÂ²=0.8
    for trial in study.trials:
        if hasattr(trial, 'user_attrs') and trial.user_attrs.get('r2_achieved', 0) >= 0.8:
            print(f"ğŸ¯ å‘ç°RÂ²è¾¾åˆ°0.8çš„è¯•éªŒ: è¯•éªŒ{trial.number}, RÂ²={trial.user_attrs.get('r2_achieved', 0):.4f}")
            # ä½¿ç”¨è¿™ä¸ªè¯•éªŒçš„å‚æ•°ä½œä¸ºæœ€ä½³å‚æ•°
            # study.best_trial æ˜¯åªè¯»å±æ€§ï¼Œä¸èƒ½ç›´æ¥è®¾ç½®
            global_best_params = trial.params  # ä¿å­˜å…¨å±€æœ€ä½³å‚æ•°
            r2_achieved = True
            best_r2 = trial.user_attrs.get('r2_achieved', 0)
            break

    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\nä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    num_classes = len(np.unique(y_cls_train))

    # æ„å»ºæœ€ç»ˆæ¨¡å‹
    # ä½¿ç”¨å…¨å±€æœ€ä½³å‚æ•°ï¼ˆå¦‚æœRÂ²è¾¾åˆ°0.8ï¼‰æˆ–Optunaæœ€ä½³å‚æ•°
    final_params = global_best_params if r2_achieved else study.best_trial.params
    print(f"ä½¿ç”¨å‚æ•°: {'å…¨å±€æœ€ä½³å‚æ•°' if r2_achieved else 'Optunaæœ€ä½³å‚æ•°'}")

    best_model = build_hybrid_model(
        input_dim=X_train.shape[1],
        num_classes=num_classes,
        params=final_params,
        strategy=strategy
    )

    # è®¾ç½®æœ€ç»ˆè®­ç»ƒçš„å›è°ƒå‡½æ•°
    final_callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=Config.dl_params['early_stop_patience'],
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            Config.final_model_file,
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        tf.keras.callbacks.LearningRateScheduler(
            create_lr_scheduler(final_params['lr'])
        ),
        TensorBoard(
            log_dir=Config.logs_dir,
            histogram_freq=1
        ),
        # ç§»é™¤monitorå›è°ƒï¼Œå› ä¸ºmonitorå˜é‡æœªå®šä¹‰
        # tf.keras.callbacks.LambdaCallback(
        #     on_epoch_end=lambda epoch, logs: monitor.update_metrics(epoch, logs)
        # ),
        R2EarlyStoppingAndSave(
            X_val, y_reg_val, min_r2=0.1, patience=3, save_path=Config.final_model_file  # å‡å°‘patienceï¼ŒåŠ å¿«æ—©åœ
        )
    ]

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    history = best_model.fit(
        X_train,
        {
            'classification': y_cls_train,
            'regression': y_reg_train
        },
        validation_data=(
            X_val,
            {
                'classification': y_cls_val,
                'regression': y_reg_val
            }
        ),
        epochs=Config.dl_params['epochs'],
        batch_size=study.best_trial.params['batch_size'],
        callbacks=final_callbacks,
        verbose=1
    )

    # ä¿å­˜è®­ç»ƒå†å²
    history_file = os.path.join(Config.logs_dir, 'final_model_history.json')
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump({
            'params': study.best_trial.params,
            'history': history.history,
            'study_trials': [
                {
                    'number': trial.number,
                    'params': trial.params,
                    'value': trial.value
                }
                for trial in study.trials
            ]
        }, f, indent=4, ensure_ascii=False, default=json_fallback)

    return best_model, history


def cross_validate_and_ensemble(X, y_cls, y_reg, n_splits=10, n_models=5, strategy=None, global_best_params=None,
                                r2_achieved=False):
    """å®ç°äº¤å‰éªŒè¯å’Œæ¨¡å‹é›†æˆï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ

    Args:
        X: ç‰¹å¾æ•°æ®
        y_cls: åˆ†ç±»æ ‡ç­¾
        y_reg: å›å½’æ ‡ç­¾
        n_splits: äº¤å‰éªŒè¯æŠ˜æ•°
        n_models: é›†æˆæ¨¡å‹æ•°é‡
        strategy: å¤šGPUè®­ç»ƒç­–ç•¥
        xgb_max_rows: XGBoosté‡‡æ ·é‡

    Returns:
        ensemble_models: é›†æˆæ¨¡å‹åˆ—è¡¨
        cv_scores: äº¤å‰éªŒè¯åˆ†æ•°
    """
    from sklearn.model_selection import StratifiedKFold
    cv_scores = {
        'cls_acc': [],
        'reg_r2': [],
        'reg_mae': []
    }
    ensemble_models = []
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=Config.random_seed)
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y_cls), 1):
        print(f"\nè®­ç»ƒç¬¬ {fold}/{n_splits} æŠ˜...")
        # åˆ†å‰²æ•°æ®
        X_train_fold = X.iloc[train_idx]
        X_val_fold = X.iloc[val_idx]
        y_cls_train_fold = y_cls[train_idx]
        y_cls_val_fold = y_cls[val_idx]
        y_reg_train_fold = y_reg.iloc[train_idx]
        y_reg_val_fold = y_reg.iloc[val_idx]
        fold_models = []
        for model_idx in range(n_models):
            print(f"\nè®­ç»ƒæ¨¡å‹ {model_idx + 1}/{n_models}...")
            # XGBoostç‰¹å¾æå–ï¼ˆä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
            X_train_combined, X_val_combined, xgb_model, y_cls_train_sample, y_cls_val_sample, y_reg_train_sample, y_reg_val_sample = extract_xgboost_features(
                X_train_fold, X_val_fold, y_cls_train_fold, y_cls_val_fold, y_reg_train_fold, y_reg_val_fold
            )
            # è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆåªç”¨é‡‡æ ·æ•°æ®ï¼‰
            # å¦‚æœRÂ²å·²è¾¾åˆ°0.8ï¼Œè·³è¿‡Optunaä¼˜åŒ–ï¼Œç›´æ¥ä½¿ç”¨æœ€ä½³å‚æ•°
            if r2_achieved and global_best_params is not None:
                print(f"ğŸ¯ äº¤å‰éªŒè¯ç¬¬{fold}æŠ˜: ä½¿ç”¨RÂ²è¾¾åˆ°0.8æ—¶çš„æœ€ä½³å‚æ•°ï¼Œè·³è¿‡è°ƒå‚")
                model, history = train_final_model(
                    X_train_combined, y_cls_train_sample, y_reg_train_sample,
                    X_val_combined, y_cls_val_sample, y_reg_val_sample,
                    strategy=strategy, skip_optuna=True, best_params=global_best_params
                )
            else:
                model, history = train_final_model(
                    X_train_combined, y_cls_train_sample, y_reg_train_sample,
                    X_val_combined, y_cls_val_sample, y_reg_val_sample,
                    strategy=strategy
                )
            fold_models.append({
                'xgb_model': xgb_model,
                'dl_model': model
            })
        # éªŒè¯é›†è¯„ä¼°ï¼ˆåªç”¨é‡‡æ ·æ•°æ®ï¼‰
        val_preds_cls = []
        val_preds_reg = []
        for model_dict in fold_models:
            cls_pred, reg_pred = model_dict['dl_model'].predict(X_val_combined)
            val_preds_cls.append(cls_pred)
            val_preds_reg.append(reg_pred)
        ensemble_cls = np.mean(val_preds_cls, axis=0)
        ensemble_reg = np.mean(val_preds_reg, axis=0)
        cls_acc = accuracy_score(y_cls_val_sample, np.argmax(ensemble_cls, axis=1))
        # åªè®¡ç®—per_muçš„RÂ²å€¼
        # ç¡®ä¿y_reg_val_sampleæ˜¯numpyæ•°ç»„
        if hasattr(y_reg_val_sample, 'values'):
            y_reg_val_sample = y_reg_val_sample.values
        reg_r2 = r2_score(y_reg_val_sample[:, 0], ensemble_reg[:, 0])  # åªä½¿ç”¨per_mu
        reg_mae = mean_absolute_error(y_reg_val_sample, ensemble_reg)
        cv_scores['cls_acc'].append(cls_acc)
        cv_scores['reg_r2'].append(reg_r2)
        cv_scores['reg_mae'].append(reg_mae)
        ensemble_models.append(fold_models)
        print(f"\nç¬¬ {fold} æŠ˜ç»“æœ:")
        print(f"åˆ†ç±»å‡†ç¡®ç‡: {cls_acc:.4f}")
        print(f"per_mu R2 åˆ†æ•°: {reg_r2:.4f}")
        print(f"å›å½’ MAE: {reg_mae:.4f}")
    print("\näº¤å‰éªŒè¯å¹³å‡åˆ†æ•°:")
    print(f"åˆ†ç±»å‡†ç¡®ç‡: {np.mean(cv_scores['cls_acc']):.4f} Â± {np.std(cv_scores['cls_acc']):.4f}")
    print(f"per_mu R2 åˆ†æ•°: {np.mean(cv_scores['reg_r2']):.4f} Â± {np.std(cv_scores['reg_r2']):.4f}")
    print(f"å›å½’ MAE: {np.mean(cv_scores['reg_mae']):.4f} Â± {np.std(cv_scores['reg_mae']):.4f}")
    print("æ³¨æ„ï¼šRÂ²å€¼åªè®¡ç®—per_muï¼Œper_quæƒé‡ä¸º0")
    return ensemble_models, cv_scores


def augment_data(
        X: Union[pd.DataFrame, np.ndarray],
        y_cls: Union[pd.Series, np.ndarray],
        y_reg: Union[pd.DataFrame, np.ndarray],
        augmentation_factor: float = 1.0
) -> tuple:
    assert isinstance(X, (pd.DataFrame, np.ndarray)), f"X must be DataFrame or ndarray, got {type(X)}"
    assert isinstance(y_cls, (pd.Series, np.ndarray)), f"y_cls must be Series or ndarray, got {type(y_cls)}"
    assert isinstance(y_reg, (pd.DataFrame, np.ndarray)), f"y_reg must be DataFrame or ndarray, got {type(y_reg)}"
    assert 0 < augmentation_factor < 1, "augmentation_factor must be between 0 and 1"

    print("\næ‰§è¡Œæ•°æ®å¢å¼º...")
    X_aug = X.copy() if hasattr(X, 'copy') else np.copy(X)
    y_cls_aug = y_cls.copy() if hasattr(y_cls, 'copy') else np.copy(y_cls)
    y_reg_aug = y_reg.copy() if hasattr(y_reg, 'copy') else np.copy(y_reg)

    n_samples = len(X)
    n_augment = int(n_samples * augmentation_factor)

    # 1. é«˜æ–¯å™ªå£°æ‰°åŠ¨ï¼ˆå¢å¼ºç‰ˆï¼‰
    noise_samples = []
    noise_cls = []
    noise_reg = []

    for i in range(n_augment):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        idx = random.randint(0, n_samples - 1)
        sample = safe_slice(X, idx)
        sample = sample.copy() if hasattr(sample, 'copy') else np.copy(sample)
        # æ·»åŠ å¢å¼ºçš„é«˜æ–¯å™ªå£°
        noise_std = Config.augmentation_params.get('noise_factor', 0.02)
        noise = np.random.normal(0, noise_std, size=len(sample))
        noisy_sample = sample + noise
        noise_samples.append(noisy_sample)
        noise_cls.append(safe_slice(y_cls, idx))
        noise_reg.append(safe_slice(y_reg, idx))

    # 2. Mixupæ•°æ®å¢å¼º
    mixup_samples = []
    mixup_cls = []
    mixup_reg = []

    mixup_alpha = Config.augmentation_params.get('mixup_alpha', 0.2)
    for i in range(n_augment):
        # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬
        idx1, idx2 = random.sample(range(n_samples), 2)
        # ç”ŸæˆBetaåˆ†å¸ƒçš„æ··åˆæƒé‡
        alpha = np.random.beta(mixup_alpha, mixup_alpha)
        s1 = safe_slice(X, idx1)
        s2 = safe_slice(X, idx2)
        mixed_sample = alpha * s1 + (1 - alpha) * s2
        r1 = safe_slice(y_reg, idx1)
        r2 = safe_slice(y_reg, idx2)
        mixed_reg_value = alpha * r1 + (1 - alpha) * r2
        mixup_samples.append(mixed_sample)
        mixup_cls.append(safe_slice(y_cls, idx1))  # ä¿æŒåŸå§‹æ ‡ç­¾
        mixup_reg.append(mixed_reg_value)

    # 3. CutMixæ•°æ®å¢å¼ºï¼ˆç‰¹å¾çº§åˆ«ï¼‰
    cutmix_samples = []
    cutmix_cls = []
    cutmix_reg = []

    cutmix_alpha = Config.augmentation_params.get('cutmix_alpha', 1.0)
    for i in range(n_augment):
        # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬
        idx1, idx2 = random.sample(range(n_samples), 2)
        s1 = safe_slice(X, idx1)
        s2 = safe_slice(X, idx2)

        # ç”ŸæˆCutMixæ©ç 
        lam = np.random.beta(cutmix_alpha, cutmix_alpha)
        cut_len = int(len(s1) * (1 - lam))
        cut_start = random.randint(0, len(s1) - cut_len)

        # åº”ç”¨CutMix
        mixed_sample = s1.copy()
        mixed_sample[cut_start:cut_start + cut_len] = s2[cut_start:cut_start + cut_len]

        cutmix_samples.append(mixed_sample)
        cutmix_cls.append(safe_slice(y_cls, idx1))
        cutmix_reg.append(safe_slice(y_reg, idx1))

    # åˆå¹¶æ‰€æœ‰å¢å¼ºæ•°æ®
    all_augmented_data = [
        (noise_samples, noise_cls, noise_reg),
        (mixup_samples, mixup_cls, mixup_reg),
        (cutmix_samples, cutmix_cls, cutmix_reg)
    ]

    for samples, cls_labels, reg_labels in all_augmented_data:
        if samples:
            if isinstance(X, pd.DataFrame):
                X_aug = pd.concat([X_aug, pd.DataFrame(samples, columns=X.columns)], axis=0)
            else:
                X_aug = np.concatenate([X_aug, np.array(samples)], axis=0)

            y_cls_aug = np.concatenate([y_cls_aug, np.array(cls_labels)], axis=0)

            if isinstance(y_reg, pd.DataFrame):
                y_reg_aug = pd.concat([y_reg_aug, pd.DataFrame(reg_labels, columns=y_reg.columns)], axis=0)
            else:
                y_reg_aug = np.concatenate([y_reg_aug, np.array(reg_labels)], axis=0)

    # ç¡®ä¿æ‰€æœ‰æ•°æ®é›†å¤§å°ä¸€è‡´
    min_samples = min(len(X_aug), len(y_cls_aug), len(y_reg_aug))
    if isinstance(X_aug, pd.DataFrame):
        X_aug = X_aug.iloc[:min_samples]
    else:
        X_aug = X_aug[:min_samples]
    y_cls_aug = y_cls_aug[:min_samples]
    if isinstance(y_reg_aug, pd.DataFrame):
        y_reg_aug = y_reg_aug.iloc[:min_samples]
    else:
        y_reg_aug = y_reg_aug[:min_samples]

    print(f"åŸå§‹æ•°æ®å¤§å°: {len(X)}")
    print(f"å¢å¼ºåæ•°æ®å¤§å°: {len(X_aug)}")
    print(f"å¢å¼ºæ¯”ä¾‹: {(len(X_aug) - len(X)) / len(X) * 100:.2f}%")

    return X_aug, y_cls_aug, y_reg_aug


def safe_slice(obj: Union[pd.DataFrame, pd.Series, np.ndarray], idx: int):
    assert isinstance(idx, int), f"idx must be int, got {type(idx)}"
    if isinstance(obj, (pd.DataFrame, pd.Series)):
        return obj.iloc[idx]
    else:
        return obj[idx]


# === å·¥å…·å‡½æ•°ï¼šå®‰å…¨è·å–åˆ— ===
def get_col_safe(df, candidates, idx=None):
    """
    ä»dfä¸­ä¼˜å…ˆè¿”å›candidatesåˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªå­˜åœ¨çš„åˆ—ã€‚
    idx: å¯é€‰ï¼Œè‹¥ä¸ºsliceæˆ–ç´¢å¼•ï¼Œåˆ™è¿”å›è¯¥èŒƒå›´/ç´¢å¼•çš„å€¼ã€‚
    """
    for cand in candidates:
        if cand in df.columns:
            if idx is not None:
                return df[cand].iloc[idx].values
            else:
                return df[cand]
    raise KeyError(f"None of {candidates} found in columns: {df.columns.tolist()}")


def check_data(X, y_reg, y_cls, name="train"):
    print(f"==== {name} æ•°æ®æ£€æŸ¥ ====")
    print("X shape:", X.shape)
    print("y_reg shape:", y_reg.shape)
    print("y_cls shape:", y_cls.shape)
    print("X NaN:", np.isnan(X).sum(), "Inf:", np.isinf(X).sum())
    print("y_reg NaN:", np.isnan(y_reg).sum(), "Inf:", np.isinf(y_reg).sum())
    print("y_cls NaN:", np.isnan(y_cls).sum(), "Inf:", np.isinf(y_cls).sum())
    print("y_reg describe:\n", pd.DataFrame(y_reg).describe())
    print("y_cls value counts:\n", pd.Series(y_cls).value_counts())
    print("=" * 40)


def generate_interaction_features(X, y=None, sample_size=100000, max_combos=100, top_n=10, selected_features=None):
    """
    ç”Ÿæˆäº¤äº’ç‰¹å¾ï¼Œå¹¶æ ¹æ®äº’ä¿¡æ¯é€‰æ‹©Topç‰¹å¾ã€‚

    å‚æ•°:
        X: pd.DataFrame
            è¾“å…¥ç‰¹å¾æ•°æ®ã€‚
        y: pd.Series or None
            ç›®æ ‡å˜é‡ï¼Œç”¨äºè®¡ç®—äº’ä¿¡æ¯ã€‚å¦‚æœä¸ºNoneï¼Œä»…è¿”å›äº¤äº’ç‰¹å¾ã€‚
        sample_size: int
            äº’ä¿¡æ¯è®¡ç®—æ—¶é‡‡æ ·çš„æ ·æœ¬æ•°ï¼ˆé¿å…å¤§æ•°æ®é›†å ç”¨è¿‡å¤šå†…å­˜ï¼‰ã€‚
        max_combos: int
            éšæœºé€‰æ‹©çš„æœ€å¤§ç»„åˆæ•°ï¼ˆé¿å…ç‰¹å¾ç»„åˆè¿‡å¤šï¼‰ã€‚
        top_n: int
            é€‰æ‹©Top Nä¸ªäº¤äº’ç‰¹å¾ï¼ˆåŸºäºäº’ä¿¡æ¯ï¼‰ã€‚
        selected_features: list or None
            å¦‚æœæä¾›ï¼Œåˆ™åªç”Ÿæˆè¿™äº›ç‰¹å¾çš„äº¤äº’ç‰¹å¾ã€‚

    è¿”å›:
        X_new: pd.DataFrame
            æ·»åŠ äº†äº¤äº’ç‰¹å¾çš„æ–°DataFrameã€‚
        selected_feats: list
            è¢«é€‰æ‹©çš„äº¤äº’ç‰¹å¾åï¼ˆå¦‚æœyä¸ä¸ºNoneï¼‰ã€‚
    """
    print("\nç”Ÿæˆäº¤äº’ç‰¹å¾...")

    # å¦‚æœ selected_features æ²¡æä¾›ï¼Œåˆ™ä½¿ç”¨ X çš„å…¨éƒ¨ç‰¹å¾
    features = selected_features if selected_features else X.columns.tolist()

    # è·å–ç‰¹å¾æ•°é‡
    num_features = len(features)
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {num_features}")

    # å¦‚æœç‰¹å¾æ•°å°äº2ï¼Œä¸ç”Ÿæˆäº¤äº’ç‰¹å¾
    if num_features < 2:
        print("ç‰¹å¾æ•°é‡ä¸è¶³ï¼Œè·³è¿‡äº¤äº’ç‰¹å¾ç”Ÿæˆã€‚")
        return X, []

    # è·å–æ‰€æœ‰ä¸¤ä¸¤ç»„åˆ
    from itertools import combinations
    combos = list(combinations(features, 2))
    print(f"ç»„åˆæ•°é‡: {len(combos)}")

    # å¦‚æœç»„åˆæ•°é‡è¿‡å¤šï¼Œéšæœºé‡‡æ · max_combos ä¸ªï¼ˆé˜²è¿‡æ‹Ÿåˆä¼˜åŒ–ï¼‰
    if len(combos) > max_combos:
        print(f"ç»„åˆæ•°é‡è¿‡å¤š({len(combos)})ï¼Œéšæœºé€‰æ‹©{max_combos}ä¸ªç»„åˆ")
        # ä¼˜å…ˆé€‰æ‹©äº’ä¿¡æ¯é«˜çš„ç‰¹å¾ç»„åˆ
        if y is not None and len(combos) > max_combos * 2:
            # å…ˆè®¡ç®—æ‰€æœ‰ç»„åˆçš„äº’ä¿¡æ¯ï¼Œé€‰æ‹©topç‰¹å¾
            from sklearn.feature_selection import mutual_info_regression
            combo_scores = []
            for f1, f2 in combos:
                if f1 in X.columns and f2 in X.columns:
                    try:
                        combo_feature = X[f1] * X[f2]
                        if y.ndim > 1:
                            y_target = y.iloc[:, 0] if hasattr(y, 'iloc') else y[:, 0]
                        else:
                            y_target = y
                        mi_score = \
                        mutual_info_regression(combo_feature.values.reshape(-1, 1), y_target, random_state=42)[0]
                        combo_scores.append((mi_score, f1, f2))
                    except:
                        combo_scores.append((0, f1, f2))

            # æŒ‰äº’ä¿¡æ¯åˆ†æ•°æ’åºï¼Œé€‰æ‹©topç»„åˆ
            combo_scores.sort(reverse=True)
            combos = [(f1, f2) for _, f1, f2 in combo_scores[:max_combos]]
        else:
            combos = random.sample(combos, max_combos)

    # åˆå§‹åŒ–äº¤äº’ç‰¹å¾ DataFrame
    inter_df = pd.DataFrame(index=X.index)

    # ç”Ÿæˆäº¤äº’ç‰¹å¾
    for f1, f2 in combos:
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨
        if f1 in X.columns and f2 in X.columns:
            inter_df[f"{f1}_x_{f2}"] = X[f1] * X[f2]
        else:
            # å°è¯•æŸ¥æ‰¾ç›¸ä¼¼çš„ç‰¹å¾åï¼ˆå¤„ç†å¯èƒ½çš„å‘½åå·®å¼‚ï¼‰
            f1_found = None
            f2_found = None

            # æŸ¥æ‰¾f1çš„åŒ¹é…ç‰¹å¾
            for col in X.columns:
                if f1 in col or col in f1:
                    f1_found = col
                    break

            # æŸ¥æ‰¾f2çš„åŒ¹é…ç‰¹å¾
            for col in X.columns:
                if f2 in col or col in f2:
                    f2_found = col
                    break

            if f1_found and f2_found:
                inter_df[f"{f1}_x_{f2}"] = X[f1_found] * X[f2_found]
            else:
                print(f"è­¦å‘Š: ç‰¹å¾ {f1} æˆ– {f2} ä¸å­˜åœ¨äºæ•°æ®ä¸­ï¼Œè·³è¿‡äº¤äº’ç‰¹å¾ {f1}_x_{f2}")
                if len(X.columns) <= 20:  # å¦‚æœç‰¹å¾ä¸å¤šï¼Œæ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾
                    print(f"å¯ç”¨ç‰¹å¾åˆ—: {X.columns.tolist()}")
                else:
                    print(f"å¯ç”¨ç‰¹å¾åˆ—: {X.columns.tolist()[:10]}...")
                # ç”¨0å¡«å……ç¼ºå¤±çš„äº¤äº’ç‰¹å¾
                inter_df[f"{f1}_x_{f2}"] = 0.0

    print(f"ç”Ÿæˆäº† {len(inter_df.columns)} ä¸ªäº¤äº’ç‰¹å¾")

    # å¦‚æœ y ä¸ºç©ºï¼Œç›´æ¥è¿”å›ï¼ˆé¢„æµ‹é˜¶æ®µç”¨ï¼‰
    if y is None:
        if selected_features is None:
            raise ValueError("é¢„æµ‹é˜¶æ®µå¿…é¡»æä¾› selected_features")

        # å¦‚æœmax_combos=0ï¼Œç›´æ¥ç”Ÿæˆé€‰ä¸­çš„äº¤äº’ç‰¹å¾ï¼Œä¸è¿›è¡Œæ–°çš„ç»„åˆ
        if max_combos == 0:
            print(f"ç›´æ¥ç”Ÿæˆ {len(selected_features)} ä¸ªé€‰ä¸­çš„äº¤äº’ç‰¹å¾...")
            result_df = pd.DataFrame(index=X.index)

            for feat in selected_features:
                # è§£æç‰¹å¾åï¼Œæ‰¾åˆ°å¯¹åº”çš„ä¸¤ä¸ªåŸå§‹ç‰¹å¾
                if '_x_' in feat:
                    f1, f2 = feat.split('_x_', 1)
                    if f1 in X.columns and f2 in X.columns:
                        result_df[feat] = X[f1] * X[f2]
                        print(f"âœ… æˆåŠŸç”Ÿæˆäº¤äº’ç‰¹å¾: {feat}")
                    else:
                        print(f"âš ï¸ è­¦å‘Š: åŸå§‹ç‰¹å¾ {f1} æˆ– {f2} ä¸å­˜åœ¨ï¼Œç”¨0å¡«å……")
                        result_df[feat] = 0.0
                else:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ•ˆçš„äº¤äº’ç‰¹å¾å {feat}ï¼Œç”¨0å¡«å……")
                    result_df[feat] = 0.0

            return result_df

        # åªç”Ÿæˆé€‰ä¸­çš„äº¤äº’ç‰¹å¾
        result_df = pd.DataFrame(index=X.index)
        print(f"å°è¯•ä¸ºéªŒè¯é›†ç”Ÿæˆ {len(selected_features)} ä¸ªäº¤äº’ç‰¹å¾...")
        print(f"éªŒè¯é›†å¯ç”¨ç‰¹å¾: {list(X.columns)[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªç‰¹å¾

        for feat in selected_features:
            if feat in inter_df.columns:
                result_df[feat] = inter_df[feat]
                print(f"âœ… æˆåŠŸç”Ÿæˆäº¤äº’ç‰¹å¾: {feat}")
            else:
                print(f"âš ï¸ è­¦å‘Š: äº¤äº’ç‰¹å¾ {feat} ä¸å­˜åœ¨ï¼Œç”¨0å¡«å……")
                print(f"   å¯ç”¨çš„äº¤äº’ç‰¹å¾: {list(inter_df.columns)[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ªå¯ç”¨ç‰¹å¾
                result_df[feat] = 0.0

        return result_df

        # ä¸è¿›è¡Œé‡‡æ ·ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©
        print(f"å¤§æ•°æ®é‡ä¼˜åŒ–ï¼šé‡‡æ ·100ä¸‡æ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©")
        if len(X) > 1000000:
            X_sample = inter_df.sample(n=1000000, random_state=42)
            y_sample = y.iloc[X_sample.index]
        else:
            X_sample = inter_df
            y_sample = y

    # è®¡ç®—äº’ä¿¡æ¯
    print("è®¡ç®—äº’ä¿¡æ¯å¹¶é€‰æ‹©topç‰¹å¾...")
    # å¦‚æœyæ˜¯å¤šç»´çš„ï¼Œå–ç¬¬ä¸€åˆ—ï¼ˆé€šå¸¸æ˜¯ä¸»è¦ç›®æ ‡ï¼‰
    if y_sample.ndim > 1:
        y_sample_1d = y_sample.iloc[:, 0] if hasattr(y_sample, 'iloc') else y_sample[:, 0]
        print(
            f"å¤šç›®æ ‡å›å½’ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡è¿›è¡Œç‰¹å¾é€‰æ‹©: {y_sample_1d.name if hasattr(y_sample_1d, 'name') else 'target_0'}")
    else:
        y_sample_1d = y_sample

    mi = mutual_info_regression(X_sample, y_sample_1d, random_state=42)

    # é€‰æ‹© top_n ä¸ªæœ€ç›¸å…³ç‰¹å¾
    top_idx = np.argsort(mi)[::-1][:top_n]
    selected_feats = [inter_df.columns[i] for i in top_idx]
    print(f"é€‰æ‹©äº† {len(selected_feats)} ä¸ªäº¤äº’ç‰¹å¾: {selected_feats}")

    # è¿”å›åŒ…å«äº¤äº’ç‰¹å¾çš„æ•°æ®
    return inter_df[selected_feats], selected_feats


# åŸå§‹è®­ç»ƒå‡½æ•°å·²åˆ é™¤ï¼Œç°åœ¨åªä¿ç•™é¢„æµ‹åŠŸèƒ½

def main():
    """ä¸»å‡½æ•°ï¼šæ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ"""
    try:
        # 1. éªŒè¯é…ç½®
        print("\n=== å¼€å§‹è¿è¡Œä¼˜åŒ–é¢„æµ‹ç¨‹åº ===")
        print("\n1. éªŒè¯é…ç½®...")
        if not Config.validate_params() or not Config.validate_paths():
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")

        # 2. åˆ›å»ºå¿…è¦çš„ç›®å½•
        print("\n2. åˆ›å»ºè¾“å‡ºç›®å½•...")
        Config.create_output_dirs()

        # 3. è®¾ç½®GPUå’Œå¤šGPUç­–ç•¥
        print("\n3. é…ç½®GPUå’Œå¤šGPUè®­ç»ƒç¯å¢ƒ...")

        # æ£€æŸ¥TensorFlow CUDAæ”¯æŒ
        print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
        print(f"CUDAæ”¯æŒ: {'æ˜¯' if tf.test.is_built_with_cuda() else 'å¦'}")

        if not tf.test.is_built_with_cuda():
            print("âš ï¸  å½“å‰TensorFlowç‰ˆæœ¬ä¸æ”¯æŒCUDAï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            print("ğŸ’¡ å»ºè®®å®‰è£…æ”¯æŒCUDAçš„TensorFlowç‰ˆæœ¬ä»¥è·å¾—GPUåŠ é€Ÿ")
            print("   å®‰è£…å‘½ä»¤: pip install tensorflow==2.10.1")
            use_gpu, num_gpus, strategy = False, 0, None
        else:
            use_gpu, num_gpus, strategy = setup_multi_gpu()

        if use_gpu and num_gpus >= 2:
            print(f"âœ… å¤šGPUè®­ç»ƒç¯å¢ƒé…ç½®æˆåŠŸï¼Œå°†ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
            print(f"ä½¿ç”¨ç­–ç•¥: {type(strategy).__name__}")
        elif use_gpu:
            print(f"âœ… å•GPUè®­ç»ƒç¯å¢ƒé…ç½®æˆåŠŸï¼Œå°†ä½¿ç”¨ {num_gpus} ä¸ªGPU")
            strategy = None
        else:
            print("â„¹ï¸  ä½¿ç”¨CPUæ¨¡å¼è¿›è¡Œè®­ç»ƒ")
            strategy = None

        # 3.1 éªŒè¯XGBoost GPUæ”¯æŒ
        print("\n3.1 éªŒè¯XGBoost GPUæ”¯æŒ...")
        Config.validate_xgboost_gpu()

        # 4. åŠ è½½æ•°æ®
        print("\n4. å¼€å§‹åŠ è½½å’Œå¤„ç†æ•°æ®...")
        X_train, y_cls_train, y_reg_train, X_val, val_ids, data_val = load_data()
        check_data(X_train, y_reg_train, y_cls_train, "train")

        # 4.1 éªŒè¯æ•°æ®è´¨é‡
        print("\n4.1 éªŒè¯æ•°æ®è´¨é‡...")
        # æ³¨æ„ï¼šè¿™é‡ŒéªŒè¯é›†è¿˜æ²¡æœ‰è¿›è¡Œlog1på˜æ¢ï¼Œæ‰€ä»¥å…ˆä¸éªŒè¯æ ‡ç­¾èŒƒå›´
        validate_training_data(X_train, y_reg_train, y_cls_train, X_val,
                               data_val[['per_mu', 'per_qu']], data_val['suit'])

        # ====== ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ ======
        print(f"\n4.1 ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ...")
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®
        train_sample_size = len(X_train)
        val_sample_size = len(X_val)

        print(f"ä½¿ç”¨å…¨éƒ¨æ•°æ®: è®­ç»ƒé›† {train_sample_size:,} æ¡ï¼ŒéªŒè¯é›† {val_sample_size:,} æ¡")
        # ä¸éœ€è¦åˆ‡ç‰‡ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ•°æ®

        # ç›®æ ‡å˜é‡çš„log1på˜æ¢å’Œclipå·²ç»åœ¨ä¸Šé¢å¤„ç†äº†

        # 2. ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè®­ç»ƒ/éªŒè¯ç”¨åŒä¸€ä¸ªscalerï¼‰- per_mué¢„æµ‹ä¼˜åŒ–
        print("æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆä½¿ç”¨RobustScalerï¼‰...")
        scaler = RobustScaler()  # ä½¿ç”¨RobustScalerå¤„ç†å¼‚å¸¸å€¼ï¼Œå¯¹per_mué¢„æµ‹æ›´é²æ£’
        X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
        X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)

        # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
        print("ğŸ” éªŒè¯ç‰¹å¾æ ‡å‡†åŒ–æ•ˆæœ...")
        try:
            train_mean = X_train.mean().mean()
            train_std = X_train.std().mean()
            val_mean = X_val.mean().mean()
            val_std = X_val.std().mean()

            print(f"è®­ç»ƒé›†æ ‡å‡†åŒ–åç»Ÿè®¡: å‡å€¼={train_mean:.6f}, æ ‡å‡†å·®={train_std:.6f}")
            print(f"éªŒè¯é›†æ ‡å‡†åŒ–åç»Ÿè®¡: å‡å€¼={val_mean:.6f}, æ ‡å‡†å·®={val_std:.6f}")

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            train_nan = X_train.isnull().sum().sum()
            train_inf = np.isinf(X_train.values).sum()
            val_nan = X_val.isnull().sum().sum()
            val_inf = np.isinf(X_val.values).sum()

            print(f"æ•°æ®è´¨é‡æ£€æŸ¥: è®­ç»ƒé›†NaN={train_nan}, Inf={train_inf}, éªŒè¯é›†NaN={val_nan}, Inf={val_inf}")

            if train_nan > 0 or train_inf > 0 or val_nan > 0 or val_inf > 0:
                print("âš ï¸ å‘ç°å¼‚å¸¸å€¼ï¼Œè¿›è¡Œæ¸…ç†...")
                X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)
                X_val = X_val.replace([np.inf, -np.inf], np.nan).fillna(0)
                print("âœ… å¼‚å¸¸å€¼æ¸…ç†å®Œæˆ")
            else:
                print("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— å¼‚å¸¸å€¼")

        except Exception as e:
            print(f"âš ï¸ æ ‡å‡†åŒ–éªŒè¯å¤±è´¥: {str(e)}")

        # ä¿å­˜scalerå’Œlabel encoder
        os.makedirs(Config.scaler_dir, exist_ok=True)
        joblib.dump(scaler, Config.scaler_file)
        print(f"Scalerå·²ä¿å­˜è‡³: {Config.scaler_file}")

        # åˆ›å»ºå¹¶ä¿å­˜label encoderï¼ˆç”¨äºåˆ†ç±»ç‰¹å¾ï¼‰
        label_encoder = LabelEncoder()
        # è¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„label encoderï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“åˆ†ç±»ç‰¹å¾è°ƒæ•´
        label_encoder.fit(['0', '1'])  # å‡è®¾åªæœ‰ä¸¤ä¸ªç±»åˆ«
        os.makedirs(Config.label_encoder_dir, exist_ok=True)
        joblib.dump(label_encoder, Config.label_encoder_file)
        print(f"Label encoderå·²ä¿å­˜è‡³: {Config.label_encoder_file}")

        # 3. ç‰¹å¾å·¥ç¨‹
        print("\n3. ç‰¹å¾å·¥ç¨‹...")

        # é¦–å…ˆå¤„ç†ç¼ºå¤±å€¼
        print("å¤„ç†ç¼ºå¤±å€¼...")

        # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
        print("è®­ç»ƒé›†ç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(X_train.isnull().sum())
        print("\néªŒè¯é›†ç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(X_val.isnull().sum())

        # å¯¹æ•°å€¼åˆ—è¿›è¡Œç¼ºå¤±å€¼å¡«å……
        numeric_columns = X_train.select_dtypes(include=[np.number]).columns

        # ä½¿ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
        for col in numeric_columns:
            if X_train[col].isnull().sum() > 0:
                median_val = X_train[col].median()
                X_train[col].fillna(median_val, inplace=True)
                if col in X_val.columns:
                    X_val[col].fillna(median_val, inplace=True)
                print(f"åˆ— {col}: ç”¨ä¸­ä½æ•° {median_val:.4f} å¡«å……äº† {X_train[col].isnull().sum()} ä¸ªç¼ºå¤±å€¼")

        # å¯¹éæ•°å€¼åˆ—è¿›è¡Œç¼ºå¤±å€¼å¡«å……
        non_numeric_columns = X_train.select_dtypes(exclude=[np.number]).columns
        for col in non_numeric_columns:
            if X_train[col].isnull().sum() > 0:
                # å¯¹äºéæ•°å€¼åˆ—ï¼Œç”¨ä¼—æ•°å¡«å……
                mode_val = X_train[col].mode().iloc[0] if len(X_train[col].mode()) > 0 else 0
                X_train[col].fillna(mode_val, inplace=True)
                if col in X_val.columns:
                    X_val[col].fillna(mode_val, inplace=True)
                print(f"åˆ— {col}: ç”¨ä¼—æ•° {mode_val} å¡«å……äº†ç¼ºå¤±å€¼")

        # å†æ¬¡æ£€æŸ¥ç¼ºå¤±å€¼
        print("\nå¡«å……åè®­ç»ƒé›†ç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(X_train.isnull().sum().sum())
        print("å¡«å……åéªŒè¯é›†ç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(X_val.isnull().sum().sum())

        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ•°å€¼ç±»å‹
        print("ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹...")
        for col in X_train.columns:
            try:
                X_train[col] = pd.to_numeric(X_train[col], errors='coerce')
                if col in X_val.columns:
                    X_val[col] = pd.to_numeric(X_val[col], errors='coerce')
            except Exception as e:
                print(f"è­¦å‘Šï¼šåˆ— {col} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹å¤±è´¥: {e}")
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œç”¨0å¡«å……
                X_train[col] = 0
                if col in X_val.columns:
                    X_val[col] = 0

        # æœ€ç»ˆæ£€æŸ¥ç¼ºå¤±å€¼
        if X_train.isnull().sum().sum() > 0 or X_val.isnull().sum().sum() > 0:
            print("è­¦å‘Šï¼šä»æœ‰ç¼ºå¤±å€¼ï¼Œç”¨0å¡«å……")
            X_train = X_train.fillna(0)
            X_val = X_val.fillna(0)

        print("ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼")

        # PCAç‰¹å¾ - ä¿®å¤ï¼šä½¿ç”¨æœ‰æ„ä¹‰çš„åˆ—å
        print("æ‰§è¡ŒPCAé™ç»´...")
        from sklearn.decomposition import PCA
        pca = PCA(n_components=0.95, random_state=42)
        X_train_pca = pd.DataFrame(pca.fit_transform(X_train))
        X_val_pca = pd.DataFrame(pca.transform(X_val))
        # ä¸ºPCAç‰¹å¾æ·»åŠ æœ‰æ„ä¹‰çš„åˆ—å
        pca_cols = [f'pca_{i}' for i in range(X_train_pca.shape[1])]
        X_train_pca.columns = pca_cols
        X_val_pca.columns = pca_cols

        # ä¿å­˜PCAå¯¹è±¡ä¾›æ¨ç†æ—¶ä½¿ç”¨
        pca_path = os.path.join(Config.scaler_dir, 'pca_AA.pkl')
        joblib.dump(pca, pca_path)
        print(f"PCAå¯¹è±¡å·²ä¿å­˜è‡³: {pca_path}")

        # äº¤äº’ç‰¹å¾ï¼ˆåªç”Ÿæˆä¸€æ¬¡ï¼‰
        print("ç”Ÿæˆäº¤äº’ç‰¹å¾...")
        print(f"è®­ç»ƒé›†ç‰¹å¾åˆ—å: {X_train.columns.tolist()[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªåˆ—å
        print(f"éªŒè¯é›†ç‰¹å¾åˆ—å: {X_val.columns.tolist()[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ªåˆ—å

        # ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—
        common_cols = X_train.columns.intersection(X_val.columns)
        print(f"å…±åŒç‰¹å¾åˆ—æ•°é‡: {len(common_cols)}")
        if len(common_cols) != len(X_train.columns):
            print(f"è­¦å‘Š: è®­ç»ƒé›†æœ‰ {len(X_train.columns)} ä¸ªç‰¹å¾ï¼ŒéªŒè¯é›†æœ‰ {len(X_val.columns)} ä¸ªç‰¹å¾")
            print("ä½¿ç”¨å…±åŒç‰¹å¾åˆ—è¿›è¡Œäº¤äº’ç‰¹å¾ç”Ÿæˆ...")
            X_train = X_train[common_cols]
            X_val = X_val[common_cols]

        # å¤§æ•°æ®é‡ä¼˜åŒ–ï¼šè·³è¿‡äº¤äº’ç‰¹å¾ç”Ÿæˆä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
        print("ä¸ºè®­ç»ƒé›†ç”Ÿæˆäº¤äº’ç‰¹å¾...")
        print("âš ï¸ å¤§æ•°æ®é‡ä¼˜åŒ–ï¼šè·³è¿‡äº¤äº’ç‰¹å¾ç”Ÿæˆä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦")
        print("   åŸå› ï¼š800ä¸‡è¡Œæ•°æ®ç”Ÿæˆäº¤äº’ç‰¹å¾éœ€è¦æ•°å°æ—¶ï¼Œå½±å“è®­ç»ƒæ•ˆç‡")

        # ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä¸ç”Ÿæˆäº¤äº’ç‰¹å¾
        X_train_inter = X_train.copy()
        selected_inter_feats = []
        print(f"è·³è¿‡äº¤äº’ç‰¹å¾ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾: {X_train_inter.shape}")

        print("ä¸ºéªŒè¯é›†ç”Ÿæˆç›¸åŒçš„äº¤äº’ç‰¹å¾...")
        # ç¡®ä¿éªŒè¯é›†æœ‰ç›¸åŒçš„ç‰¹å¾åˆ—
        X_val_aligned = X_val[X_train.columns]  # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´

        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦ä¸€è‡´
        print(f"è®­ç»ƒé›†ç‰¹å¾åˆ—æ•°: {len(X_train.columns)}")
        print(f"éªŒè¯é›†ç‰¹å¾åˆ—æ•°: {len(X_val_aligned.columns)}")
        print(f"ç‰¹å¾åˆ—æ˜¯å¦ä¸€è‡´: {list(X_train.columns) == list(X_val_aligned.columns)}")

        # æ£€æŸ¥é€‰ä¸­çš„äº¤äº’ç‰¹å¾
        print(f"é€‰ä¸­çš„äº¤äº’ç‰¹å¾: {selected_inter_feats}")

        # ç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œä¸ç”Ÿæˆäº¤äº’ç‰¹å¾
        X_val_inter = X_val_aligned.copy()
        print(f"éªŒè¯é›†è·³è¿‡äº¤äº’ç‰¹å¾ç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾: {X_val_inter.shape}")

        # ä¿å­˜äº¤äº’ç‰¹å¾å’Œç‰¹å¾é¡ºåºä¾›æ¨ç†æ—¶ä½¿ç”¨
        selected_inter_feats_path = os.path.join(Config.output_dir, 'selected_inter_feats.json')
        with open(selected_inter_feats_path, 'w', encoding='utf-8') as f:
            json.dump(selected_inter_feats, f, ensure_ascii=False, indent=2)
        print(f"äº¤äº’ç‰¹å¾å·²ä¿å­˜è‡³: {selected_inter_feats_path}")

        # ä¿å­˜ç‰¹å¾é¡ºåº
        feature_order_path = os.path.join(Config.output_dir, 'feature_order.json')
        with open(feature_order_path, 'w', encoding='utf-8') as f:
            json.dump(list(X_train.columns), f, ensure_ascii=False, indent=2)
        print(f"ç‰¹å¾é¡ºåºå·²ä¿å­˜è‡³: {feature_order_path}")

        # æ‹¼æ¥å‰å»é‡ï¼šå»é™¤é‡å¤åˆ—åï¼Œä¼˜å…ˆä¿ç•™åŸå§‹ç‰¹å¾
        def remove_duplicate_columns(df_list):
            seen = set()
            new_list = []
            for df in df_list:
                cols = []
                for col in df.columns:
                    if col not in seen:
                        cols.append(col)
                        seen.add(col)
                new_list.append(df[cols])
            return new_list

        X_train, X_train_pca, X_train_inter = remove_duplicate_columns([X_train, X_train_pca, X_train_inter])
        X_val, X_val_pca, X_val_inter = remove_duplicate_columns([X_val, X_val_pca, X_val_inter])

        # æ‹¼æ¥å…¨éƒ¨ç‰¹å¾
        X_train_all = pd.concat([X_train, X_train_pca, X_train_inter], axis=1)
        X_val_all = pd.concat([X_val, X_val_pca, X_val_inter], axis=1)

        # ä¿å­˜ç‰¹å¾é¡ºåºä¾›æ¨ç†æ—¶ä½¿ç”¨
        feature_order_path = os.path.join(Config.output_dir, 'feature_order.json')
        with open(feature_order_path, 'w', encoding='utf-8') as f:
            json.dump(list(X_train_all.columns), f, ensure_ascii=False, indent=2)
        print(f"ç‰¹å¾é¡ºåºå·²ä¿å­˜è‡³: {feature_order_path}")

        # æ‹¼æ¥åå±•å¹³å¤šé‡åˆ—
        def flatten_multicolumns(df):
            for col in list(df.columns):  # ä½¿ç”¨listé¿å…RuntimeError
                if isinstance(df[col], pd.DataFrame):
                    # å¤šé‡åˆ—ï¼Œå±•å¹³
                    for i in range(df[col].shape[1]):
                        new_col_name = f"{col}_{i}"
                        df[new_col_name] = df[col].iloc[:, i]
                    df.drop(columns=[col], inplace=True)
            return df

        X_train_all = flatten_multicolumns(X_train_all)
        X_val_all = flatten_multicolumns(X_val_all)

        # ç»Ÿä¸€è¡¥é½å’Œå¯¹é½
        missing_cols = set(X_train_all.columns) - set(X_val_all.columns)
        if missing_cols:
            print(f"ä¸ºéªŒè¯é›†æ·»åŠ ç¼ºå¤±ç‰¹å¾: {len(missing_cols)} ä¸ª")
            for col in missing_cols:
                X_val_all[col] = 0
        X_val_all = X_val_all[X_train_all.columns]

        # æ£€æŸ¥å¹¶å¼ºåˆ¶æ‰€æœ‰ç‰¹å¾ä¸ºæ•°å€¼å‹ï¼Œé˜²æ­¢objectåˆ—å¯¼è‡´XGBoostæŠ¥é”™
        for df_name, df in zip(['X_train_all', 'X_val_all'], [X_train_all, X_val_all]):
            for col in list(df.columns):  # Use list to avoid RuntimeError during iteration if columns are dropped
                try:
                    col_data = df[col]
                    # å¦‚æœæ˜¯DataFrameï¼ˆå¤šé‡åˆ—åæˆ–æ‹¼æ¥å‡ºé”™ï¼‰ï¼Œè‡ªåŠ¨å±•å¹³
                    if isinstance(col_data, pd.DataFrame):
                        print(f"ä¸¥é‡è­¦å‘Š: {df_name} çš„ {col} æ˜¯DataFrameï¼Œshape={col_data.shape}ï¼Œè‡ªåŠ¨å±•å¹³ï¼")
                        for i in range(col_data.shape[1]):
                            new_col_name = f"{col}_{i}"
                            df[new_col_name] = col_data.iloc[:, i]
                        df.drop(columns=[col], inplace=True)  # Drop original multi-column
                    elif not np.issubdtype(col_data.dtype, np.number):
                        print(f"è­¦å‘Š: {df_name} çš„ {col} ä¸æ˜¯æ•°å€¼å‹, å®é™…ç±»å‹: {col_data.dtype}")
                        df[col] = pd.to_numeric(col_data, errors='coerce')
                except (KeyError, TypeError) as e:
                    print(f"è·³è¿‡åˆ— {col}: {str(e)}")
                    continue

        # æœ€åå†å…¨é‡å±•å¹³ä¸€æ¬¡ï¼Œç¡®ä¿æ²¡æœ‰é—æ¼
        def flatten_all_multicolumns(df):
            for col in list(df.columns):
                try:
                    if isinstance(df[col], pd.DataFrame):
                        for i in range(df[col].shape[1]):
                            new_col_name = f"{col}_{i}"
                            df[new_col_name] = df[col].iloc[:, i]
                        df.drop(columns=[col], inplace=True)
                except (KeyError, TypeError) as e:
                    print(f"å±•å¹³æ—¶è·³è¿‡åˆ— {col}: {str(e)}")
                    continue
            return df

        X_train_all = flatten_all_multicolumns(X_train_all)
        X_val_all = flatten_all_multicolumns(X_val_all)

        # å¤§æ•°æ®é‡å†…å­˜ä¼˜åŒ–ï¼šåœ¨XGBoostç‰¹å¾æå–å‰è¿›è¡Œé‡‡æ ·
        print("4. å¤§æ•°æ®é‡å†…å­˜ä¼˜åŒ–ï¼šé‡‡æ ·åˆ°50ä¸‡æ¡æ•°æ®è¿›è¡ŒXGBoostè®­ç»ƒ...")
        max_samples = 500000  # 50ä¸‡æ¡æ•°æ®ï¼ˆè¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼‰

        if len(X_train_all) > max_samples:
            print(f"åŸå§‹è®­ç»ƒé›†: {len(X_train_all):,} è¡Œ")
            print(f"é‡‡æ ·åˆ°: {max_samples:,} è¡Œ")

            # éšæœºé‡‡æ ·
            np.random.seed(42)
            sample_indices = np.random.choice(len(X_train_all), max_samples, replace=False)
            X_train_sample = X_train_all.iloc[sample_indices]
            y_reg_sample = y_reg_train.iloc[sample_indices]

            print(f"é‡‡æ ·åè®­ç»ƒé›†: {len(X_train_sample):,} è¡Œ")
        else:
            print(f"æ•°æ®é‡é€‚ä¸­({len(X_train_all):,}è¡Œ)ï¼Œæ— éœ€é‡‡æ ·")
            X_train_sample = X_train_all
            y_reg_sample = y_reg_train

        # 4. XGBoostå¶å­èŠ‚ç‚¹ç‰¹å¾ï¼ˆstackingï¼‰
        print("4. æå–XGBoostå¶å­èŠ‚ç‚¹ç‰¹å¾...")
        xgb_model = xgb.XGBRegressor(n_estimators=50, max_depth=5, learning_rate=0.15)  # å‡å°‘æ ‘æ•°é‡ï¼ŒåŠ å¿«è®­ç»ƒ
        xgb_model.fit(X_train_sample, y_reg_sample['per_mu'])  # å•ç›®æ ‡

        # ä¿å­˜XGBoostæ¨¡å‹
        os.makedirs(Config.xgboost_dir, exist_ok=True)
        xgb_model.save_model(Config.xgboost_model_file)
        print(f"XGBoostæ¨¡å‹å·²ä¿å­˜è‡³: {Config.xgboost_model_file}")

        # åˆ†æ‰¹æå–å¶å­èŠ‚ç‚¹ç‰¹å¾ä»¥é¿å…å†…å­˜ä¸è¶³
        print("åˆ†æ‰¹æå–å¶å­èŠ‚ç‚¹ç‰¹å¾...")
        batch_size = 200000  # æ¯æ‰¹20ä¸‡è¡Œï¼Œæé«˜å¤„ç†æ•ˆç‡

        # è®­ç»ƒé›†åˆ†æ‰¹å¤„ç†
        leaf_train_list = []
        for i in range(0, len(X_train_all), batch_size):
            end_idx = min(i + batch_size, len(X_train_all))
            batch_X = X_train_all.iloc[i:end_idx]
            batch_leaf = xgb_model.apply(batch_X)
            leaf_train_list.append(batch_leaf)
            print(f"å¤„ç†è®­ç»ƒé›†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(X_train_all) - 1) // batch_size + 1}")

        leaf_train = np.vstack(leaf_train_list)

        # éªŒè¯é›†åˆ†æ‰¹å¤„ç†
        leaf_val_list = []
        for i in range(0, len(X_val_all), batch_size):
            end_idx = min(i + batch_size, len(X_val_all))
            batch_X = X_val_all.iloc[i:end_idx]
            batch_leaf = xgb_model.apply(batch_X)
            leaf_val_list.append(batch_leaf)
            print(f"å¤„ç†éªŒè¯é›†æ‰¹æ¬¡ {i // batch_size + 1}/{(len(X_val_all) - 1) // batch_size + 1}")

        leaf_val = np.vstack(leaf_val_list)
        # ç›´æ¥ç”¨DataFrameåŒ…è£…ï¼Œé˜²æ­¢åµŒå¥—
        leaf_train = pd.DataFrame(leaf_train, index=X_train_all.index)
        leaf_val = pd.DataFrame(leaf_val, index=X_val_all.index)
        X_train_all = pd.concat([X_train_all, leaf_train], axis=1)
        X_val_all = pd.concat([X_val_all, leaf_val], axis=1)

        # æœ€ç»ˆæ£€æŸ¥ç¡®ä¿æ‰€æœ‰ç‰¹å¾ä¸ºæ•°å€¼å‹
        print("æœ€ç»ˆæ£€æŸ¥ç‰¹å¾ç±»å‹...")
        for df_name, df in zip(['X_train_all', 'X_val_all'], [X_train_all, X_val_all]):
            for col in list(df.columns):
                try:
                    col_data = df[col]
                    if isinstance(col_data, pd.DataFrame):
                        print(f"æœ€ç»ˆè­¦å‘Š: {df_name} çš„ {col} ä»æ˜¯DataFrameï¼Œshape={col_data.shape}ï¼Œå¼ºåˆ¶å±•å¹³ï¼")
                        for i in range(col_data.shape[1]):
                            new_col_name = f"{col}_{i}"
                            df[new_col_name] = col_data.iloc[:, i]
                        df.drop(columns=[col], inplace=True)
                    elif not np.issubdtype(col_data.dtype, np.number):
                        print(f"æœ€ç»ˆè­¦å‘Š: {df_name} çš„ {col} ä¸æ˜¯æ•°å€¼å‹, å®é™…ç±»å‹: {col_data.dtype}")
                        df[col] = pd.to_numeric(col_data, errors='coerce')
                except (KeyError, TypeError) as e:
                    print(f"æœ€ç»ˆæ£€æŸ¥è·³è¿‡åˆ— {col}: {str(e)}")
                    continue

        # ç¡®ä¿æ‰€æœ‰åˆ—åéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œé¿å…sklearnæŠ¥é”™
        print("ç¡®ä¿æ‰€æœ‰åˆ—åéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹...")
        X_train_all.columns = X_train_all.columns.astype(str)
        X_val_all.columns = X_val_all.columns.astype(str)

        print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: X_train_all={X_train_all.shape[1]}, X_val_all={X_val_all.shape[1]}")
        print(f"X_train_allåˆ—åç±»å‹: {set(type(col) for col in X_train_all.columns)}")
        print(f"X_val_allåˆ—åç±»å‹: {set(type(col) for col in X_val_all.columns)}")

        # 5. RidgeåŸºçº¿ - åªå…³æ³¨per_mu
        from sklearn.linear_model import Ridge
        from sklearn.metrics import r2_score
        y_reg_train_1d = np.asarray(y_reg_train['per_mu'])
        ridge = Ridge()
        ridge.fit(X_train_all, y_reg_train_1d)
        print("Ridgeå›å½’RÂ² (per_mu):", r2_score(y_reg_train_1d, ridge.predict(X_train_all)))

        # 6. ç‰¹å¾é‡è¦æ€§ç­›é€‰top 50ç‰¹å¾
        feature_names = X_train_all.columns.tolist()
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {len(feature_names)}")
        print(f"åŸå§‹ç‰¹å¾åç¤ºä¾‹: {feature_names[:10]}")

        # è¿‡æ»¤æ‰æ•°å­—ç‰¹å¾åï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„ç‰¹å¾å
        meaningful_features = [f for f in feature_names if not f.replace('_', '').replace('.', '').isdigit()]
        if len(meaningful_features) < 10:
            # å¦‚æœæœ‰æ„ä¹‰ç‰¹å¾å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            meaningful_features = feature_names

        print(f"æœ‰æ„ä¹‰çš„ç‰¹å¾æ•°é‡: {len(meaningful_features)}")

        feature_importance = analyze_feature_importance(
            xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1).fit(X_train_all, y_reg_train['per_mu']),
            # å‡å°‘æ ‘æ•°é‡ï¼ŒåŠ å¿«è®­ç»ƒ
            X_train_all, y_reg_train['per_mu'], meaningful_features, threshold=0.01
        )

        # è·å–topç‰¹å¾ï¼Œä½†ç¡®ä¿è¿™äº›ç‰¹å¾ååœ¨X_train_allä¸­å­˜åœ¨
        top_features_df = feature_importance.sort_values('importance', ascending=False)
        top_features = []

        for _, row in top_features_df.iterrows():
            feature_name = row['feature']
            if feature_name in X_train_all.columns:
                top_features.append(feature_name)
            elif feature_name.startswith('feature_'):
                # å¦‚æœæ˜¯é€šç”¨ç‰¹å¾åï¼Œå°è¯•æ˜ å°„åˆ°å®é™…ç‰¹å¾
                try:
                    feature_idx = int(feature_name.split('_')[1])
                    if feature_idx < len(feature_names):
                        actual_feature = feature_names[feature_idx]
                        if actual_feature in X_train_all.columns:
                            top_features.append(actual_feature)
                except (ValueError, IndexError):
                    continue

            if len(top_features) >= 50:
                break

        print(f"æˆåŠŸåŒ¹é…çš„Topç‰¹å¾æ•°é‡: {len(top_features)}")
        print(f"Topç‰¹å¾ç¤ºä¾‹: {top_features[:10]}")

        # ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
        X_train_final = X_train_all[top_features]
        X_val_final = X_val_all[top_features]

        # ç¡®ä¿ç‰¹å¾å¯¹é½
        if hasattr(X_train_final, 'columns') and hasattr(X_val_final, 'columns'):
            missing_cols = set(X_train_final.columns) - set(X_val_final.columns)
            if missing_cols:
                print(f"éªŒè¯é›†ç¼ºå¤±ç‰¹å¾: {len(missing_cols)} ä¸ªï¼Œè‡ªåŠ¨è¡¥0")
                for col in missing_cols:
                    X_val_final[col] = 0
            X_val_final = X_val_final[X_train_final.columns]
        else:
            print("è­¦å‘Š: X_train_finalæˆ–X_val_finalä¸æ˜¯DataFrameï¼Œè·³è¿‡ç‰¹å¾å¯¹é½æ£€æŸ¥")

        # æ›´æ–°è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        X_train = X_train_final
        X_val = X_val_final

        # 7. å‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†æ ‡ç­¾
        print("\n7. å‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†æ ‡ç­¾...")

        # ä»åŸå§‹æ ‡ç­¾ä¸­æå–å¯¹åº”çš„æ ‡ç­¾
        y_cls_train_final = y_cls_train[:len(X_train)]
        y_reg_train_final = y_reg_train.iloc[:len(X_train)]

        # ä»data_valä¸­æå–å¯¹åº”çš„æ ‡ç­¾
        y_cls_val = data_val['suit'].iloc[:len(X_val)]
        y_reg_val = data_val[['per_mu', 'per_qu']].iloc[:len(X_val)]

        # === å¯¹éªŒè¯é›†ç›®æ ‡å˜é‡è¿›è¡Œç›¸åŒçš„é¢„å¤„ç† ===
        print("å¯¹éªŒè¯é›†ç›®æ ‡å˜é‡è¿›è¡Œlog1på˜æ¢...")
        y_reg_val = np.log1p(y_reg_val)

        # ä¿å­˜å˜æ¢åçš„éªŒè¯é›†æ ‡ç­¾ï¼Œä¾›åç»­é‡‡æ ·ä½¿ç”¨
        y_reg_val_transformed = y_reg_val.copy()

        # å¤§æ•°æ®é‡å†…å­˜ä¼˜åŒ–ï¼šåœ¨XGBoostç‰¹å¾æå–å‰è¿›è¡Œé‡‡æ ·
        print("4. å¤§æ•°æ®é‡å†…å­˜ä¼˜åŒ–ï¼šé‡‡æ ·åˆ°100ä¸‡æ¡æ•°æ®è¿›è¡Œè®­ç»ƒ...")
        max_samples = 1000000  # 100ä¸‡æ¡æ•°æ®ï¼ˆè¿›ä¸€æ­¥ä¼˜åŒ–è¿è¡Œæ—¶é—´ï¼‰

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡‡æ ·
        if len(X_train_all) > max_samples:
            print(f"åŸå§‹è®­ç»ƒé›†: {len(X_train_all):,} è¡Œ")
            print(f"é‡‡æ ·åˆ°: {max_samples:,} è¡Œ")

            # åˆ†å±‚é‡‡æ ·ä¿æŒæ•°æ®åˆ†å¸ƒ
            X_train_sampled, _, y_cls_train_sampled, _, y_reg_train_sampled, _ = train_test_split(
                X_train_all, y_cls_train_final, y_reg_train_final,
                train_size=max_samples,
                random_state=42,
                stratify=y_cls_train_final
            )

            # éªŒè¯é›†ä¹Ÿç›¸åº”é‡‡æ ·
            val_samples = min(max_samples // 4, len(X_val_all))  # éªŒè¯é›†ä¸ºè®­ç»ƒé›†çš„1/4
            X_val_sampled, _, y_cls_val_sampled, _, y_reg_val_sampled, _ = train_test_split(
                X_val_all, y_cls_val, y_reg_val_transformed,  # ä½¿ç”¨å˜æ¢åçš„éªŒè¯é›†æ ‡ç­¾
                train_size=val_samples,
                random_state=42,
                stratify=y_cls_val
            )

            print(f"é‡‡æ ·åè®­ç»ƒé›†: {len(X_train_sampled):,} è¡Œ")
            print(f"é‡‡æ ·åéªŒè¯é›†: {len(X_val_sampled):,} è¡Œ")

            # æ›´æ–°å˜é‡
            X_train_all = X_train_sampled
            X_val_all = X_val_sampled
            y_cls_train_final = y_cls_train_sampled
            y_cls_val = y_cls_val_sampled
            y_reg_train_final = y_reg_train_sampled
            y_reg_val = y_reg_val_sampled

            # é‡‡æ ·åçš„éªŒè¯é›†æ ‡ç­¾å·²ç»æ˜¯log1på˜æ¢åçš„ï¼Œæ— éœ€å†æ¬¡å˜æ¢
            print("é‡‡æ ·åçš„éªŒè¯é›†æ ‡ç­¾å·²ç»æ˜¯log1på˜æ¢åçš„ï¼Œæ— éœ€å†æ¬¡å˜æ¢")
        else:
            print(f"æ•°æ®é‡é€‚ä¸­({len(X_train_all):,}è¡Œ)ï¼Œæ— éœ€é‡‡æ ·")

        # å¯¹éªŒè¯é›†ä¹Ÿè¿›è¡Œç›¸åŒçš„è£å‰ª
        for col in y_reg_val.columns:
            q_low = y_reg_val[col].quantile(0.001)
            q_high = y_reg_val[col].quantile(0.999)
            y_reg_val[col] = np.clip(y_reg_val[col], q_low, q_high)

        print(f"éªŒè¯é›†log1på˜æ¢åç»Ÿè®¡ä¿¡æ¯:")
        print(
            f"per_mu: min={y_reg_val['per_mu'].min():.4f}, max={y_reg_val['per_mu'].max():.4f}, mean={y_reg_val['per_mu'].mean():.4f}")
        print(
            f"per_qu: min={y_reg_val['per_qu'].min():.4f}, max={y_reg_val['per_qu'].max():.4f}, mean={y_reg_val['per_qu'].mean():.4f}")

        print(f"è®­ç»ƒé›†æ ‡ç­¾å½¢çŠ¶: y_cls={y_cls_train_final.shape}, y_reg={y_reg_train_final.shape}")
        print(f"éªŒè¯é›†æ ‡ç­¾å½¢çŠ¶: y_cls={y_cls_val.shape}, y_reg={y_reg_val.shape}")

        # 8. äº¤å‰éªŒè¯å’Œæ¨¡å‹é›†æˆ
        print("\n8. æ‰§è¡Œäº¤å‰éªŒè¯å’Œæ¨¡å‹é›†æˆ...")

        # ä½¿ç”¨å…¨å±€å˜é‡æ£€æŸ¥RÂ²æ˜¯å¦å·²è¾¾åˆ°0.8
        global GLOBAL_R2_ACHIEVED, GLOBAL_BEST_PARAMS, GLOBAL_BEST_R2

        print(f"ğŸ” å…¨å±€RÂ²çŠ¶æ€æ£€æŸ¥: GLOBAL_R2_ACHIEVED={GLOBAL_R2_ACHIEVED}, GLOBAL_BEST_R2={GLOBAL_BEST_R2}")
        print(f"ğŸ” å…¨å±€æœ€ä½³å‚æ•°: {GLOBAL_BEST_PARAMS}")

        if GLOBAL_R2_ACHIEVED:
            print(f"ğŸ¯ æ£€æµ‹åˆ°RÂ²å·²è¾¾åˆ°{GLOBAL_BEST_R2:.4f}ï¼Œäº¤å‰éªŒè¯å°†ä½¿ç”¨æœ€ä½³å‚æ•°ï¼Œè·³è¿‡è°ƒå‚")
            print(f"ä½¿ç”¨å…¨å±€æœ€ä½³å‚æ•°: {GLOBAL_BEST_PARAMS}")
        else:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ°RÂ²è¾¾åˆ°0.8ï¼Œäº¤å‰éªŒè¯å°†è¿›è¡Œæ­£å¸¸è°ƒå‚")

        ensemble_models, cv_scores = cross_validate_and_ensemble(
            X_train_all,  # ä½¿ç”¨é‡‡æ ·åçš„è®­ç»ƒæ•°æ®
            y_cls_train_final,
            y_reg_train_final,
            n_splits=Config.ensemble_params['n_splits'],
            n_models=Config.ensemble_params['n_models'],
            strategy=strategy,
            global_best_params=GLOBAL_BEST_PARAMS,
            r2_achieved=GLOBAL_R2_ACHIEVED
        )

        # 9. ç‰¹å¾æå–å’Œç»„åˆ
        print("\n9. ç‰¹å¾æå–å’Œç»„åˆ...")
        # ä½¿ç”¨é‡‡æ ·åçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
        print(f"ä½¿ç”¨é‡‡æ ·åè®­ç»ƒæ•°æ®: {len(X_train_all):,} ä¸ªæ ·æœ¬")
        print(f"ä½¿ç”¨é‡‡æ ·åéªŒè¯æ•°æ®: {len(X_val_all):,} ä¸ªæ ·æœ¬")

        X_train_sampled = X_train_all
        y_cls_train_sampled = y_cls_train_final
        y_reg_train_sampled = y_reg_train_final
        X_val_sampled = X_val_all
        y_cls_val_sampled = y_cls_val
        y_reg_val_sampled = y_reg_val

        X_train_combined, X_val_combined, xgb_model, y_cls_train_sample, y_cls_val_sample, y_reg_train_sample, y_reg_val_sample = extract_xgboost_features(
            X_train_sampled, X_val_sampled, y_cls_train_sampled, y_cls_val_sampled, y_reg_train_sampled,
            y_reg_val_sampled
        )
        # ä¿®å¤ï¼šå°†X_train_combinedã€X_val_combinedè½¬ä¸ºDataFrameå¹¶æ‹¼æ¥åˆ—å
        # å®‰å…¨è·å–åŸå§‹ç‰¹å¾å
        if hasattr(X_train_final, 'columns'):
            orig_feature_names = list(X_train_final.columns)
        else:
            # å¦‚æœX_train_finalæ˜¯numpyæ•°ç»„ï¼Œä½¿ç”¨ç‰¹å¾ååˆ—è¡¨
            orig_feature_names = feature_names[:X_train_final.shape[1]]
        n_leaf = X_train_combined.shape[1] - len(orig_feature_names)
        leaf_feature_names = [f'xgb_leaf_{i}' for i in range(n_leaf)]
        all_feature_names = orig_feature_names + leaf_feature_names
        X_train_combined = pd.DataFrame(X_train_combined, columns=all_feature_names)
        X_val_combined = pd.DataFrame(X_val_combined, columns=all_feature_names)

        # 10. å¤„ç†éªŒè¯é›†ç‰¹å¾
        print("\n10. å¤„ç†éªŒè¯é›†ç‰¹å¾...")
        # æ³¨æ„ï¼šX_val_combinedå·²ç»åœ¨extract_xgboost_featuresä¸­å¤„ç†è¿‡äº†ï¼Œä¸éœ€è¦é‡æ–°å¤„ç†
        # åªéœ€è¦ç¡®ä¿åˆ—åä¸€è‡´
        X_val_original_for_ensemble = X_val_all.copy()

        # ç¡®ä¿æ‰€æœ‰åˆ—åä¸ºstrç±»å‹
        X_train_combined.columns = [str(c) for c in X_train_combined.columns]
        X_val_combined.columns = [str(c) for c in X_val_combined.columns]

        # æ£€æŸ¥æ•°æ®ç»´åº¦æ˜¯å¦åŒ¹é…
        print(f"è®­ç»ƒé›†ç‰¹å¾å½¢çŠ¶: {X_train_combined.shape}")
        print(f"éªŒè¯é›†ç‰¹å¾å½¢çŠ¶: {X_val_combined.shape}")
        print(f"è®­ç»ƒé›†æ ‡ç­¾å½¢çŠ¶: y_cls={y_cls_train_sample.shape}, y_reg={y_reg_train_sample.shape}")
        print(f"éªŒè¯é›†æ ‡ç­¾å½¢çŠ¶: y_cls={y_cls_val_sample.shape}, y_reg={y_reg_val_sample.shape}")

        # ç¡®ä¿æ•°æ®ç»´åº¦åŒ¹é…
        if X_train_combined.shape[0] != len(y_cls_train_sample) or X_train_combined.shape[0] != len(y_reg_train_sample):
            print("è­¦å‘Šï¼šè®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¯¹é½...")
            min_len = min(X_train_combined.shape[0], len(y_cls_train_sample), len(y_reg_train_sample))
            X_train_combined = X_train_combined.iloc[:min_len]
            y_cls_train_sample = y_cls_train_sample.iloc[:min_len] if hasattr(y_cls_train_sample,
                                                                              'iloc') else y_cls_train_sample[:min_len]
            y_reg_train_sample = y_reg_train_sample.iloc[:min_len] if hasattr(y_reg_train_sample,
                                                                              'iloc') else y_reg_train_sample[:min_len]

        if X_val_combined.shape[0] != len(y_cls_val_sample) or X_val_combined.shape[0] != len(y_reg_val_sample):
            print("è­¦å‘Šï¼šéªŒè¯é›†ç‰¹å¾å’Œæ ‡ç­¾ç»´åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå¯¹é½...")
            min_len = min(X_val_combined.shape[0], len(y_cls_val_sample), len(y_reg_val_sample))
            X_val_combined = X_val_combined.iloc[:min_len]
            y_cls_val_sample = y_cls_val_sample.iloc[:min_len] if hasattr(y_cls_val_sample,
                                                                          'iloc') else y_cls_val_sample[:min_len]
            y_reg_val_sample = y_reg_val_sample.iloc[:min_len] if hasattr(y_reg_val_sample,
                                                                          'iloc') else y_reg_val_sample[:min_len]

        # 11. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print("\n11. è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        final_model_path = os.path.join(Config.final_model_dir, 'best_hybrid_multioutput_AA.h5')
        if os.path.exists(final_model_path):
            print("å‘ç°å·²å­˜åœ¨çš„æœ€ç»ˆæ¨¡å‹ï¼Œå°è¯•åŠ è½½...")
            try:
                best_model = load_model_with_custom_objects(final_model_path)
                print("æˆåŠŸåŠ è½½å·²å­˜åœ¨çš„æœ€ç»ˆæ¨¡å‹")
                history = None  # æ²¡æœ‰å†å²è®°å½•
            except Exception as e:
                print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                print("å°†é‡æ–°è®­ç»ƒæ¨¡å‹...")
                best_model, history = None, None
        else:
            best_model, history = None, None

        if best_model is None:
            # æ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
            print("æœ€ç»ˆè®­ç»ƒæ•°æ®è´¨é‡æ£€æŸ¥:")
            print(f"X_train_combined NaNæ•°é‡: {X_train_combined.isnull().sum().sum()}")
            print(f"X_val_combined NaNæ•°é‡: {X_val_combined.isnull().sum().sum()}")
            print(
                f"y_reg_train_sample NaNæ•°é‡: {np.isnan(y_reg_train_sample).sum() if hasattr(y_reg_train_sample, 'shape') else 'N/A'}")
            print(
                f"y_reg_val_sample NaNæ•°é‡: {np.isnan(y_reg_val_sample).sum() if hasattr(y_reg_val_sample, 'shape') else 'N/A'}")

            # æ£€æŸ¥æ˜¯å¦æœ‰æ— ç©·å¤§å€¼
            print(
                f"X_train_combined Infæ•°é‡: {np.isinf(X_train_combined.select_dtypes(include=[np.number])).sum().sum()}")
            print(f"X_val_combined Infæ•°é‡: {np.isinf(X_val_combined.select_dtypes(include=[np.number])).sum().sum()}")

            # å¦‚æœå‘ç°NaNæˆ–Infï¼Œè¿›è¡Œæ¸…ç†
            if X_train_combined.isnull().sum().sum() > 0:
                print("æ¸…ç†è®­ç»ƒé›†NaNå€¼...")
                X_train_combined = X_train_combined.fillna(0)

            if X_val_combined.isnull().sum().sum() > 0:
                print("æ¸…ç†éªŒè¯é›†NaNå€¼...")
                X_val_combined = X_val_combined.fillna(0)

            # ç¡®ä¿æ ‡ç­¾æ•°æ®æ˜¯numpyæ•°ç»„ä¸”æ²¡æœ‰NaN
            if hasattr(y_reg_train_sample, 'values'):
                y_reg_train_sample = y_reg_train_sample.values
            if hasattr(y_reg_val_sample, 'values'):
                y_reg_val_sample = y_reg_val_sample.values

            # æ£€æŸ¥å¹¶æ¸…ç†æ ‡ç­¾ä¸­çš„NaN
            if np.isnan(y_reg_train_sample).any():
                print("æ¸…ç†è®­ç»ƒæ ‡ç­¾NaNå€¼...")
                y_reg_train_sample = np.nan_to_num(y_reg_train_sample, nan=0.0)

            if np.isnan(y_reg_val_sample).any():
                print("æ¸…ç†éªŒè¯æ ‡ç­¾NaNå€¼...")
                y_reg_val_sample = np.nan_to_num(y_reg_val_sample, nan=0.0)

            try:
                best_model, history = train_final_model(
                    X_train_combined, y_cls_train_sample, y_reg_train_sample,
                    X_val_combined, y_cls_val_sample, y_reg_val_sample,
                    strategy=strategy
                )
            except Exception as e:
                print(f"æœ€ç»ˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°é‡æ–°è®­ç»ƒ...")
                # ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
                best_model, history = train_final_model_conservative(
                    X_train_combined, y_cls_train_sample, y_reg_train_sample,
                    X_val_combined, y_cls_val_sample, y_reg_val_sample,
                    strategy=strategy
                )

        # 12. åˆ†æç‰¹å¾é‡è¦æ€§
        print("\n12. åˆ†æç‰¹å¾é‡è¦æ€§...")
        # ä½¿ç”¨åŸå§‹ç‰¹å¾åè¿›è¡Œåˆ†æï¼Œè€Œä¸æ˜¯æ•°å­—ç‰¹å¾å
        # å®‰å…¨è·å–åŸå§‹ç‰¹å¾å
        if hasattr(X_train_final, 'columns'):
            original_feature_names = list(X_train_final.columns)
        else:
            # å¦‚æœX_train_finalæ˜¯numpyæ•°ç»„ï¼Œä½¿ç”¨ç‰¹å¾ååˆ—è¡¨
            original_feature_names = feature_names[:X_train_final.shape[1]]
        # è¿‡æ»¤æ‰æ•°å­—ç‰¹å¾åï¼Œåªä¿ç•™æœ‰æ„ä¹‰çš„ç‰¹å¾å
        meaningful_features = [f for f in original_feature_names if not f.replace('_', '').replace('.', '').isdigit()]
        if len(meaningful_features) < 3:
            # å¦‚æœæœ‰æ„ä¹‰ç‰¹å¾å¤ªå°‘ï¼Œä½¿ç”¨æ‰€æœ‰ç‰¹å¾
            meaningful_features = original_feature_names

        # ä¿®å¤ï¼šç¡®ä¿ä¼ å…¥çš„X_trainå’Œfeature_namesåŒ¹é…
        # ä½¿ç”¨X_train_finalè€Œä¸æ˜¯X_train_combinedï¼Œå› ä¸ºxgb_modelæ˜¯åœ¨X_train_combinedä¸Šè®­ç»ƒçš„
        # ä½†æˆ‘ä»¬éœ€è¦åˆ†æåŸå§‹ç‰¹å¾çš„é‡è¦æ€§

        # å®‰å…¨è·å–ç‰¹å¾åˆ—å
        if hasattr(X_train_final, 'columns'):
            feature_names_for_analysis = list(X_train_final.columns)
        else:
            # å¦‚æœX_train_finalæ˜¯numpyæ•°ç»„ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾å
            feature_names_for_analysis = original_feature_names[:X_train_final.shape[1]]

        feature_importance = analyze_feature_importance(
            xgb_model,
            X_train_combined[:10000] if X_train_combined.shape[0] > 10000 else X_train_combined,
            y_reg_train['per_mu'][:10000] if y_reg_train.shape[0] > 10000 else y_reg_train['per_mu'],
            feature_names_for_analysis,  # ä½¿ç”¨å®‰å…¨çš„ç‰¹å¾åˆ—å
            threshold=Config.feature_importance['threshold']
        )

        # 13. æ¨¡å‹é¢„æµ‹ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
        print("\n13. åœ¨éªŒè¯é›†ä¸Šæ‰§è¡Œé¢„æµ‹...")
        # åˆ†å—é¢„æµ‹å’Œåˆ†å—å†™å‡ºç»“æœï¼Œé˜²æ­¢çˆ†å†…å­˜
        batch_size = 200_000  # å¢å¤§æ‰¹æ¬¡å¤§å°ï¼Œæé«˜é¢„æµ‹æ•ˆç‡
        n_val = X_val_combined.shape[0]
        results_chunks = []
        for start in range(0, n_val, batch_size):
            end = min(start + batch_size, n_val)
            cls_pred_batch, reg_pred_batch = best_model.predict(X_val_combined[start:end])
            # æ–°å¢ï¼šé¢„æµ‹ååå˜æ¢
            reg_pred_batch = np.expm1(reg_pred_batch)
            results_chunk = pd.DataFrame({
                'x': get_col_safe(data_val, ['x', 'x_product', 'right_x'], slice(start, end)),
                'y': get_col_safe(data_val, ['y', 'y_product', 'right_y'], slice(start, end)),
                'yyyy': get_col_safe(data_val, ['yyyy', 'right_yyyy', 'year'], slice(start, end)),
                'suit': np.argmax(cls_pred_batch, axis=1),
                'per_mu': reg_pred_batch[:, 1],
                'per_qu': reg_pred_batch[:, 0]
            })
            results_chunks.append(results_chunk)
            import gc;
            gc.collect()
        results_df = pd.concat(results_chunks, ignore_index=True)
        results_df = results_df[['x', 'y', 'yyyy', 'suit', 'per_mu', 'per_qu']]
        os.makedirs(os.path.dirname(Config.result_file), exist_ok=True)
        results_df.to_csv(Config.result_file, index=False, encoding='utf-8')
        del results_chunks, results_df;
        gc.collect()

        # 14. ä¿å­˜è®­ç»ƒå†å²
        print("\n14. ä¿å­˜è®­ç»ƒå†å²...")
        history_file = os.path.join(Config.logs_dir, "training_history.json")
        os.makedirs(os.path.dirname(history_file), exist_ok=True)

        if history is not None:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history.history, f, default=json_fallback, ensure_ascii=False, indent=2)
            print(f"è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_file}")
        else:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒå†å²è®°å½•ï¼ˆä½¿ç”¨äº†å·²å­˜åœ¨çš„æ¨¡å‹ï¼‰")
            # ä¿å­˜ä¸€ä¸ªç©ºçš„è®­ç»ƒå†å²è®°å½•
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump({"message": "ä½¿ç”¨å·²å­˜åœ¨çš„æ¨¡å‹ï¼Œæ— è®­ç»ƒå†å²è®°å½•"}, f, ensure_ascii=False, indent=2)

        # 15. ä¿å­˜ç‰¹å¾é‡è¦æ€§
        print("\n15. ä¿å­˜ç‰¹å¾é‡è¦æ€§...")
        os.makedirs(Config.feature_importance_dir, exist_ok=True)
        feature_importance.to_csv(Config.feature_importance_file, index=False, encoding='utf-8')

        # ä¿å­˜å…³é”®ç‰¹å¾åˆ—è¡¨
        key_features = feature_importance[feature_importance['importance'] > Config.feature_importance['threshold']][
            'feature'].tolist()
        with open(Config.key_features_list_file, 'w', encoding='utf-8') as f:
            json.dump(key_features, f, ensure_ascii=False, indent=2)

        # 16. ç”Ÿæˆé¢„æµ‹åœ°å›¾
        print("\n16. ç”Ÿæˆé¢„æµ‹åœ°å›¾...")
        # ä¿®å¤ï¼šä½¿ç”¨æœ€åä¸€æ¬¡batchçš„é¢„æµ‹ç»“æœï¼Œæˆ–è€…é‡æ–°é¢„æµ‹ä¸€å°éƒ¨åˆ†æ•°æ®ç”¨äºå¯è§†åŒ–
        sample_size = min(1000, X_val_combined.shape[0])  # å–1000ä¸ªæ ·æœ¬ç”¨äºå¯è§†åŒ–
        sample_preds = best_model.predict(X_val_combined[:sample_size])
        if isinstance(sample_preds, (list, tuple)) and len(sample_preds) == 2:
            sample_cls_pred, sample_reg_pred = sample_preds
        else:
            sample_cls_pred = sample_preds
            sample_reg_pred = None
        plot_prediction_maps(X_val_combined[:sample_size], sample_cls_pred, sample_reg_pred, val_ids[:sample_size],
                             data_val.iloc[:sample_size])
        # è‡ªåŠ¨ä¿å­˜äº¤äº’ç‰¹å¾åå’Œæœ€ç»ˆç‰¹å¾é¡ºåº
        with open(os.path.join(Config.output_dir, "selected_inter_feats.json"), "w", encoding="utf-8") as f:
            json.dump(selected_inter_feats, f, ensure_ascii=False)
        with open(os.path.join(Config.output_dir, "feature_order.json"), "w", encoding="utf-8") as f:
            # å®‰å…¨è·å–ç‰¹å¾åˆ—å
            if hasattr(X_train_final, 'columns'):
                feature_columns = list(X_train_final.columns)
            else:
                # å¦‚æœX_train_finalæ˜¯numpyæ•°ç»„ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾å
                feature_columns = original_feature_names[:X_train_final.shape[1]]
            json.dump(feature_columns, f, ensure_ascii=False)
        # 17. ç”Ÿæˆåˆ†æå›¾è¡¨
        print("\n17. ç”Ÿæˆåˆ†æå›¾è¡¨...")

        # 17.1 è®­ç»ƒå†å²å›¾è¡¨
        plot_training_history(history)

        # 17.2 æ··æ·†çŸ©é˜µ
        # ä¿®å¤ï¼šå¤šè¾“å‡ºæ¨¡å‹predictè¿”å›(cls_pred, reg_pred)ï¼Œä¸èƒ½ç›´æ¥argmax
        preds = best_model.predict(X_val_combined)
        if isinstance(preds, (list, tuple)) and len(preds) == 2:
            cls_pred, reg_pred = preds
        else:
            # å…¼å®¹æ€§å…œåº•
            cls_pred = preds
            reg_pred = None
        y_cls_pred = np.argmax(cls_pred, axis=1)
        plot_confusion_matrix(y_cls_val_sample, y_cls_pred, classes=['ä¸é€‚åˆ', 'é€‚åˆ'])

        # 17.3 å›å½’æ•£ç‚¹å›¾
        if reg_pred is not None:
            y_reg_pred = reg_pred
        else:
            # å…¼å®¹æ€§å…œåº• - é¿å…é‡å¤predictè°ƒç”¨
            y_reg_pred = preds[1] if isinstance(preds, (list, tuple)) and len(preds) > 1 else preds
        plot_regression_scatter(y_reg_val_sample, y_reg_pred, ['per_mu', 'per_qu'])

        # 17.4 ç‰¹å¾é‡è¦æ€§å›¾
        plot_feature_importance(xgb_model, feature_names)

        # 17.5 ç”Ÿæˆevaluation_report.json
        evaluate_model(best_model, X_val_combined, y_cls_val_sample, y_reg_val_sample)

        print("\n=== é¢„æµ‹ç¨‹åºè¿è¡Œå®Œæˆ! ===")
        print(f"GPUé…ç½®: {num_gpus}ä¸ªGPU, ç­–ç•¥: {type(strategy).__name__ if strategy else 'None'}")
        print(f"ç»“æœå·²ä¿å­˜è‡³: {Config.result_file}")
        print(f"è®­ç»ƒå†å²å·²ä¿å­˜è‡³: {history_file}")

        return True

    except Exception as e:
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        if hasattr(e, '__traceback__') and e.__traceback__ is not None:
            print(f"é”™è¯¯ä½ç½®: {e.__traceback__.tb_frame.f_code.co_filename}:{e.__traceback__.tb_lineno}")
        raise


def train_final_model_conservative(X_train, y_cls_train, y_reg_train, X_val, y_cls_val, y_reg_val, strategy=None):
    """ä½¿ç”¨ä¿å®ˆå‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œé¿å…NaNé—®é¢˜"""
    print("\nä½¿ç”¨ä¿å®ˆå‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")

    # æ•°æ®æ¸…ç†ï¼šç¡®ä¿æ²¡æœ‰NaNæˆ–Inf
    print("ä¿å®ˆè®­ç»ƒæ•°æ®æ¸…ç†...")

    # æ¸…ç†ç‰¹å¾æ•°æ®
    if hasattr(X_train, 'values'):
        X_train_values = X_train.values
    else:
        X_train_values = X_train

    if hasattr(X_val, 'values'):
        X_val_values = X_val.values
    else:
        X_val_values = X_val

    # æ¸…ç†NaNå’ŒInf
    X_train_values = np.nan_to_num(X_train_values, nan=0.0, posinf=0.0, neginf=0.0)
    X_val_values = np.nan_to_num(X_val_values, nan=0.0, posinf=0.0, neginf=0.0)

    if hasattr(X_train, 'values'):
        X_train = pd.DataFrame(X_train_values, columns=X_train.columns, index=X_train.index)
    else:
        X_train = X_train_values

    if hasattr(X_val, 'values'):
        X_val = pd.DataFrame(X_val_values, columns=X_val.columns, index=X_val.index)
    else:
        X_val = X_val_values

    # æ¸…ç†æ ‡ç­¾æ•°æ®
    if hasattr(y_reg_train, 'shape'):
        y_reg_train = np.nan_to_num(y_reg_train, nan=0.0, posinf=0.0, neginf=0.0)

    if hasattr(y_reg_val, 'shape'):
        y_reg_val = np.nan_to_num(y_reg_val, nan=0.0, posinf=0.0, neginf=0.0)

    print("æ•°æ®æ¸…ç†å®Œæˆ")

    # ä½¿ç”¨éå¸¸ä¿å®ˆçš„å‚æ•°
    conservative_params = {
        'lr': 0.0001,  # å¾ˆå°çš„å­¦ä¹ ç‡
        'neurons1': 64,  # è¾ƒå°çš„ç½‘ç»œ
        'neurons2': 32,
        'dropout_rate': 0.1,
        'batch_size': 32,
        'attention_units': 16,
        'l1_lambda': 1e-5,  # æ·»åŠ ç¼ºå¤±çš„l1_lambdaå‚æ•°
        'l2_lambda': 1e-4,
        'optimizer_type': 'adam',
        'activation': 'relu'
    }

    print(f"ä¿å®ˆå‚æ•°: {conservative_params}")

    # æ„å»ºæ¨¡å‹
    num_classes = len(np.unique(y_cls_train))
    model = build_hybrid_model(X_train.shape[1], num_classes, conservative_params, strategy)

    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=5,  # å‡å°‘patienceï¼ŒåŠ å¿«æ—©åœ
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            os.path.join(Config.final_model_dir, 'best_hybrid_multioutput_AA.h5'),
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        )
    ]

    # è®­ç»ƒæ¨¡å‹
    history = model.fit(
        X_train,
        {
            'classification': y_cls_train,
            'output_clipped': y_reg_train
        },
        validation_data=(
            X_val,
            {
                'classification': y_cls_val,
                'output_clipped': y_reg_val
            }
        ),
        epochs=30,  # è¿›ä¸€æ­¥å‡å°‘epochsï¼ŒåŠ å¿«è®­ç»ƒ
        batch_size=conservative_params['batch_size'],
        callbacks=callbacks,
        verbose=1
    )

    return model, history


print('=== è„šæœ¬å·²å¯åŠ¨ ===')


def set_weather_row_range(start_row, end_row):
    """
    è®¾ç½®æ°”è±¡æ•°æ®è¯»å–çš„è¡Œæ•°èŒƒå›´

    Args:
        start_row (int): èµ·å§‹è¡Œæ•°ï¼ˆ0-basedï¼ŒåŒ…å«æ­¤è¡Œï¼‰
        end_row (int): ç»“æŸè¡Œæ•°ï¼ˆ0-basedï¼Œä¸åŒ…å«æ­¤è¡Œï¼ŒNoneè¡¨ç¤ºåˆ°æ–‡ä»¶æœ«å°¾ï¼‰
    """
    Config.weather_start_row = start_row
    Config.weather_end_row = end_row
    Config.use_weather_row_range = True
    print(f"âœ… å·²è®¾ç½®æ°”è±¡æ•°æ®è¡Œæ•°èŒƒå›´: èµ·å§‹è¡Œ={start_row}, ç»“æŸè¡Œ={end_row}")


def get_model_expected_features(model):
    """
    åŠ¨æ€è·å–æ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡
    
    Args:
        model: åŠ è½½çš„Kerasæ¨¡å‹
        
    Returns:
        int: æœŸæœ›çš„ç‰¹å¾æ•°é‡
    """
    try:
        # æ–¹æ³•1ï¼šä»æ¨¡å‹è¾“å…¥å±‚è·å–
        if hasattr(model, 'input_shape'):
            if isinstance(model.input_shape, list):
                # å¤šè¾“å…¥æ¨¡å‹
                input_shape = model.input_shape[0]
            else:
                # å•è¾“å…¥æ¨¡å‹
                input_shape = model.input_shape
            
            # input_shapeæ ¼å¼é€šå¸¸æ˜¯ (batch_size, feature_count)
            if len(input_shape) >= 2:
                expected_features = input_shape[1]
                print(f"ä»æ¨¡å‹è¾“å…¥å±‚æ£€æµ‹åˆ°ç‰¹å¾æ•°é‡: {expected_features}")
                return expected_features
        
        # æ–¹æ³•2ï¼šä»ç¬¬ä¸€å±‚è·å–
        if model.layers:
            first_layer = model.layers[0]
            if hasattr(first_layer, 'input_shape'):
                if isinstance(first_layer.input_shape, list):
                    input_shape = first_layer.input_shape[0]
                else:
                    input_shape = first_layer.input_shape
                
                if len(input_shape) >= 2:
                    expected_features = input_shape[1]
                    print(f"ä»æ¨¡å‹ç¬¬ä¸€å±‚æ£€æµ‹åˆ°ç‰¹å¾æ•°é‡: {expected_features}")
                    return expected_features
        
        # æ–¹æ³•3ï¼šä»æ¨¡å‹é…ç½®è·å–
        if hasattr(model, 'get_config'):
            try:
                config = model.get_config()
                if 'layers' in config and config['layers']:
                    first_layer_config = config['layers'][0]
                    if 'config' in first_layer_config and 'input_shape' in first_layer_config['config']:
                        input_shape = first_layer_config['config']['input_shape']
                        if len(input_shape) >= 2:
                            expected_features = input_shape[1]
                            print(f"ä»æ¨¡å‹é…ç½®æ£€æµ‹åˆ°ç‰¹å¾æ•°é‡: {expected_features}")
                            return expected_features
            except Exception as e:
                print(f"ä»æ¨¡å‹é…ç½®æ£€æµ‹ç‰¹å¾æ•°é‡å¤±è´¥: {e}")
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
        print("âš ï¸ æ— æ³•åŠ¨æ€æ£€æµ‹æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼")
        return None
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€æ£€æµ‹ç‰¹å¾æ•°é‡æ—¶å‡ºé”™: {e}")
        return None


def create_model_info_file(model_path, expected_features=None):
    """
    åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶ï¼Œè®°å½•æ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        expected_features: æœŸæœ›çš„ç‰¹å¾æ•°é‡ï¼ˆå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨æ£€æµ‹ï¼‰
    """
    try:
        model_info_dir = os.path.dirname(model_path)
        model_info_file = os.path.join(model_info_dir, "model_info.json")
        
        info = {
            "model_path": model_path,
            "created_time": datetime.now().isoformat(),
            "input_features": expected_features,
            "detection_method": "automatic" if expected_features is None else "manual"
        }
        
        with open(model_info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å·²ä¿å­˜: {model_info_file}")
        return model_info_file
        
    except Exception as e:
        print(f"âš ï¸ åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
        return None


def load_model_info_file(model_path):
    """
    åŠ è½½æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        
    Returns:
        dict: æ¨¡å‹ä¿¡æ¯ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    try:
        model_info_dir = os.path.dirname(model_path)
        model_info_file = os.path.join(model_info_dir, "model_info.json")
        
        if os.path.exists(model_info_file):
            with open(model_info_file, 'r', encoding='utf-8') as f:
                info = json.load(f)
            print(f"âœ… åŠ è½½æ¨¡å‹ä¿¡æ¯æ–‡ä»¶: {model_info_file}")
            return info
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹ä¿¡æ¯æ–‡ä»¶")
            return None
            
    except Exception as e:
        print(f"âš ï¸ åŠ è½½æ¨¡å‹ä¿¡æ¯æ–‡ä»¶å¤±è´¥: {e}")
        return None


def determine_model_features(model_path, model_obj):
    """
    ç¡®å®šæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡ï¼ˆç»¼åˆåˆ©ç”¨å¤šç§æ–¹æ³•ï¼‰
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_obj: åŠ è½½çš„æ¨¡å‹å¯¹è±¡
        
    Returns:
        int: æœŸæœ›çš„ç‰¹å¾æ•°é‡
    """
    print("\nğŸ” ç¡®å®šæ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡...")
    
    # æ–¹æ³•1ï¼šå°è¯•åŠ è½½å·²æœ‰çš„æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
    model_info = load_model_info_file(model_path)
    if model_info and 'input_features' in model_info:
        expected_features = model_info['input_features']
        if expected_features is not None:
            print(f"âœ… ä»æ¨¡å‹ä¿¡æ¯æ–‡ä»¶è·å–ç‰¹å¾æ•°é‡: {expected_features}")
            return expected_features
    
    # æ–¹æ³•2ï¼šåŠ¨æ€ detect from model
    detected_features = get_model_expected_features(model_obj)
    if detected_features is not None:
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç‰¹å¾æ•°é‡: {detected_features}")
        
        # ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        create_model_info_file(model_path, detected_features)
        return detected_features
    
    # æ–¹æ³•3ï¼šä»æ–‡ä»¶åæˆ–é…ç½®æ¨æ–­ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    print("âš ï¸ ä½¿ç”¨åŸºäºæ–‡ä»¶åçš„æ¨æ–­...")
    
    # æ ¹æ®è®­ç»ƒè„šæœ¬çš„ç‰¹å¾ç”Ÿæˆé€»è¾‘æ¨æ–­ï¼š
    # åŸå§‹ç‰¹å¾(~26) + PCAç‰¹å¾(~13) + äº¤äº’ç‰¹å¾(~26) â‰ˆ 65
    # + XGBoostå¶å­ç‰¹å¾(~39) â‰ˆ 104ï¼Œä½†å®é™…å¯èƒ½æ˜¯89
    inferred_features = 89  # åŸºäºæœ€æ–°çš„è®­ç»ƒç»“æœ
    
    print(f"â„¹ï¸ æ¨æ–­ç‰¹å¾æ•°é‡: {inferred_features}")
    
    # éªŒè¯æ¨æ–­ç»“æœ
    try:
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•è¾“å…¥æ¥éªŒè¯
        test_input = np.random.random((1, inferred_features))
        model_obj.predict(test_input, verbose=0)
        print(f"âœ… ç‰¹å¾æ•°é‡æ¨æ–­éªŒè¯æˆåŠŸ: {inferred_features}")
        
        # ä¿å­˜éªŒè¯ç»“æœ
        create_model_info_file(model_path, inferred_features)
        return inferred_features
        
    except Exception as e:
        print(f"âŒ ç‰¹å¾æ•°é‡æ¨æ–­éªŒè¯å¤±è´¥: {e}")
        print("å¯èƒ½éœ€è¦æ‰‹åŠ¨æŒ‡å®šç‰¹å¾æ•°é‡")
        
        # å°è¯•ä¸€äº›å¸¸è§ç‰¹å¾æ•°é‡çš„å¯èƒ½æ€§
        common_features = [65, 78, 85, 89, 91, 95, 100, 104, 110, 113]
        for features in common_features:
            try:
                test_input = np.random.random((1, features))
                model_obj.predict(test_input, verbose=0)
                print(f"âœ… æ‰¾åˆ°åŒ¹é…çš„ç‰¹å¾æ•°é‡: {features}")
                create_model_info_file(model_path, features)
                return features
            except:
                continue
        
        raise ValueError(f"æ— æ³•ç¡®å®šæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®Œæ•´æ€§")


def detect_and_adapt_model_features(prediction_data, feature_order_file, scaler_file):
    """
    æ£€æµ‹å¹¶é€‚é…ä¸åŒæ¨¡å‹çš„ç‰¹å¾æ ¼å¼
    
    Args:
        prediction_data: é¢„æµ‹æ•°æ®
        feature_order_file: feature_order.jsonæ–‡ä»¶è·¯å¾„
        scaler_file: scaleræ–‡ä»¶è·¯å¾„
        
    Returns:
        tuple: (adapted_feature_order, is_conversion_needed)
    """
    print("ğŸ” æ£€æµ‹æ¨¡å‹ç‰¹å¾æ ¼å¼...")
    
    # åŠ è½½ç‰¹å¾é¡ºåº
    with open(feature_order_file, 'r', encoding='utf-8') as f:
        loaded_feature_order = json.load(f)
    
    print(f"   åŸå§‹ç‰¹å¾é¡ºåºæ–‡ä»¶ä¸­çš„æ ¼å¼: {loaded_feature_order}")
    
    # æ£€æµ‹ç‰¹å¾é¡ºåºæ ¼å¼
    has_numeric_order = any(str(col).isdigit() for col in loaded_feature_order)
    has_name_order = any(not str(col).isdigit() for col in loaded_feature_order)
    
    print(f"   åŒ…å«æ•°å­—ç´¢å¼•: {has_numeric_order}")
    print(f"   åŒ…å«ç‰¹å¾åç§°: {has_name_order}")
    
    # æ£€æµ‹é¢„æµ‹æ•°æ®æ ¼å¼
    sample_data = prediction_data.head(1)
    feature_cols = [col for col in sample_data.columns 
                   if col not in Config.exclude_columns and not col.startswith('pca_')]
    
    has_original_features = any(not col.isdigit() for col in feature_cols)
    print(f"   é¢„æµ‹æ•°æ®åŒ…å«åŸå§‹ç‰¹å¾å: {has_original_features}")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦é€‚é…
    needs_adaptation = has_numeric_order and has_original_features
    print(f"ğŸ”§ éœ€è¦ç‰¹å¾é€‚é…: {needs_adaptation}")
    
    return loaded_feature_order, needs_adaptation


def main(test_mode=False, test_size=10000, weather_start_row=None, weather_end_row=None, model_name=None):
    """
    ä¸»å‡½æ•° - çº¯é¢„æµ‹ç¨‹åºå…¥å£
    ä½¿ç”¨ç°æœ‰æ¨¡å‹å¯¹é¢„æµ‹æ°”è±¡æ•°æ®è¿›è¡Œäº§é‡é¢„æµ‹

    Args:
        test_mode (bool): æ˜¯å¦å¯ç”¨å°å‹æµ‹è¯•æ¨¡å¼
        test_size (int): æµ‹è¯•æ¨¡å¼ä¸‹çš„æ•°æ®é‡é™åˆ¶
        weather_start_row (int, optional): æ°”è±¡æ•°æ®èµ·å§‹è¡Œæ•°
        weather_end_row (int, optional): æ°”è±¡æ•°æ®ç»“æŸè¡Œæ•°
        model_name (str, optional): æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print('=== è¿›å…¥é¢„æµ‹æµç¨‹ ===')

    # è®¾ç½®æ¨¡å‹åç§°
    if model_name:
        print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šæ¨¡å‹: {model_name}")
        Config.set_model_name(model_name)
    else:
        print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤æ¨¡å‹: {Config.get_model_name()}")

    if test_mode:
        print(f"ğŸ”¬ å°å‹æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ•°æ®é‡ä¸º {test_size:,} è¡Œ")

    # è®¾ç½®æ°”è±¡æ•°æ®è¡Œæ•°èŒƒå›´ï¼ˆå¦‚æœæä¾›äº†å‚æ•°ï¼‰
    if weather_start_row is not None or weather_end_row is not None:
        set_weather_row_range(weather_start_row, weather_end_row)

    try:
        # 1. éªŒè¯é…ç½®
        print("\n1. éªŒè¯é…ç½®...")
        if not Config.validate_params() or not Config.validate_paths():
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")

        # 2. æ£€æŸ¥ç°æœ‰æ¨¡å‹æ˜¯å¦å­˜åœ¨
        print("\n2. æ£€æŸ¥ç°æœ‰æ¨¡å‹...")
        if not os.path.exists(Config.final_model_file):
            print(f"âŒ ç°æœ‰æ¨¡å‹ä¸å­˜åœ¨: {Config.final_model_file}")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒæ¨¡å¼æˆ–æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„")
            return False

        print(f"âœ… æ‰¾åˆ°ç°æœ‰æ¨¡å‹: {Config.final_model_file}")

        # 3. åˆ†ææ¨¡å‹ç‰¹å¾æ˜ å°„å…³ç³»
        print("\n3. åˆ†ææ¨¡å‹ç‰¹å¾æ˜ å°„å…³ç³»...")
        mapping_info = analyze_model_feature_mapping(Config.output_dir)
        
        # 4. åŠ è½½é¢„æµ‹æ•°æ®ï¼ˆä½¿ç”¨ä¸è®­ç»ƒä»£ç ç›¸åŒçš„æ•°æ®é¢„å¤„ç†ï¼‰
        print("\n4. åŠ è½½å’Œé¢„å¤„ç†é¢„æµ‹æ•°æ®...")
        prediction_data = load_prediction_data(test_mode=test_mode, test_size=test_size,
                                               weather_start_row=Config.weather_start_row,
                                               weather_end_row=Config.weather_end_row)

        # 5. åŠ è½½ç°æœ‰æ¨¡å‹å’Œé¢„å¤„ç†å™¨
        print("\n5. åŠ è½½ç°æœ‰æ¨¡å‹å’Œé¢„å¤„ç†å™¨...")
        best_model = load_model_with_custom_objects(Config.final_model_file)
        scaler = joblib.load(Config.scaler_file)
        pca = joblib.load(os.path.join(Config.scaler_dir, 'pca_AA.pkl'))

        # åŠ¨æ€ç¡®å®šæ¨¡å‹æœŸæœ›çš„ç‰¹å¾æ•°é‡
        expected_features = determine_model_features(Config.final_model_file, best_model)

        # æ™ºèƒ½æ£€æµ‹å’Œé€‚é…ç‰¹å¾æ ¼å¼
        feature_order_file = os.path.join(Config.output_dir, 'feature_order.json')
        feature_order, needs_adaptation = detect_and_adapt_model_features(prediction_data, feature_order_file, Config.scaler_file)
        
        if needs_adaptation:
            print("âœ… æ£€æµ‹åˆ°éœ€è¦è¿›è¡Œç‰¹å¾é€‚é…çš„æ¨¡å‹")
        else:
            print("âœ… æ¨¡å‹ç‰¹å¾æ ¼å¼å…¼å®¹")

        # 6. æ•°æ®é¢„å¤„ç†ï¼ˆå®Œå…¨æŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼‰
        print("\n6. æ•°æ®é¢„å¤„ç†...")
        X_processed = preprocess_prediction_data(prediction_data, scaler, pca, feature_order, expected_features, mapping_info=mapping_info)
        
        # 7. ğŸ”§ æœ€ç»ˆç‰¹å¾æ•°é‡ä¿®æ­£ - ç¡®ä¿ä¸æ¨¡å‹å®Œå…¨åŒ¹é…
        if expected_features and X_processed.shape[1] != expected_features:
            print(f"ğŸ”§ æœ€ç»ˆç‰¹å¾æ•°é‡ä¿®æ­£: {X_processed.shape[1]} -> {expected_features}")
            if X_processed.shape[1] > expected_features:
                X_processed = X_processed.iloc[:, :expected_features]
                print(f"   æˆªå–å‰ {expected_features} ä¸ªç‰¹å¾")
            else:
                # æ·»åŠ ç¼ºå¤±ç‰¹å¾
                missing_features = expected_features - X_processed.shape[1]
                for i in range(missing_features):
                    X_processed[f'final_missing_feature_{i}'] = 0
                print(f"   æ·»åŠ  {missing_features} ä¸ªç¼ºå¤±ç‰¹å¾")
            print(f"âœ… æœ€ç»ˆç‰¹å¾æ•°é‡: {X_processed.shape[1]}")

        # 8. æ¨¡å‹é¢„æµ‹ï¼ˆåˆ†å—ä¿å­˜ï¼‰
        print("\n8. æ‰§è¡Œæ¨¡å‹é¢„æµ‹...")
        saved_files = predict_with_processed_data(prediction_data, X_processed, best_model, save_chunk_size=5000000)

        # 9. é¢„æµ‹å®Œæˆç»Ÿè®¡
        print("\n9. é¢„æµ‹å®Œæˆç»Ÿè®¡...")
        print(f"\nâœ… é¢„æµ‹å®Œæˆï¼")
        print(f"å…±ä¿å­˜äº† {len(saved_files)} ä¸ªé¢„æµ‹ç»“æœæ–‡ä»¶:")
        return True

    except Exception as e:
        print(f"âŒ é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False


def load_prediction_data(test_mode=False, test_size=10000, weather_start_row=None, weather_end_row=None):
    """åŠ è½½é¢„æµ‹æ°”è±¡æ•°æ®"""
    print("åŠ è½½é¢„æµ‹æ°”è±¡æ•°æ®...")

    # é¢„æµ‹æ°”è±¡æ•°æ®é…ç½®ï¼ˆä½¿ç”¨é¢„æµ‹æ°”è±¡æ•°æ®ï¼‰
    prediction_weather_folder = r"C:\Users\Administrator\Desktop\2. B774æ¨¡å‹è„šæœ¬\2-é€‚å®œåº¦äº§é‡é¢„æµ‹æ¨¡å‹\train+text_csvæ•°æ®\é¢„æµ‹æ°”è±¡"
    soil_data_csv = Config.soil_data_csv

    # åŠ è½½é¢„æµ‹æ°”è±¡æ•°æ®ï¼ˆæ”¯æŒè¡Œæ•°èŒƒå›´ï¼‰
    weather_data = load_weather_data_from_folder(prediction_weather_folder,
                                                 start_row=weather_start_row,
                                                 end_row=weather_end_row)
    print(f"é¢„æµ‹æ°”è±¡æ•°æ®å½¢çŠ¶: {weather_data.shape}")

    # å°å‹æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ°”è±¡æ•°æ®é‡
    if test_mode:
        print(f"ğŸ”¬ å°å‹æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶æ°”è±¡æ•°æ®ä¸º {test_size:,} è¡Œ")
        weather_data = weather_data.head(test_size)
        print(f"é™åˆ¶åæ°”è±¡æ•°æ®å½¢çŠ¶: {weather_data.shape}")

    # åŠ è½½åœŸå£¤æ•°æ®
    soil_data = pd.read_csv(soil_data_csv, encoding='utf-8')
    soil_data.columns = soil_data.columns.str.strip().str.lower()
    soil_data = soil_data.dropna()
    print(f"åœŸå£¤æ•°æ®å½¢çŠ¶: {soil_data.shape}")

    # å°å‹æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶åœŸå£¤æ•°æ®é‡
    if test_mode:
        print(f"ğŸ”¬ å°å‹æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶åœŸå£¤æ•°æ®ä¸º {test_size:,} è¡Œ")
        soil_data = soil_data.head(test_size)
        print(f"é™åˆ¶ååœŸå£¤æ•°æ®å½¢çŠ¶: {soil_data.shape}")

    # åˆå¹¶æ•°æ®ï¼ˆæŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼‰
    # é¦–å…ˆé‡å‘½åæ°”è±¡æ•°æ®åˆ—å
    print("é‡å‘½åæ°”è±¡æ•°æ®åˆ—å...")
    weather_data_renamed = weather_data.copy()

    # é‡å‘½ååæ ‡åˆ—
    if 'Lon' in weather_data_renamed.columns:
        weather_data_renamed = weather_data_renamed.rename(columns={'Lon': 'x'})
    if 'Lat' in weather_data_renamed.columns:
        weather_data_renamed = weather_data_renamed.rename(columns={'Lat': 'y'})
    if 'YYYY' in weather_data_renamed.columns:
        weather_data_renamed = weather_data_renamed.rename(columns={'YYYY': 'yyyy'})

    print(f"é‡å‘½ååæ°”è±¡æ•°æ®åˆ—å: {weather_data_renamed.columns.tolist()[:10]}...")

    # ä¸ºæ°”è±¡æ•°æ®æ·»åŠ right_å‰ç¼€
    weather_pref = weather_data_renamed.add_prefix('right_')
    print(f"æ·»åŠ å‰ç¼€åæ°”è±¡æ•°æ®åˆ—å: {weather_pref.columns.tolist()[:10]}...")

    # åˆ›å»ºè™šæ‹Ÿäº§å“æ•°æ®ï¼ˆç”¨äºé¢„æµ‹ï¼‰
    product_data = soil_data[['x', 'y']].copy()
    if 'yyyy' in weather_data_renamed.columns:
        product_data['yyyy'] = weather_data_renamed['yyyy'].iloc[0]
    else:
        product_data['yyyy'] = 2015  # é»˜è®¤å¹´ä»½

    print(f"è™šæ‹Ÿäº§å“æ•°æ®å½¢çŠ¶: {product_data.shape}")
    print(f"è™šæ‹Ÿäº§å“æ•°æ®åˆ—å: {product_data.columns.tolist()}")

    # ç²¾ç¡®é”®åˆå¹¶
    merged_data = pd.merge(
        product_data,
        weather_pref,
        left_on=['x', 'y', 'yyyy'],
        right_on=['right_x', 'right_y', 'right_yyyy'],
        how='inner'
    )

    if merged_data.empty:
        print("ç²¾ç¡®é”®åˆå¹¶ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°ç©ºé—´è¿‘é‚»åˆå¹¶...")
        merged_data = spatial_temporal_merge(product_data, weather_data_renamed, xy_cols=['x', 'y'], time_col='yyyy',
                                             tolerance=50000)

    # ä¸åœŸå£¤æ•°æ®åˆå¹¶
    merged_data = spatial_merge(merged_data, soil_data, on=['x', 'y'], tolerance=50000)

    # ç»Ÿä¸€åˆ—åä¸ºå°å†™
    merged_data.columns = merged_data.columns.str.strip().str.lower()

    # å¤„ç†é‡å¤åˆ—åé—®é¢˜
    print("æ£€æŸ¥å¹¶å¤„ç†é‡å¤åˆ—å...")
    original_columns = merged_data.columns.tolist()
    seen_columns = set()
    new_columns = []

    for col in original_columns:
        if col in seen_columns:
            # ä¸ºé‡å¤åˆ—æ·»åŠ åç¼€
            counter = 1
            new_col = f"{col}_{counter}"
            while new_col in seen_columns:
                counter += 1
                new_col = f"{col}_{counter}"
            new_columns.append(new_col)
            seen_columns.add(new_col)
            print(f"é‡å‘½åé‡å¤åˆ—: {col} -> {new_col}")
        else:
            new_columns.append(col)
            seen_columns.add(col)

    merged_data.columns = new_columns

    # æ·»åŠ ç¼ºå¤±çš„ç‰¹å¾åˆ—
    if 'right_x_original' not in merged_data.columns and 'right_x' in merged_data.columns:
        merged_data['right_x_original'] = merged_data['right_x']
        print("æ·»åŠ  right_x_original åˆ—")

    if 'right_y_original' not in merged_data.columns and 'right_y' in merged_data.columns:
        merged_data['right_y_original'] = merged_data['right_y']
        print("æ·»åŠ  right_y_original åˆ—")

    print(f"æœ€ç»ˆé¢„æµ‹æ•°æ®å½¢çŠ¶: {merged_data.shape}")
    return merged_data


def _needs_feature_name_conversion(actual_columns, expected_feature_order):
    """
    æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹å¾åç§°è½¬æ¢
    
    Args:
        actual_columns: é¢„æµ‹æ•°æ®çš„å®é™…åˆ—å
        expected_feature_order: æœŸæœ›çš„ç‰¹å¾é¡ºåº
        
    Returns:
        bool: æ˜¯å¦éœ€è¦è½¬æ¢
    """
    # æ£€æŸ¥æœŸæœ›ç‰¹å¾æ˜¯å¦ä¸»è¦æ˜¯æ•°å­—å­—ç¬¦ä¸²ï¼ˆè¡¨ç¤ºè®­ç»ƒæ—¶ä¿å­˜çš„æ•°å­—ç´¢å¼•ï¼‰
    expect_numeric_features = any(col.isdigit() for col in expected_feature_order)
    
    # æ£€æŸ¥å®é™…åˆ—æ˜¯å¦ä¸»è¦æ˜¯åŸå§‹ç‰¹å¾åï¼ˆéæ•°å­—å­—ç¬¦ä¸²ï¼‰
    have_original_features = any(not col.isdigit() and not col.startswith('pca_') 
                               for col in actual_columns)
    
    # å¦‚æœæœŸæœ›æ•°å­—ç´¢å¼•ä½†æœ‰çš„æ˜¯åŸå§‹ç‰¹å¾åï¼Œåˆ™éœ€è¦è½¬æ¢
    return expect_numeric_features and have_original_features


def _convert_feature_names_to_match_scaler(prediction_data, expected_feature_order, scaler, mapping_info=None):
    """
    å°†æ•°å­—ç´¢å¼•é‡æ–°æ˜ å°„ä¸ºåŸå§‹ç‰¹å¾åä»¥åŒ¹é…scaler
    
    Args:
        prediction_data: åŸå§‹é¢„æµ‹æ•°æ®DataFrameï¼ˆåŒ…å«åŸå§‹ç‰¹å¾åï¼‰
        expected_feature_order: ç‰¹å¾é¡ºåºï¼ˆæ•°å­—ç´¢å¼•å­—ç¬¦ä¸²å¦‚"0", "1", "2"ï¼‰
        scaler: sklearn scalerå¯¹è±¡ï¼ˆæœŸæœ›åŸå§‹ç‰¹å¾åï¼‰
        mapping_info: æ¨¡å‹æ˜ å°„ä¿¡æ¯å­—å…¸
        
    Returns:
        pd.DataFrame: è½¬æ¢åçš„æ•°æ®ï¼ˆç‰¹å¾åä¸ºåŸå§‹ç‰¹å¾åï¼‰
    """
    print(f"ğŸ”§ æ™ºèƒ½ç‰¹å¾è½¬æ¢:")
    print(f"   è¾“å…¥ç‰¹å¾: {list(prediction_data.columns)}")
    print(f"   æœŸæœ›ç‰¹å¾: {expected_feature_order}")
    
    # è·å–scaleræœŸæœ›çš„ç‰¹å¾åç§°
    scaler_feature_names = []
    if hasattr(scaler, 'feature_names_in_'):
        scaler_feature_names = list(scaler.feature_names_in_)
        print(f"   ScaleræœŸæœ›ç‰¹å¾å: {scaler_feature_names}")
    
    # è·å–é¢„æµ‹æ•°æ®ä¸­çš„åŸå§‹ç‰¹å¾åˆ—è¡¨
    original_features = [col for col in prediction_data.columns 
                        if not col.isdigit() and not col.startswith('pca_') 
                        and col not in Config.exclude_columns]
    
    print(f"   é¢„æµ‹æ•°æ®åŸå§‹ç‰¹å¾: {original_features}")
    
    # åˆ›å»ºè½¬æ¢åçš„DataFrame
    converted_data = pd.DataFrame(index=prediction_data.index)
    
    # å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨scaleræœŸæœ›çš„ç‰¹å¾åç§°ï¼Œç¡®ä¿å®Œå…¨åŒ¹é…
    for scaler_feature_name in scaler_feature_names:
        if scaler_feature_name in prediction_data.columns:
            converted_data[scaler_feature_name] = prediction_data[scaler_feature_name]
            print(f"   âœ… ç›´æ¥æ˜ å°„: {scaler_feature_name}")
        else:
            # å¦‚æœåœ¨é¢„æµ‹æ•°æ®ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•æ™ºèƒ½æ˜ å°„
            mapped_feature_name, mapped_value = _smart_feature_mapping(scaler_feature_name, prediction_data, expected_feature_order, original_features, mapping_info)
            if isinstance(mapped_value, (pd.Series, pd.DataFrame)):
                converted_data[scaler_feature_name] = mapped_value
                print(f"   ğŸ”„ æ™ºèƒ½æ˜ å°„: {scaler_feature_name}")
            elif mapped_value != 0:
                converted_data[scaler_feature_name] = mapped_value
                print(f"   ğŸ”„ æ™ºèƒ½æ˜ å°„: {scaler_feature_name}")
            else:
                converted_data[scaler_feature_name] = 0
                print(f"   âŒ ç¼ºå¤±æ˜ å°„: {scaler_feature_name} = 0")
    
    print(f"ğŸ”§ è½¬æ¢å®Œæˆï¼Œç‰¹å¾æ•°: {len(converted_data.columns)}")
    print(f"ğŸ”§ è½¬æ¢åçš„ç‰¹å¾åç§°: {list(converted_data.columns)}")
    
    # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
    nan_count = converted_data.isnull().sum().sum()
    if nan_count > 0:
        print(f"âš ï¸ å‘ç° {nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……...")
        converted_data = converted_data.fillna(0)
    
    # ç¡®ä¿ç‰¹å¾æ•°é‡ä¸scalerå®Œå…¨åŒ¹é…
    expected_scaler_count = len(scaler_feature_names) if scaler_feature_names else 0
    if len(converted_data.columns) != expected_scaler_count:
        print(f"âš ï¸ ç‰¹å¾æ•°é‡ä¸åŒ¹é…: è½¬æ¢å{len(converted_data.columns)} vs æœŸæœ›{expected_scaler_count}")
        
        # åˆ›å»ºä¸scalerå®Œå…¨åŒ¹é…çš„æ•°æ®æ¡†
        matched_data = pd.DataFrame(index=converted_data.index)
        
        # æŒ‰scaleræœŸæœ›çš„ç‰¹å¾é¡ºåºå¡«å……æ•°æ®
        for scaler_feature in scaler_feature_names:
            if scaler_feature in converted_data.columns:
                matched_data[scaler_feature] = converted_data[scaler_feature]
            else:
                matched_data[scaler_feature] = 0
                print(f"   âš ï¸ è¡¥å……ç¼ºå¤±ç‰¹å¾: {scaler_feature}")
        
        print(f"ğŸ”§ ç‰¹å¾æ•°é‡è°ƒæ•´ä¸º: {len(matched_data.columns)}")
        return matched_data
    
    return converted_data


def analyze_model_feature_mapping(model_path):
    """
    åˆ†ææŒ‡å®šæ¨¡å‹è·¯å¾„ä¸‹çš„ç‰¹å¾æ˜ å°„å…³ç³»
    
    Args:
        model_path: æ¨¡å‹ç›®å½•è·¯å¾„
        
    Returns:
        dict: åŒ…å«æ˜ å°„ä¿¡æ¯çš„å­—å…¸
    """
    mapping_info = {
        'feature_order_type': None,  # 'numeric' æˆ– 'named'
        'scaler_features': [],
        'feature_order': [],
        'mapping_strategy': None
    }
    
    try:
        # 1. æ£€æŸ¥feature_order.jsonæ–‡ä»¶
        feature_order_file = os.path.join(model_path, 'feature_order.json')
        if os.path.exists(feature_order_file):
            with open(feature_order_file, 'r', encoding='utf-8') as f:
                feature_order = json.load(f)
            mapping_info['feature_order'] = feature_order
            
            # åˆ¤æ–­ç‰¹å¾é¡ºåºç±»å‹
            if all(str(item).isdigit() for item in feature_order):
                mapping_info['feature_order_type'] = 'numeric'
                mapping_info['mapping_strategy'] = 'numeric_to_named'
            else:
                mapping_info['feature_order_type'] = 'named'
                mapping_info['mapping_strategy'] = 'direct_match'
        
        # 2. æ£€æŸ¥scaleræ–‡ä»¶
        scaler_file = os.path.join(model_path, 'ScalerAA', 'scaler_AA.pkl')
        if os.path.exists(scaler_file):
            scaler = joblib.load(scaler_file)
            if hasattr(scaler, 'feature_names_in_'):
                mapping_info['scaler_features'] = list(scaler.feature_names_in_)
        
        # 3. æ£€æŸ¥ç‰¹å¾é‡è¦æ€§æ–‡ä»¶
        feature_importance_file = os.path.join(model_path, 'feature_importanceAA', 'enhanced_feature_mapping_results.csv')
        if os.path.exists(feature_importance_file):
            try:
                import pandas as pd
                df = pd.read_csv(feature_importance_file)
                if 'feature' in df.columns and 'original_column_name' in df.columns:
                    # åˆ›å»ºç‰¹å¾æ˜ å°„å­—å…¸
                    feature_mapping = dict(zip(df['feature'], df['original_column_name']))
                    mapping_info['feature_mapping'] = feature_mapping
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç‰¹å¾é‡è¦æ€§æ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"ğŸ” æ¨¡å‹ç‰¹å¾æ˜ å°„åˆ†æå®Œæˆ:")
        print(f"   ç‰¹å¾é¡ºåºç±»å‹: {mapping_info['feature_order_type']}")
        print(f"   æ˜ å°„ç­–ç•¥: {mapping_info['mapping_strategy']}")
        print(f"   Scalerç‰¹å¾æ•°é‡: {len(mapping_info['scaler_features'])}")
        
        return mapping_info
        
    except Exception as e:
        print(f"âŒ åˆ†ææ¨¡å‹ç‰¹å¾æ˜ å°„å¤±è´¥: {e}")
        return mapping_info


def _smart_feature_mapping(scaler_feature_name, prediction_data, expected_feature_order, original_features, mapping_info=None):
    """
    æ™ºèƒ½ç‰¹å¾æ˜ å°„ï¼šå°è¯•å°†scaleræœŸæœ›çš„ç‰¹å¾åç§°æ˜ å°„åˆ°é¢„æµ‹æ•°æ®ä¸­çš„ç‰¹å¾
    
    Args:
        scaler_feature_name: scaleræœŸæœ›çš„ç‰¹å¾åç§°
        prediction_data: é¢„æµ‹æ•°æ®DataFrame
        expected_feature_order: ç‰¹å¾é¡ºåºï¼ˆæ•°å­—ç´¢å¼•ï¼‰
        original_features: åŸå§‹ç‰¹å¾åˆ—è¡¨
        mapping_info: æ¨¡å‹æ˜ å°„ä¿¡æ¯å­—å…¸
        
    Returns:
        tuple: (æ˜ å°„åçš„ç‰¹å¾åç§°, æ˜ å°„åçš„æ•°æ®) æˆ– (None, 0) å¦‚æœæ‰¾ä¸åˆ°
    """
    # ç­–ç•¥1ï¼šç›´æ¥åŒ¹é…
    if scaler_feature_name in prediction_data.columns:
        return scaler_feature_name, prediction_data[scaler_feature_name]
    
    # ç­–ç•¥2ï¼šä½¿ç”¨æ˜ å°„ä¿¡æ¯è¿›è¡Œç²¾ç¡®æ˜ å°„
    if mapping_info and 'feature_mapping' in mapping_info:
        feature_mapping = mapping_info['feature_mapping']
        if scaler_feature_name in feature_mapping:
            mapped_feature = feature_mapping[scaler_feature_name]
            if mapped_feature in prediction_data.columns:
                # è¿”å›æ˜ å°„åçš„Seriesï¼Œä½†ä¿æŒscaleræœŸæœ›çš„ç‰¹å¾åç§°
                mapped_series = prediction_data[mapped_feature].copy()
                mapped_series.name = scaler_feature_name
                return scaler_feature_name, mapped_series
    
    # ç­–ç•¥3ï¼šæ¨¡ç³ŠåŒ¹é…ï¼ˆåŸºäºç‰¹å¾åç§°çš„ç›¸ä¼¼æ€§ï¼‰
    # ä¾‹å¦‚ï¼šright_tsun_sum å¯èƒ½å¯¹åº” right_tsun_mean
    base_name = scaler_feature_name.replace('_sum', '').replace('_max', '').replace('_min', '').replace('_mean', '')
    
    # ä¼˜å…ˆå°è¯•æ‰¾åˆ°*_meanç‰ˆæœ¬çš„ç‰¹å¾
    mean_feature_name = base_name + '_mean'
    if mean_feature_name in prediction_data.columns and mean_feature_name != scaler_feature_name:
        mapped_series = prediction_data[mean_feature_name].copy()
        mapped_series.name = scaler_feature_name
        return scaler_feature_name, mapped_series
    
    # å…¶æ¬¡å°è¯•ä»»ä½•åŒ…å«åŸºç¡€åç§°çš„ç‰¹å¾
    for col in prediction_data.columns:
        if base_name in col and col != scaler_feature_name:
            # æ‰¾åˆ°ç›¸ä¼¼çš„ç‰¹å¾ï¼Œä½¿ç”¨è¯¥ç‰¹å¾çš„å€¼
            mapped_series = prediction_data[col].copy()
            mapped_series.name = scaler_feature_name
            return scaler_feature_name, mapped_series
    
    # ç­–ç•¥4ï¼šå¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›0
    return scaler_feature_name, 0


def preprocess_prediction_data(prediction_data, scaler, pca, feature_order, expected_features=None, chunk_size=100000, mapping_info=None):
    """é¢„å¤„ç†é¢„æµ‹æ•°æ®ï¼ˆæŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼Œç»Ÿä¸€ä½¿ç”¨åŠ¨æ€ç‰¹å¾æ•°é‡è°ƒæ•´ï¼‰"""
    print("é¢„å¤„ç†é¢„æµ‹æ•°æ®...")
    print(f"æ•°æ®æ€»é‡: {prediction_data.shape[0]:,} è¡Œ")

    # å¦‚æœæ•°æ®é‡å¤ªå¤§ï¼Œä½¿ç”¨åˆ†å—å¤„ç†
    if prediction_data.shape[0] > chunk_size:
        print(f"æ•°æ®é‡è¿‡å¤§ï¼Œä½¿ç”¨åˆ†å—å¤„ç†ï¼ˆå—å¤§å°: {chunk_size:,} è¡Œï¼‰")
        return preprocess_prediction_data_chunked(prediction_data, scaler, pca, feature_order, expected_features, chunk_size, mapping_info)

    # 1. æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤PCAç‰¹å¾ï¼Œå› ä¸ºæ ‡å‡†åŒ–å™¨ä¸åŒ…å«PCAç‰¹å¾ï¼‰
    feature_columns = [col for col in prediction_data.columns if
                       col not in Config.exclude_columns and not col.startswith('pca_')]
    X_prediction = prediction_data[feature_columns].copy()

    # 2. æ™ºèƒ½ç‰¹å¾é€‚é…ï¼ˆä¸éåˆ†å—ç‰ˆæœ¬ä¸€è‡´ï¼‰
    original_feature_order = [col for col in feature_order if not col.startswith('pca_')]
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œç‰¹å¾åç§°è½¬æ¢
    needs_feature_conversion = _needs_feature_name_conversion(X_prediction.columns, original_feature_order)
    
    if needs_feature_conversion:
        print("ğŸ”§ æ£€æµ‹åˆ°ç‰¹å¾åç§°ä¸åŒ¹é…ï¼Œè¿›è¡Œæ™ºèƒ½è½¬æ¢...")
        X_prediction = _convert_feature_names_to_match_scaler(X_prediction, original_feature_order, scaler, mapping_info)
    else:
        # åŸæœ‰çš„ç‰¹å¾å¯¹é½é€»è¾‘
        available_features = [col for col in original_feature_order if col in X_prediction.columns]
        missing_features = [col for col in original_feature_order if col not in X_prediction.columns]

        if missing_features:
            print(f"æ·»åŠ  {len(missing_features)} ä¸ªç¼ºå¤±ç‰¹å¾...")
            for feat in missing_features:
                X_prediction[feat] = 0

        X_prediction = X_prediction[original_feature_order]

    # 3. æ•°æ®æ¸…ç†å’Œç±»å‹è½¬æ¢
    print("æ•°æ®æ¸…ç†å’Œç±»å‹è½¬æ¢...")
    for col in X_prediction.columns:
        if X_prediction[col].dtype == 'object':
            X_prediction[col] = pd.to_numeric(X_prediction[col], errors='coerce')
            X_prediction[col] = X_prediction[col].fillna(0)
    X_prediction = X_prediction.astype(float)

    # 4. æ•°æ®æ ‡å‡†åŒ–
    print("æ•°æ®æ ‡å‡†åŒ–...")
    X_scaled = scaler.transform(X_prediction)
    X_scaled = pd.DataFrame(X_scaled, columns=X_prediction.columns, index=X_prediction.index)

    # 5. æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
    print("æ£€æŸ¥NaNå€¼...")
    nan_count = X_scaled.isnull().sum().sum()
    if nan_count > 0:
        print(f"å‘ç° {nan_count} ä¸ªNaNå€¼ï¼Œä½¿ç”¨0å¡«å……...")
        X_scaled = X_scaled.fillna(0)

    # 6. åˆ†ä½æ•°è£å‰ªï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿per_muä¸è¶…è¿‡300ï¼‰
    print("åº”ç”¨åˆ†ä½æ•°è£å‰ªï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
    for col in X_scaled.columns:
        q_low = X_scaled[col].quantile(0.001)  # æ›´ä¸¥æ ¼çš„ä¸‹ç•Œ
        q_high = X_scaled[col].quantile(0.999)  # æ›´ä¸¥æ ¼çš„ä¸Šç•Œ
        X_scaled[col] = np.clip(X_scaled[col], q_low, q_high)

    # 7. é¢å¤–æ•°æ®èŒƒå›´é™åˆ¶ï¼ˆç¡®ä¿é¢„æµ‹å€¼åˆç†ï¼‰
    print("åº”ç”¨é¢å¤–æ•°æ®èŒƒå›´é™åˆ¶...")
    for col in X_scaled.columns:
        # è¿›ä¸€æ­¥é™åˆ¶æ•°æ®èŒƒå›´ï¼Œé¿å…æç«¯å€¼
        X_scaled[col] = np.clip(X_scaled[col], -2.5, 2.5)

    # 6. PCAé™ç»´
    print("PCAé™ç»´...")
    X_pca = pca.transform(X_scaled)
    X_pca_df = pd.DataFrame(X_pca, index=X_prediction.index)

    # 6. äº¤äº’ç‰¹å¾ç”Ÿæˆï¼ˆæŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼šè·³è¿‡ï¼‰
    print("äº¤äº’ç‰¹å¾ç”Ÿæˆï¼ˆè·³è¿‡ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰...")
    X_inter = X_scaled.copy()

    # 7. åˆå¹¶æ‰€æœ‰ç‰¹å¾
    X_final = pd.concat([X_scaled, X_pca_df, X_inter], axis=1)
    print(f"åˆå¹¶åç‰¹å¾æ•°é‡: {X_final.shape[1]}")

    # 8. XGBoostç‰¹å¾æå–ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    try:
        print("XGBoostç‰¹å¾æå–...")
        # åŠ è½½XGBoostæ¨¡å‹
        xgb_model = xgb.XGBRegressor()
        
        # å°è¯•åŠ è½½JSONæ ¼å¼çš„æ¨¡å‹
        if Config.xgboost_model_file.endswith('.json'):
            xgb_model.load_model(Config.xgboost_model_file)
        else:
            # å°è¯•åŠ è½½PKLæ ¼å¼çš„æ¨¡å‹
            import joblib
            xgb_model = joblib.load(Config.xgboost_model_file)

        # è°ƒæ•´ç‰¹å¾æ•°é‡
        if X_final.shape[1] != xgb_model.n_features_in_:
            if X_final.shape[1] < xgb_model.n_features_in_:
                missing_features = xgb_model.n_features_in_ - X_final.shape[1]
                print(f"æ·»åŠ  {missing_features} ä¸ªé›¶ç‰¹å¾ä»¥åŒ¹é…XGBoostæ¨¡å‹...")
                for i in range(missing_features):
                    X_final[f'missing_feature_{i}'] = 0
            else:
                print(f"æˆªå–å‰ {xgb_model.n_features_in_} ä¸ªç‰¹å¾...")
                X_final = X_final.iloc[:, :xgb_model.n_features_in_]

        # æå–XGBoostç‰¹å¾
        # ç¡®ä¿è¾“å…¥æ˜¯DataFrameå¹¶ä¿æŒç‰¹å¾åç§°
        if isinstance(X_final, pd.DataFrame):
            X_final_with_names = X_final
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œéœ€è¦é‡æ–°åˆ›å»ºDataFrame
            X_final_with_names = pd.DataFrame(X_final, index=X_prediction.index)

        # ä¿®å¤ç‰¹å¾åç§°ä¸åŒ¹é…é—®é¢˜
        # å¦‚æœXGBoostæ¨¡å‹æœŸæœ›ç‰¹å®šçš„ç‰¹å¾åç§°ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´è¾“å…¥æ•°æ®
        if hasattr(xgb_model, 'feature_names_in_') and xgb_model.feature_names_in_ is not None:
            expected_feature_names = xgb_model.feature_names_in_
            print(f"XGBoostæ¨¡å‹æœŸæœ›ç‰¹å¾åç§°: {len(expected_feature_names)} ä¸ª")
            print(f"è¾“å…¥æ•°æ®ç‰¹å¾åç§°: {len(X_final_with_names.columns)} ä¸ª")
            
            # å¦‚æœç‰¹å¾æ•°é‡åŒ¹é…ä½†åç§°ä¸åŒ¹é…ï¼Œé‡å‘½ååˆ—
            if len(X_final_with_names.columns) == len(expected_feature_names):
                X_final_with_names.columns = expected_feature_names
                print("âœ… å·²é‡å‘½åç‰¹å¾ä»¥åŒ¹é…XGBoostæ¨¡å‹æœŸæœ›")
            else:
                print("âš ï¸ ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œä½¿ç”¨æ•°å€¼ç´¢å¼•")
                # åˆ›å»ºæ•°å€¼ç´¢å¼•çš„DataFrame
                X_final_with_names = pd.DataFrame(X_final_with_names.values, 
                                                columns=[f'feature_{i}' for i in range(X_final_with_names.shape[1])])

        # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
        # XGBoost.apply() æœŸæœ›numpyæ•°ç»„è€Œä¸æ˜¯DataFrame
        try:
            leaf_features = xgb_model.apply(X_final_with_names.values)
        except Exception as e:
            print(f"âš ï¸ XGBoost.apply()å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨predictæ–¹æ³•: {e}")
            # å›é€€åˆ°ä½¿ç”¨predictæ–¹æ³•
            leaf_features = xgb_model.predict(X_final_with_names.values, pred_leaf=True)
        if leaf_features.ndim == 1:   
            leaf_features = leaf_features.reshape(-1, 1)

        # åˆå¹¶ç‰¹å¾
        X_combined = np.hstack([X_final_with_names.values, leaf_features])

        # ä½¿ç”¨ä¼ å…¥çš„åŠ¨æ€ç‰¹å¾æ•°é‡
        if expected_features is None:
            # å¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå‘åå…¼å®¹ï¼‰
            expected_features = 89
            print("âš ï¸ æœªæä¾›æœŸæœ›ç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼89")
        
        print(f"æ¨¡å‹æœŸæœ›ç‰¹å¾æ•°é‡: {expected_features}")
        
        # ç¡®ä¿ç‰¹å¾æ•°é‡åŒ¹é…æ¨¡å‹æœŸæœ›
        if X_combined.shape[1] != expected_features:
            if X_combined.shape[1] < expected_features:
                missing_features = expected_features - X_combined.shape[1]
                print(f"æ·»åŠ  {missing_features} ä¸ªé›¶ç‰¹å¾ä»¥åŒ¹é…æ¨¡å‹æœŸæœ›...")
                for i in range(missing_features):
                    X_combined = np.hstack([X_combined, np.zeros((X_combined.shape[0], 1))])
            else:
                print(f"æˆªå–å‰ {expected_features} ä¸ªç‰¹å¾...")
                X_combined = X_combined[:, :expected_features]

        X_combined_df = pd.DataFrame(X_combined)
        print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {X_combined_df.shape[1]}")

    except Exception as e:
        print(f"XGBoostç‰¹å¾æå–å¤±è´¥: {e}")
        print("ä½¿ç”¨åŸå§‹ç‰¹å¾ä½œä¸ºå¤‡é€‰...")
        # ä½¿ç”¨åŸå§‹ç‰¹å¾
        X_combined_df = X_final.copy()
        if X_combined_df.shape[1] != (expected_features or 113):
            if X_combined_df.shape[1] < 113:
                missing_features = (expected_features or 113) - X_combined_df.shape[1]
                print(f"æ·»åŠ  {missing_features} ä¸ªé›¶ç‰¹å¾ä»¥è¾¾åˆ°{expected_features or 113}ä¸ªç‰¹å¾...")
                for i in range(missing_features):
                    X_combined_df[f'missing_feature_{i}'] = 0
            else:
                print(f"æˆªå–å‰ 113 ä¸ªç‰¹å¾...")
                X_combined_df = X_combined_df.iloc[:, :113]

        print(f"å¤‡é€‰æ–¹æ¡ˆæœ€ç»ˆç‰¹å¾æ•°é‡: {X_combined_df.shape[1]}")

        # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç‰¹å¾æ•°é‡æ˜¯113
        if X_combined_df.shape[1] != (expected_features or 113):
            print(f"âš ï¸ ç‰¹å¾æ•°é‡ä¸æ­£ç¡®: {X_combined_df.shape[1]}ï¼Œè°ƒæ•´ä¸º113")
            if X_combined_df.shape[1] < 113:
                missing_cols = 113 - X_combined_df.shape[1]
                for i in range(missing_cols):
                    X_combined_df[f'final_zero_feat_{i}'] = 0
            else:
                X_combined_df = X_combined_df.iloc[:, :113]
            print(f"è°ƒæ•´åæœ€ç»ˆç‰¹å¾æ•°é‡: {X_combined_df.shape[1]}")

    return X_combined_df


def preprocess_prediction_data_chunked(prediction_data, scaler, pca, feature_order, expected_features=None, chunk_size=100000, mapping_info=None):
    """åˆ†å—é¢„å¤„ç†é¢„æµ‹æ•°æ®ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼Œç»Ÿä¸€ä½¿ç”¨åŠ¨æ€ç‰¹å¾æ•°é‡è°ƒæ•´ï¼‰"""
    print(f"å¼€å§‹åˆ†å—é¢„å¤„ç†ï¼Œå—å¤§å°: {chunk_size:,} è¡Œ")

    n_total = prediction_data.shape[0]
    n_chunks = (n_total + chunk_size - 1) // chunk_size
    processed_chunks = []

    for i in range(n_chunks):
        start = i * chunk_size
        end = min((i + 1) * chunk_size, n_total)
        print(f"å¤„ç†å— {i + 1}/{n_chunks}: è¡Œ {start:,} - {end:,}")

        # æå–å½“å‰å—çš„æ•°æ®
        chunk_data = prediction_data.iloc[start:end].copy()

        # é¢„å¤„ç†å½“å‰å—
        chunk_processed = preprocess_single_chunk(chunk_data, scaler, pca, feature_order, expected_features, mapping_info)
        processed_chunks.append(chunk_processed)

        # æ¸…ç†å†…å­˜
        del chunk_data, chunk_processed

    # åˆå¹¶æ‰€æœ‰å¤„ç†åçš„å—
    print("åˆå¹¶æ‰€æœ‰å¤„ç†åçš„å—...")
    X_combined_df = pd.concat(processed_chunks, ignore_index=True)
    del processed_chunks

    print(f"åˆ†å—é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾æ•°é‡: {X_combined_df.shape[1]}")
    return X_combined_df


def preprocess_single_chunk(chunk_data, scaler, pca, feature_order, expected_features=None, mapping_info=None):
    """é¢„å¤„ç†å•ä¸ªæ•°æ®å—ï¼ˆæ™ºèƒ½ç‰¹å¾é€‚é…ï¼‰"""
    # 1. æå–ç‰¹å¾åˆ—ï¼ˆæ’é™¤PCAç‰¹å¾ï¼Œå› ä¸ºæ ‡å‡†åŒ–å™¨ä¸åŒ…å«PCAç‰¹å¾ï¼‰
    feature_columns = [col for col in chunk_data.columns if
                       col not in Config.exclude_columns and not col.startswith('pca_')]
    X_prediction = chunk_data[feature_columns].copy()

    # 2. æ™ºèƒ½ç‰¹å¾é€‚é… - æ£€æµ‹å¹¶ä¿®å¤ç‰¹å¾åç§°ä¸åŒ¹é…é—®é¢˜
    original_feature_order = [col for col in feature_order if not col.startswith('pca_')]
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œç‰¹å¾åç§°è½¬æ¢
    needs_feature_conversion = _needs_feature_name_conversion(X_prediction.columns, original_feature_order)
    
    if needs_feature_conversion:
        print("ğŸ”§ æ£€æµ‹åˆ°ç‰¹å¾åç§°ä¸åŒ¹é…ï¼Œè¿›è¡Œæ™ºèƒ½è½¬æ¢...")
        X_prediction = _convert_feature_names_to_match_scaler(X_prediction, original_feature_order, scaler, mapping_info)
    else:
        # åŸæœ‰çš„ç‰¹å¾å¯¹é½é€»è¾‘
        available_features = [col for col in original_feature_order if col in X_prediction.columns]
        missing_features = [col for col in original_feature_order if col not in X_prediction.columns]

        if missing_features:
            for feat in missing_features:
                X_prediction[feat] = 0

        X_prediction = X_prediction[original_feature_order]

    # 3. æ•°æ®æ¸…ç†å’Œç±»å‹è½¬æ¢
    for col in X_prediction.columns:
        if X_prediction[col].dtype == 'object':
            X_prediction[col] = pd.to_numeric(X_prediction[col], errors='coerce')
            X_prediction[col] = X_prediction[col].fillna(0)
    X_prediction = X_prediction.astype(float)

    # 4. æ•°æ®æ ‡å‡†åŒ–
    X_scaled = scaler.transform(X_prediction)
    X_scaled = pd.DataFrame(X_scaled, columns=X_prediction.columns, index=X_prediction.index)

    # 5. æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
    nan_count = X_scaled.isnull().sum().sum()
    if nan_count > 0:
        X_scaled = X_scaled.fillna(0)

    # 6. åˆ†ä½æ•°è£å‰ªï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿per_muä¸è¶…è¿‡300ï¼‰
    for col in X_scaled.columns:
        q_low = X_scaled[col].quantile(0.001)  # æ›´ä¸¥æ ¼çš„ä¸‹ç•Œ
        q_high = X_scaled[col].quantile(0.999)  # æ›´ä¸¥æ ¼çš„ä¸Šç•Œ
        X_scaled[col] = np.clip(X_scaled[col], q_low, q_high)

    # 7. é¢å¤–æ•°æ®èŒƒå›´é™åˆ¶ï¼ˆç¡®ä¿é¢„æµ‹å€¼åˆç†ï¼Œå¼ºåŒ–ç‰ˆæœ¬ï¼‰
    for col in X_scaled.columns:
        # è¿›ä¸€æ­¥é™åˆ¶æ•°æ®èŒƒå›´ï¼Œé¿å…æç«¯å€¼
        X_scaled[col] = np.clip(X_scaled[col], -1.5, 1.5)  # è¿›ä¸€æ­¥ç¼©å°èŒƒå›´

    # 8. PCAé™ç»´
    print("åº”ç”¨PCAå˜æ¢...")
    print(f"ğŸ” PCAæœŸæœ›ç‰¹å¾æ•°é‡: {pca.n_features_in_}")
    print(f"ğŸ” PCAæœŸæœ›ç‰¹å¾åç§°: {pca.feature_names_in_}")
    print(f"ğŸ” è¾“å…¥æ•°æ®ç‰¹å¾åç§°: {list(X_scaled.columns)}")
    
    # å…³é”®ä¿®å¤ï¼šç¡®ä¿PCAè¾“å…¥çš„ç‰¹å¾åç§°ä¸PCAè®­ç»ƒæ—¶ä¸€è‡´
    # PCAæœŸæœ›çš„æ˜¯åŸå§‹ç‰¹å¾åç§°ï¼Œä¸æ˜¯scalerå¤„ç†åçš„ç‰¹å¾åç§°
    pca_expected_features = list(pca.feature_names_in_)
    pca_input_features = []
    
    # ä¸ºPCAé€‰æ‹©æ­£ç¡®çš„ç‰¹å¾
    for pca_feature in pca_expected_features:
        if pca_feature in X_scaled.columns:
            pca_input_features.append(pca_feature)
        else:
            # å¦‚æœPCAæœŸæœ›çš„ç‰¹å¾ä¸åœ¨scalerè¾“å‡ºä¸­ï¼Œå°è¯•æ‰¾åˆ°å¯¹åº”çš„åŸºç¡€ç‰¹å¾
            # ä¾‹å¦‚ï¼šPCAæœŸæœ›right_tsun_meanï¼Œä½†scalerè¾“å‡ºright_tsun_sum
            base_name = pca_feature.replace('_sum', '').replace('_max', '').replace('_min', '').replace('_mean', '')
            mean_feature = base_name + '_mean'
            if mean_feature in X_scaled.columns:
                pca_input_features.append(mean_feature)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨0å¡«å……
                pca_input_features.append(pca_feature)
                print(f"âš ï¸ PCAç‰¹å¾ {pca_feature} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨0å¡«å……")
    
    # åˆ›å»ºPCAè¾“å…¥æ•°æ®
    X_pca_input = X_scaled[pca_input_features].copy()
    print(f"ğŸ”§ PCAè¾“å…¥ç‰¹å¾æ•°é‡: {len(X_pca_input.columns)}")
    print(f"ğŸ”§ PCAè¾“å…¥ç‰¹å¾åç§°: {list(X_pca_input.columns)}")
    
    X_pca = pca.transform(X_pca_input)
    X_pca_df = pd.DataFrame(X_pca, index=X_prediction.index)

    # 9. äº¤äº’ç‰¹å¾ç”Ÿæˆï¼ˆæŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼šè·³è¿‡ï¼‰
    X_inter = X_scaled.copy()

    # 10. åˆå¹¶æ‰€æœ‰ç‰¹å¾
    X_final = pd.concat([X_scaled, X_pca_df, X_inter], axis=1)

    # 11. XGBoostç‰¹å¾æå–ï¼ˆä½¿ç”¨ä¸ä¸»å‡½æ•°ç›¸åŒçš„åŠ¨æ€è°ƒæ•´é€»è¾‘ï¼‰
    try:
        # åŠ è½½XGBoostæ¨¡å‹
        xgb_model = xgb.XGBRegressor()
        xgb_model.load_model(Config.xgboost_model_file)

        # è°ƒæ•´ç‰¹å¾æ•°é‡ä»¥åŒ¹é…XGBoostæ¨¡å‹æœŸæœ›ï¼ˆä¸ä¸»å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        if X_final.shape[1] != xgb_model.n_features_in_:
            if X_final.shape[1] < xgb_model.n_features_in_:
                missing_features = xgb_model.n_features_in_ - X_final.shape[1]
                print(f"æ·»åŠ  {missing_features} ä¸ªé›¶ç‰¹å¾ä»¥åŒ¹é…XGBoostæ¨¡å‹...")
                for i in range(missing_features):
                    X_final[f'missing_feature_{i}'] = 0
            else:
                print(f"æˆªå–å‰ {xgb_model.n_features_in_} ä¸ªç‰¹å¾...")
                X_final = X_final.iloc[:, :xgb_model.n_features_in_]

        # æå–XGBoostç‰¹å¾ï¼ˆä¸ä¸»å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        # ç¡®ä¿è¾“å…¥æ˜¯DataFrameå¹¶ä¿æŒç‰¹å¾åç§°
        if isinstance(X_final, pd.DataFrame):
            X_final_with_names = X_final
        else:
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œéœ€è¦é‡æ–°åˆ›å»ºDataFrame
            X_final_with_names = pd.DataFrame(X_final, index=X_prediction.index)

        # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
        # XGBoost.apply() æœŸæœ›numpyæ•°ç»„è€Œä¸æ˜¯DataFrame
        leaf_features = xgb_model.apply(X_final_with_names.values)
        if leaf_features.ndim == 1:
            leaf_features = leaf_features.reshape(-1, 1)

        # åˆå¹¶ç‰¹å¾ï¼ˆä¸ä¸»å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        X_combined = np.hstack([X_final_with_names.values, leaf_features])
        X_combined_df = pd.DataFrame(X_combined, index=X_prediction.index)

    except Exception as e:
        print(f"XGBoostç‰¹å¾æå–å¤±è´¥: {str(e)}")
        print("ä½¿ç”¨åŸå§‹ç‰¹å¾ä½œä¸ºå¤‡é€‰...")

        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨åŸå§‹ç‰¹å¾
        X_combined_df = X_final.copy()

    # 12. ç¡®ä¿ç‰¹å¾æ•°é‡æ­£ç¡®ï¼ˆä¸ä¸»å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
    # ä½¿ç”¨ä¼ å…¥çš„åŠ¨æ€ç‰¹å¾æ•°é‡
    if expected_features is None:
        # å¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå‘åå…¼å®¹ï¼‰
        expected_features = 89
        print("âš ï¸ æœªæä¾›æœŸæœ›ç‰¹å¾æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼89")
    
    if X_combined_df.shape[1] != expected_features:
        if X_combined_df.shape[1] < expected_features:
            missing_cols = expected_features - X_combined_df.shape[1]
            print(f"æ·»åŠ  {missing_cols} ä¸ªé›¶ç‰¹å¾ä»¥è¾¾åˆ°{expected_features}ä¸ªç‰¹å¾...")
            for i in range(missing_cols):
                X_combined_df[f'zero_feat_{i}'] = 0
        else:
            print(f"æˆªå–å‰ {expected_features} ä¸ªç‰¹å¾...")
            X_combined_df = X_combined_df.iloc[:, :expected_features]
        print(f"è°ƒæ•´åæœ€ç»ˆç‰¹å¾æ•°é‡: {X_combined_df.shape[1]}")

    return X_combined_df


def predict_with_processed_data(prediction_data, X_processed, model, save_chunk_size=5000000):
    """ä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œå¹¶åˆ†å—ä¿å­˜ç»“æœ"""
    print("æ‰§è¡Œæ¨¡å‹é¢„æµ‹...")

    batch_size = 500000
    n_pred = X_processed.shape[0]
    results_chunks = []
    saved_files = []

    print(f"æ•°æ®æ€»é‡: {n_pred:,} è¡Œï¼Œåˆ† {(n_pred + batch_size - 1) // batch_size} æ‰¹å¤„ç†")
    print(f"ä¿å­˜å—å¤§å°: {save_chunk_size:,} è¡Œ")

    # è®¡ç®—éœ€è¦ä¿å­˜çš„æ–‡ä»¶æ•°é‡
    num_save_files = (n_pred + save_chunk_size - 1) // save_chunk_size
    print(f"å°†ä¿å­˜ {num_save_files} ä¸ªé¢„æµ‹ç»“æœæ–‡ä»¶")

    for start in range(0, n_pred, batch_size):
        end = min(start + batch_size, n_pred)
        print(f"å¤„ç†æ‰¹æ¬¡: è¡Œ {start:,} - {end:,}")

        # è°ƒè¯•ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼
        batch_data = X_processed.iloc[start:end]
        print(f"æ‰¹æ¬¡æ•°æ®å½¢çŠ¶: {batch_data.shape}")
        print(f"æ‰¹æ¬¡æ•°æ®ç±»å‹: {type(batch_data)}")
        print(f"æ‰¹æ¬¡æ•°æ®å‰5è¡Œå‰5åˆ—:\n{batch_data.iloc[:5, :5]}")

        # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„æ ¼å¼
        if isinstance(batch_data, pd.DataFrame):
            batch_data = batch_data.values

        # æ£€æŸ¥å¹¶å¤„ç†å¼‚å¸¸å€¼
        print(f"æ•°æ®èŒƒå›´æ£€æŸ¥:")
        print(f"  æœ€å°å€¼: {batch_data.min()}")
        print(f"  æœ€å¤§å€¼: {batch_data.max()}")
        print(f"  æ˜¯å¦æœ‰æ— ç©·å€¼: {np.isinf(batch_data).any()}")
        print(f"  æ˜¯å¦æœ‰NaNå€¼: {np.isnan(batch_data).any()}")

        # å¤„ç†å¼‚å¸¸å€¼
        batch_data = np.nan_to_num(batch_data, nan=0.0, posinf=0.0, neginf=0.0)

        # é™åˆ¶æ•°æ®èŒƒå›´åˆ°åˆç†åŒºé—´ï¼ˆå¼ºåŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿per_muä¸è¶…è¿‡300ï¼‰
        # è®­ç»ƒæ—¶æ¨¡å‹è¾“å‡ºè¢«é™åˆ¶åœ¨[3.0, 10.0]ï¼Œå¯¹åº”expm1åä¸º[19.09, 22025.46]
        # æˆ‘ä»¬éœ€è¦ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ›´ä¸¥æ ¼çš„èŒƒå›´å†…ï¼Œé¿å…æç«¯å€¼
        batch_data = np.clip(batch_data, -1.5, 1.5)  # è¿›ä¸€æ­¥ç¼©å°èŒƒå›´

        print(f"å¤„ç†åæ•°æ®èŒƒå›´: [{batch_data.min():.6f}, {batch_data.max():.6f}]")

        # æ£€æŸ¥æ¨¡å‹è¾“å…¥å½¢çŠ¶
        print(f"æ¨¡å‹è¾“å…¥å½¢çŠ¶æœŸæœ›: {model.input_shape}")
        print(f"å®é™…è¾“å…¥å½¢çŠ¶: {batch_data.shape}")

        # å°è¯•é¢„æµ‹
        try:
            predictions = model.predict(batch_data)
            print(f"é¢„æµ‹ç»“æœç±»å‹: {type(predictions)}")
            if isinstance(predictions, list):
                print(f"é¢„æµ‹ç»“æœæ•°é‡: {len(predictions)}")
                cls_pred_batch = predictions[0]
                reg_pred_batch = predictions[1]
                print(f"åˆ†ç±»é¢„æµ‹å½¢çŠ¶: {cls_pred_batch.shape}")
                print(f"å›å½’é¢„æµ‹å½¢çŠ¶: {reg_pred_batch.shape}")
            else:
                print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {predictions.shape}")
                cls_pred_batch = predictions
                reg_pred_batch = predictions
        except Exception as e:
            print(f"æ¨¡å‹é¢„æµ‹é”™è¯¯: {str(e)}")
            raise

        # åå˜æ¢ï¼ˆæŒ‰ç…§è®­ç»ƒä»£ç çš„æ–¹å¼ï¼‰
        reg_pred_batch = np.expm1(reg_pred_batch)

        results_chunk = pd.DataFrame({
            'x': prediction_data['x'].iloc[start:end],
            'y': prediction_data['y'].iloc[start:end],
            'yyyy': prediction_data['yyyy'].iloc[start:end],
            'suit': np.argmax(cls_pred_batch, axis=1),
            'per_mu': reg_pred_batch[:, 1],  # ç¬¬äºŒåˆ—æ˜¯per_mu
            'per_qu': reg_pred_batch[:, 0]  # ç¬¬ä¸€åˆ—æ˜¯per_qu
        })
        results_chunks.append(results_chunk)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æ–‡ä»¶
        current_total_rows = sum(len(chunk) for chunk in results_chunks)
        if current_total_rows >= save_chunk_size or start + batch_size >= n_pred:
            # åˆå¹¶å½“å‰æ‰€æœ‰ç»“æœ
            current_results = pd.concat(results_chunks, ignore_index=True)

            # å¦‚æœè¶…è¿‡å—å¤§å°ï¼Œåªä¿å­˜å‰é¢çš„éƒ¨åˆ†
            if current_total_rows > save_chunk_size:
                save_results = current_results.head(save_chunk_size)
                remaining_results = current_results.iloc[save_chunk_size:]
                results_chunks = [remaining_results] if len(remaining_results) > 0 else []
            else:
                save_results = current_results
                results_chunks = []

            # ä¿å­˜æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_index = len(saved_files) + 1
            filename = f"prediction_results_chunk_{file_index:03d}_{timestamp}.csv"
            filepath = os.path.join(Config.result_dir, filename)

            save_results.to_csv(filepath, index=False)
            saved_files.append(filepath)

            print(f"âœ… å·²ä¿å­˜é¢„æµ‹ç»“æœæ–‡ä»¶ {file_index}: {filename}")
            print(f"   æ–‡ä»¶åŒ…å«: {len(save_results):,} è¡Œæ•°æ®")
            print(f"   per_muèŒƒå›´: {save_results['per_mu'].min():.2f} - {save_results['per_mu'].max():.2f}")
            print(f"   æ–‡ä»¶è·¯å¾„: {filepath}")

    return saved_files


# åœ¨ç±»å®šä¹‰å®Œæˆååˆå§‹åŒ–è·¯å¾„
Config._update_paths()

if __name__ == "__main__":
    import sys

    print("=== äº§é‡é¢„æµ‹ç¨‹åº ===")
    print("ä½¿ç”¨ç°æœ‰æ¨¡å‹å¯¹é¢„æµ‹æ°”è±¡æ•°æ®è¿›è¡Œäº§é‡é¢„æµ‹")
    print("æ•°æ®æº: é¢„æµ‹æ°”è±¡æ•°æ® + åœŸå£¤æ•°æ®")
    print("=" * 50)

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å°å‹æµ‹è¯•æ¨¡å¼
    test_mode = False
    test_size = 10000
    weather_start_row = None
    weather_end_row = None
    model_name = None

    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print("\nğŸ” æ£€æµ‹å¯ç”¨æ¨¡å‹...")
    available_models = Config.list_available_models()

    if len(sys.argv) > 1:
        if sys.argv[1].lower() in ['--test', '--small', '--mini']:
            test_mode = True
            print("ğŸ”¬ å¯ç”¨å°å‹æµ‹è¯•æ¨¡å¼")
        elif sys.argv[1].lower().startswith('--test-size='):
            test_mode = True
            try:
                test_size = int(sys.argv[1].split('=')[1])
                print(f"ğŸ”¬ å¯ç”¨å°å‹æµ‹è¯•æ¨¡å¼ï¼Œæ•°æ®é‡é™åˆ¶: {test_size:,} è¡Œ")
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„æµ‹è¯•å¤§å°å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 10,000")
                test_size = 10000

    # è§£ææ‰€æœ‰å‚æ•°
    for arg in sys.argv[1:]:
        if arg.startswith('--weather-start='):
            try:
                weather_start_row = int(arg.split('=')[1])
                print(f"ğŸŒ¤ï¸ æ°”è±¡æ•°æ®èµ·å§‹è¡Œè®¾ç½®ä¸º: {weather_start_row:,}")
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„èµ·å§‹è¡Œå‚æ•°")
                weather_start_row = None
        elif arg.startswith('--weather-end='):
            try:
                weather_end_row = int(arg.split('=')[1])
                print(f"ğŸŒ¤ï¸ æ°”è±¡æ•°æ®ç»“æŸè¡Œè®¾ç½®ä¸º: {weather_end_row:,}")
            except ValueError:
                print("âš ï¸ æ— æ•ˆçš„ç»“æŸè¡Œå‚æ•°")
                weather_end_row = None
        elif arg.startswith('--model='):
            model_name = arg.split('=')[1]
            if model_name in available_models:
                print(f"ğŸ¯ æŒ‡å®šä½¿ç”¨æ¨¡å‹: {model_name}")
            else:
                print(f"âš ï¸ æŒ‡å®šçš„æ¨¡å‹ '{model_name}' ä¸å­˜åœ¨ï¼Œå¯ç”¨æ¨¡å‹: {available_models}")
                model_name = None
        elif arg == '--list-models':
            print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
            for i, model in enumerate(available_models, 1):
                print(f"   {i}. {model}")
            print("\nä½¿ç”¨æ–¹æ³•: python äº§é‡æ¨¡å‹è°ƒç”¨é¢„æµ‹.py --model=æ¨¡å‹åç§°")
            exit(0)

    # è¿è¡Œé¢„æµ‹
    success = main(test_mode=test_mode, test_size=test_size,
                   weather_start_row=weather_start_row, weather_end_row=weather_end_row,
                   model_name=model_name)

    if success:
        print("\nğŸ‰ é¢„æµ‹ç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
    else:
        print("\nâŒ é¢„æµ‹ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼")
        exit(1)


